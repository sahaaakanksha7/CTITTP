{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "299fe012-87e2-4e8d-8be8-ab78bf6a45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_citations(citations_raw):\n",
    "    \"\"\"\n",
    "    This function takes a raw string of citations, extracts individual citations,\n",
    "    removes duplicates, cleans up irrelevant characters, and ensures no extra spaces or invalid characters.\n",
    "\n",
    "    Args:\n",
    "        citations_raw (str): The raw citation string from which citations are to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique citations.\n",
    "    \"\"\"\n",
    "    #print(citations_raw)\n",
    "    if pd.isna(citations_raw) or citations_raw == '':\n",
    "        return []\n",
    "\n",
    "    # Replace any occurrence of 'Citation: ' to standardize the string\n",
    "    citations_raw = citations_raw.replace(\"Citation: \", \"\")\n",
    "    \n",
    "    # Split citations by both the delimiters '), (' and ')('\n",
    "    citations = re.split(r\"\\)\\s*,?\\s*\\(|\\)\\(\", citations_raw)\n",
    "\n",
    "    # Clean each citation by removing unwanted characters and extra spaces\n",
    "    cleaned_citations = []\n",
    "    for citation in citations:\n",
    "        citation = citation.replace(\"(\", \"\").replace(\")\", \"\").strip(\", \").strip()\n",
    "        \n",
    "        # Use regex to extract only the relevant citation portion before any period or embedded sentence\n",
    "        # Here, we match the first part (e.g., \"US-CERT HIDDEN COBRA June 2017\") and ignore the rest\n",
    "        citation = re.sub(r'\\s*\\..*$', '', citation)  # Remove everything after the first period\n",
    "        \n",
    "        # We also use another regex to handle extra commas, which might still linger after cleaning\n",
    "        citation = re.sub(r',\\s*$', '', citation)  # Remove comma at the end of citation\n",
    "        \n",
    "        if citation:  # Only add to the list if there is any valid citation\n",
    "            cleaned_citations.append(citation)\n",
    "    \n",
    "    # Remove duplicates by converting the list to a set and back to a list\n",
    "    cleaned_citations = list(set(cleaned_citations))\n",
    "\n",
    "    #print(cleaned_citations)\n",
    "\n",
    "    return cleaned_citations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "63c16354-3311-48ad-bf4e-4759da5165c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enterprise Groups File Path: ..\\ATTACK Excel sheets\\enterprise-attack-v15.1-groups.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load configuration from the JSON file located outside the current directory\n",
    "config_file_path = os.path.join(\"..\", \"Malpedia Bib files Analysis\", \"config.json\")\n",
    "\n",
    "with open(config_file_path, \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Step 2: Extract the data directory and file paths for groups and software from the configuration\n",
    "data_directory = config[\"data_directory\"]\n",
    "groups_file_paths = {key: os.path.normpath(os.path.join(data_directory, value)) for key, value in config[\"file_paths_groups_v15\"].items()}\n",
    "\n",
    "# Example to access one of the group files (you can select enterprise, ics, or mobile)\n",
    "enterprise_groups_file = groups_file_paths['enterprise']\n",
    "print(f\"Enterprise Groups File Path: {enterprise_groups_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "07fd6e24-3d8a-47bf-ae90-7af77a711d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load Excel sheets for groups and citations\n",
    "df_groups = pd.read_excel(enterprise_groups_file, sheet_name='groups')\n",
    "df_citations = pd.read_excel(enterprise_groups_file, sheet_name='citations')\n",
    "\n",
    "# Step 4: Extract citation key and URL mapping from 'citations' sheet\n",
    "citation_map = dict(zip(df_citations['reference'], df_citations['url']))\n",
    "\n",
    "# Step 5: Initialize a dictionary to store the final data structure\n",
    "group_citations_map = {}\n",
    "\n",
    "# Process each group in the 'groups' sheet\n",
    "for index, row in df_groups.iterrows():\n",
    "    group_id = row['ID']\n",
    "    \n",
    "    # Extract and clean associated groups citations\n",
    "    associated_citations_raw = row['associated groups citations']\n",
    "    associated_citations = extract_citations(associated_citations_raw) if pd.notna(associated_citations_raw) else []\n",
    "    \n",
    "    # Extract and clean relationship citations\n",
    "    relationship_citations_raw = row['relationship citations']\n",
    "    relationship_citations = extract_citations(relationship_citations_raw) if pd.notna(relationship_citations_raw) else []\n",
    "    \n",
    "    # Map citations to URLs with uniqueness check\n",
    "    associated_citations_with_urls = {}\n",
    "    for citation in associated_citations:\n",
    "        url = citation_map.get(citation, \"URL not found\")\n",
    "        # Only add to map if citation and URL are not already present\n",
    "        if citation not in associated_citations_with_urls:\n",
    "            associated_citations_with_urls[citation] = url\n",
    "\n",
    "    relationship_citations_with_urls = {}\n",
    "    for citation in relationship_citations:\n",
    "        url = citation_map.get(citation, \"URL not found\")\n",
    "        if citation not in relationship_citations_with_urls:\n",
    "            relationship_citations_with_urls[citation] = url\n",
    "    \n",
    "    # Store the data in the final structure\n",
    "    group_citations_map[group_id] = {\n",
    "        \"associated_groups_citations\": associated_citations_with_urls,\n",
    "        \"relationship_citations\": relationship_citations_with_urls\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "e4906778-1c2c-4e10-92ba-2e9f61f4bccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#group_citations_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "75073e71-2433-49d3-866c-0578170a021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of URLs across the whole dictionary: 1176\n",
      "Totak number of groups: 148\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "total_urls = 0  # Total number of URLs across both associated_groups_citations and relationship_citations\n",
    "\n",
    "# Iterate through each group in the dictionary\n",
    "for group_id, group_data in group_citations_map.items():\n",
    "    # Count URLs in 'associated_groups_citations'\n",
    "    associated_urls = group_data.get('associated_groups_citations', {})\n",
    "    total_urls += len(associated_urls)  # Add the number of URLs in associated_groups_citations\n",
    "    \n",
    "    # Count URLs in 'relationship_citations'\n",
    "    relationship_urls = group_data.get('relationship_citations', {})\n",
    "    total_urls += len(relationship_urls)  # Add the number of URLs in relationship_citations\n",
    "\n",
    "# Output the total number of URLs\n",
    "print(f\"Total number of URLs across the whole dictionary: {total_urls}\")\n",
    "num_groups = len(group_citations_map)  \n",
    "print(f\"Totak number of groups: {num_groups}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "54a01859-582c-41ea-9bc6-6a01bb8fb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to load JSONL data\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Loads a JSONL file into a list of dictionaries.\n",
    "    Each line of the file is parsed as a JSON object.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = [json.loads(line) for line in file]\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f3b3f510-4d65-4326-a3ff-93e53d786aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_file_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\downloads\\downloads\\20241008_downloads.jsonl\"\n",
    "\n",
    "# Step 1: Load the JSONL data\n",
    "jsonl_data = load_jsonl(jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ad4d217e-5c36-4811-9c74-52f99b5c0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Function to load the JSONL data into a Python dictionary\n",
    "def load_jsonl(jsonl_file_path):\n",
    "    with open(jsonl_file_path, 'r') as file:\n",
    "        return [json.loads(line.strip()) for line in file.readlines()]\n",
    "\n",
    "# Function to extract file hash from the filename (assuming the hash is the entire filename without extension)\n",
    "def extract_file_hash_from_filename(filename):\n",
    "    # Extract the file hash from the filename by removing the .iocs extension\n",
    "    if filename.endswith('.iocs'):\n",
    "        return filename[:-5]  # Remove the last 5 characters to strip the '.iocs' extension\n",
    "    return None\n",
    "\n",
    "# Folder where the files are stored\n",
    "folder_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\CVEs\"\n",
    "\n",
    "# Dictionary to store file hash -> URL mapping\n",
    "hash_to_url_map = {}\n",
    "\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Extract file hash from the filename\n",
    "    file_hash = extract_file_hash_from_filename(filename)\n",
    "    if file_hash:\n",
    "        # Look for the matching download_sha256 in the JSONL data\n",
    "        for entry in jsonl_data:\n",
    "            if entry.get('download_sha256') == file_hash:\n",
    "                # If the hashes match, store the URL\n",
    "                hash_to_url_map[file_hash] = entry.get('url')\n",
    "                break  # Exit loop once a match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "cc763645-8ae4-4326-9202-e023567e519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n"
     ]
    }
   ],
   "source": [
    "print(len(hash_to_url_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fe85b757-6ade-4f99-a659-aae5532dc9b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "255\n"
     ]
    }
   ],
   "source": [
    "# Create a new dictionary to store the hash, group_id, and URL\n",
    "hash_group_url_map = {}\n",
    "\n",
    "# Iterate through each hash, url pair in hash_to_url_map\n",
    "for file_hash, url in hash_to_url_map.items():\n",
    "    # Iterate through each group in group_citations_map\n",
    "    for group_id, group_data in group_citations_map.items():\n",
    "        # Search for the URL in associated_groups_citations\n",
    "        if url in group_data.get('associated_groups_citations', {}).values():\n",
    "            # If the URL is found, add the file_hash, group_id, and URL to the hash_group_url_map\n",
    "            hash_group_url_map[file_hash] = {'group_id': group_id, 'url': url}\n",
    "            break  # Exit once we find the match (no need to check further in this group)\n",
    "\n",
    "        # Search for the URL in relationship_citations\n",
    "        if url in group_data.get('relationship_citations', {}).values():\n",
    "            # If the URL is found, add the file_hash, group_id, and URL to the hash_group_url_map\n",
    "            hash_group_url_map[file_hash] = {'group_id': group_id, 'url': url}\n",
    "            break  # Exit once we find the match (no need to check further in this group)\n",
    "\n",
    "# Output the final dictionary with hash, group_id, and url\n",
    "print(len(hash_group_url_map))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a42c57e3-fe8b-45ca-bea1-d6527ba9e206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash: 05bc7a68fdfe54c95ba1fb7360f2cb73bbfbdbbe939c29d764abf59f975a6a3a, URL: https://www.infosecurity-magazine.com/news/microsoft-zero-day-traced-russian/\n",
      "Hash: 2c9e582e0194bacc4e4bbb37ffe61ed7e89af5cc5748fdc001e9dd65ddfaa32f, URL: https://securelist.com/apt-trends-report-q1-2018/85280/\n",
      "Hash: 5b6328ed41cb49229d8d47046caabae1fdb90045c467d6509ae1f459a9b5b518, URL: https://www.intezer.com/wp-content/uploads/2021/09/TeamTNT-Cryptomining-Explosion.pdf\n",
      "Hash: 72beb22ceed285d666ec7912dfcb95e7107c4232e622026915ef1bcd3c593490, URL: https://unit42.paloaltonetworks.com/ukraine-targeted-outsteel-saintbot/\n",
      "Hash: 73eac7a13e4c15ce849d7a12a8d56eb3d831b6b442bf9ce7bc43afc1caafde9c, URL: https://www.us-cert.gov/ncas/alerts/TA17-164A\n",
      "Hash: cabd66802a057829a0113bc5e53ac0c2c48f91142e8a40e10aac0d9d6aebbe98, URL: https://www.bleepingcomputer.com/news/security/ukraine-links-members-of-gamaredon-hacker-group-to-russian-fsb/\n",
      "Hash: e2f84d3c77547f31ba782c0bb5525980059f651931e2b1dbbcd0a81f4430a1db, URL: https://securelist.com/the-naikon-apt/69953/\n",
      "Hash: e6af21bf6c751aabe190006f4f60b2b2a4164c6cb1bf34cc461b4f116eaf479d, URL: https://www.cyberscoop.com/middle-eastern-hacking-group-using-finfisher-malware-conduct-international-espionage/\n"
     ]
    }
   ],
   "source": [
    "# Find the hashes that are in hash_to_url_map but not in hash_group_url_map\n",
    "missing_hashes = {key: value for key, value in hash_to_url_map.items() if key not in hash_group_url_map}\n",
    "\n",
    "# Print the list of hashes and URLs that are missing in hash_group_url_map\n",
    "for hash_key, url in missing_hashes.items():\n",
    "    print(f\"Hash: {hash_key}, URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b59185ef-1fab-4378-8336-d736bd3f2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    " import os\n",
    "\n",
    "def read_files_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all the files in a specified folder with .iocs extension and creates a dictionary\n",
    "    with the file hash (from the filename) as the key and the list of CVEs as the value.\n",
    "    It also removes the 'cve' prefix from each CVE and handles the file content appropriately.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the .iocs files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the key is the file hash and the value is a list of CVEs.\n",
    "    \"\"\"\n",
    "    iocs_dict = {}\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a '.iocs' extension\n",
    "        if filename.endswith(\".iocs\"):\n",
    "            file_hash = filename.split(\".\")[0]  # Extract file hash from the filename\n",
    "            \n",
    "            # Construct the full path to the file\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            try:\n",
    "                # Open the file and read its content\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read().strip()  # Read and strip any unwanted whitespace\n",
    "\n",
    "                    # Remove \"cve\" prefix and the tab character, and store CVEs in a list\n",
    "                    content_lines = content.split('\\n')  # Split content by lines\n",
    "                    cleaned_content = []\n",
    "                    for line in content_lines:\n",
    "                        if line.startswith(\"cve\"):\n",
    "                            # Remove 'cve' and the tab character '\\t'\n",
    "                            cleaned_line = line[4:].strip()  # Strip the 'cve' prefix and any leading/trailing spaces\n",
    "                            cleaned_content.append(cleaned_line)\n",
    "\n",
    "                # Store the file hash and list of CVEs in the dictionary\n",
    "                iocs_dict[file_hash] = cleaned_content\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {filename}: {e}\")\n",
    "\n",
    "    return iocs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "0ed43b1b-9b1b-4afd-9680-ebd322bc170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\CVEs\"  \n",
    "iocs_data = read_files_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "2585a6de-ebca-47b6-919c-329e77224aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize a new dictionary to store the combined data\n",
    "hash_cve_map = {}\n",
    "\n",
    "# Step 2: Iterate through the hashes in iocs_data\n",
    "for file_hash, cve_list in iocs_data.items():\n",
    "    # Check if the file_hash exists in hash_group_url_map\n",
    "    if file_hash in hash_group_url_map:\n",
    "        # Retrieve the group information from hash_group_url_map\n",
    "        group_info = hash_group_url_map[file_hash]\n",
    "        \n",
    "        # Combine the data: CVEs, group_id, and url\n",
    "        hash_cve_map[file_hash] = {\n",
    "            'cves': cve_list,\n",
    "            'group_id': group_info['group_id'],\n",
    "            'url': group_info['url']\n",
    "        }\n",
    "\n",
    "# Step 3: Print the combined data (or return it for further use)\n",
    "#print(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "3019181b-a83e-4ccc-bae2-6a2a1080e44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the new structure where the key is group_id and the values are dictionaries\n",
    "# containing hashes, CVEs, and URLs\n",
    "group_cve_map = {}\n",
    "\n",
    "# Iterate over each entry in the hash_cve_map\n",
    "for hash_val, data in hash_cve_map.items():\n",
    "    group_id = data['group_id']\n",
    "    cves = data['cves']\n",
    "    url = data['url']\n",
    "    \n",
    "    # If the group_id doesn't exist in group_cve_map, initialize it\n",
    "    if group_id not in group_cve_map:\n",
    "        group_cve_map[group_id] = {\n",
    "            'hashes': [],\n",
    "            'url': url,\n",
    "        }\n",
    "    \n",
    "    # Add the hash and associated CVEs to the group\n",
    "    group_cve_map[group_id]['hashes'].append({\n",
    "        'hash': hash_val,\n",
    "        'cves': cves,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "1be67307-5c16-4ffe-833f-ad92f8c7f33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of group IDs with CVE data: 85\n"
     ]
    }
   ],
   "source": [
    "total_group_ids = len(group_cve_map)\n",
    "print(f\"Total number of group IDs with CVE data: {total_group_ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "869be641-8827-45fa-8ecd-706c082d25ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of CVEs: 955\n",
      "Total number of unique CVEs: 322\n",
      "Top 10 most common CVEs:\n",
      "CVE-2012-0158: 29\n",
      "CVE-2017-11882: 22\n",
      "CVE-2017-0199: 21\n",
      "CVE-2022-38028: 15\n",
      "CVE-2024-3400: 12\n",
      "CVE-2021-26855: 11\n",
      "CVE-2021-27065: 11\n",
      "CVE-2010-3333: 10\n",
      "CVE-2014-6332: 10\n",
      "CVE-2018-13379: 10\n"
     ]
    }
   ],
   "source": [
    "#Initialize a list to store all CVEs across groups for counting and determining common CVEs\n",
    "all_cves = []\n",
    "\n",
    "# Go through each group and gather all CVEs from the hashes\n",
    "for group_id, group_data in group_cve_map.items():\n",
    "    for hash_data in group_data['hashes']:\n",
    "        all_cves.extend(hash_data['cves'])\n",
    "\n",
    "# Calculate the total number of CVEs\n",
    "total_cves = len(all_cves)\n",
    "\n",
    "# Calculate unique CVEs\n",
    "unique_cves = set(all_cves)\n",
    "total_unique_cves = len(unique_cves)\n",
    "\n",
    "# Calculate the top 10 most common CVEs\n",
    "cve_counter = Counter(all_cves)\n",
    "top_10_cves = cve_counter.most_common(10)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Total number of CVEs: {total_cves}\")\n",
    "print(f\"Total number of unique CVEs: {total_unique_cves}\")\n",
    "print(\"Top 10 most common CVEs:\")\n",
    "for cve, count in top_10_cves:\n",
    "    print(f\"{cve}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "d2dd7d82-a2c9-4af5-830b-8ac270898ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "Group ID: G0029 - Unique CVEs: ['CVE-2012-4969', 'CVE-2010-2572']\n",
      "Group ID: G1015 - Unique CVEs: ['CVE-2015-2291', 'CVE-2021-3490', 'CVE-2021-35464']\n",
      "Group ID: G0121 - Unique CVEs: ['CVE-2019-2215', 'CVE-2020-0674', 'CVE-2024-9284']\n",
      "Group ID: G1023 - Unique CVEs: ['CVE-2021-20023', 'CVE-2021-20021', 'CVE-2022-27518']\n",
      "Group ID: G0139 - Unique CVEs: ['CVE-2019-5736', 'CVE-2024-6387']\n",
      "Group ID: G0032 - Unique CVEs: ['CVE-2018-202501', 'CVE-2021-1647']\n",
      "Group ID: G0069 - Unique CVEs: ['CVE-2017-01995']\n",
      "Group ID: G0027 - Unique CVEs: ['CVE-2014-6324', 'CVE-2017-15303', 'CVE-2017-0213', 'CVE-2011-3544', 'CVE-2010-0738']\n",
      "Group ID: G1017 - Unique CVEs: ['CVE-2021-27860']\n",
      "Group ID: G0046 - Unique CVEs: ['CVE-2024-47076', 'CVE-2024-47176', 'CVE-2024-29847', 'CVE-2024-47175', 'CVE-2024-47177']\n",
      "Group ID: G0034 - Unique CVEs: ['CVE-2018-8405', 'CVE-2014-3828', 'CVE-2015-5374', 'CVE-2018-8406', 'CVE-2018-4878']\n",
      "Group ID: G0040 - Unique CVEs: ['CVE-2017-8570']\n",
      "Group ID: G0125 - Unique CVEs: ['CVE-2018-18913']\n",
      "Group ID: G0102 - Unique CVEs: ['CVE-2021-34527']\n",
      "Group ID: G1021 - Unique CVEs: ['CVE-2021-44832', 'CVE-2020-10189', 'CVE-2020-8243', 'CVE-2024-20399', 'CVE-2021-45105', 'CVE-2021-20016', 'CVE-2021-22894', 'CVE-2022-30190', 'CVE-2021-44428', 'CVE-2021-22900', 'CVE-2021-22899', 'CVE-2021-26084', 'CVE-2021-35247', 'CVE-2020-8260', 'CVE-2021-45046', 'CVE-2021-4428']\n",
      "Group ID: G0016 - Unique CVEs: ['CVE-2019-7609', 'CVE-2014-9583', 'CVE-2015-1130', 'CVE-2014-4404', 'CVE-2015-2051', 'CVE-2021-21972', 'CVE-2013-0640', 'CVE-2021-1879', 'CVE-2021-36934', 'CVE-2012-5687', 'CVE-2020-14882', 'CVE-2018-10562', 'CVE-2019-9670', 'CVE-2013-0641', 'CVE-2019-1653', 'CVE-2020-0796', 'CVE-2020-8554', 'CVE-2014-8361', 'CVE-2021-4034', 'CVE-2021-38647', 'CVE-2019-0708', 'CVE-2020-4006', 'CVE-2017-5638', 'CVE-2015-1635']\n",
      "Group ID: G0131 - Unique CVEs: ['CVE-2018-8174', 'CVE-2019-9489', 'CVE-2020-8468']\n",
      "Group ID: G0022 - Unique CVEs: ['CVE-2010-3962', 'CVE-2014-4113', 'CVE-2018-100060', 'CVE-2019-100300']\n",
      "Group ID: G0077 - Unique CVEs: ['CVE-2014-0160']\n",
      "Group ID: G0049 - Unique CVEs: ['CVE-2020-1350', 'CVE-2017-1099']\n",
      "Group ID: G0007 - Unique CVEs: ['CVE-2015-2424', 'CVE-2015-2360', 'CVE-2015-3105', 'CVE-2017-0143', 'CVE-2017-0147', 'CVE-2014-8439', 'CVE-2016-7255', 'CVE-2015-3104', 'CVE-2015-4902', 'CVE-2014-3897', 'CVE-2015-2387', 'CVE-2013-3897', 'CVE-2015-0336', 'CVE-2023-23397', 'CVE-2015-7645', 'CVE-2014-4076', 'CVE-2015-0097', 'CVE-2015-3043', 'CVE-2014-1511', 'CVE-2015-1769', 'CVE-2016-0728', 'CVE-2017-0145', 'CVE-2015-3090', 'CVE-2016-725521', 'CVE-2017-0146', 'CVE-2014-1510', 'CVE-2006-6456', 'CVE-2015-2590', 'CVE-2009-0075', 'CVE-2017-0148', 'CVE-2015-0359', 'CVE-2011-3874', 'CVE-2014-0515', 'CVE-2016-7855']\n",
      "Group ID: G0035 - Unique CVEs: ['CVE-2013-2465']\n",
      "Group ID: G0064 - Unique CVEs: ['CVE-2017-11774']\n",
      "Group ID: G0106 - Unique CVEs: ['CVE-2017-3066', 'CVE-2017-10271']\n",
      "Group ID: G0092 - Unique CVEs: ['CVE-2021-27104', 'CVE-2021-27102', 'CVE-2021-27101', 'CVE-2021-27103']\n",
      "Group ID: G0098 - Unique CVEs: ['CVE-2017-7269']\n",
      "Group ID: G0055 - Unique CVEs: ['CVE-2012-0056', 'CVE-2015-1671', 'CVE-2013-1493', 'CVE-2013-2423', 'CVE-2015-0072', 'CVE-2010-1297', 'CVE-2016-0165', 'CVE-2008-2551', 'CVE-2013-3896', 'CVE-2016-0189', 'CVE-2011-0097', 'CVE-2010-3653']\n",
      "Group ID: G0010 - Unique CVEs: ['CVE-2008-3431', 'CVE-2013-3346', 'CVE-2013-5065']\n",
      "Group ID: G0117 - Unique CVEs: ['CVE-2018-1579']\n",
      "Group ID: G0060 - Unique CVEs: ['CVE-2016-7836']\n",
      "Group ID: G0066 - Unique CVEs: ['CVE-2012-1535', 'CVE-2012-0779', 'CVE-2011-0609', 'CVE-2012-1875', 'CVE-2010-0249', 'CVE-2011-2110']\n",
      "Group ID: G0001 - Unique CVEs: ['CVE-2011-2462', 'CVE-2013-3893', 'CVE-2013-3163']\n",
      "Group ID: G0107 - Unique CVEs: ['CVE-2016-0051']\n",
      "Group ID: G1002 - Unique CVEs: ['CVE-2021-28310']\n",
      "Group ID: G1019 - Unique CVEs: ['CVE-2022-27926']\n",
      "Group ID: G0008 - Unique CVEs: ['CVE-2013-3660']\n",
      "Group ID: G1024 - Unique CVEs: ['CVE-2019-6693', 'CVE-2023-35078', 'CVE-2022-40684']\n",
      "Group ID: G0067 - Unique CVEs: ['CVE-2018-8120', 'CVE-2013-0808', 'CVE-2021-26411', 'CVE-2016-0147', 'CVE-2020-1380']\n",
      "Group ID: G0128 - Unique CVEs: ['CVE-2017-0005', 'CVE-2013-3128', 'CVE-2011-3402']\n",
      "Group ID: G0082 - Unique CVEs: ['CVE-2015-6585']\n",
      "Group ID: G0068 - Unique CVEs: ['CVE-2015-2546', 'CVE-2013-1331']\n",
      "Group ID: G0096 - Unique CVEs: ['CVE-2019-3369', 'CVE-2021-44207']\n",
      "Group ID: G0020 - Unique CVEs: ['CVE-2012-0159', 'CVE-2013-3918']\n",
      "Group ID: G0123 - Unique CVEs: ['CVE-2012-3152', 'CVE-2019-11581']\n",
      "Group ID: G1016 - Unique CVEs: ['CVE-2001-0507', 'CVE-2010-5326', 'CVE-2015-7450', 'CVE-2017-100048']\n",
      "Group ID: G0087 - Unique CVEs: ['CVE-2019-13720']\n",
      "Group ID: G0063 - Unique CVEs: ['CVE-2017-11292']\n"
     ]
    }
   ],
   "source": [
    "unique_cves_per_group = {}\n",
    "\n",
    "for group_id, data in group_cve_map.items():\n",
    "    group_cves = set()\n",
    "    for hash_entry in data['hashes']:\n",
    "        group_cves.update(hash_entry['cves'])\n",
    "\n",
    "    # Find CVEs unique to this group (not shared with any other group)\n",
    "    other_groups_cves = set()\n",
    "    for other_group_id, other_data in group_cve_map.items():\n",
    "        if other_group_id != group_id:\n",
    "            for other_hash_entry in other_data['hashes']:\n",
    "                other_groups_cves.update(other_hash_entry['cves'])\n",
    "\n",
    "    unique_cves = group_cves - other_groups_cves\n",
    "    if unique_cves:\n",
    "        unique_cves_per_group[group_id] = list(unique_cves)\n",
    "\n",
    "print(len(unique_cves_per_group))\n",
    "# Output unique CVEs per group\n",
    "for group_id, unique_cves in unique_cves_per_group.items():\n",
    "    print(f\"Group ID: {group_id} - Unique CVEs: {unique_cves}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
