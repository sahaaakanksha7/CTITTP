{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0f516d4-d62b-41ad-b658-8ff365b39220",
   "metadata": {},
   "outputs": [],
   "source": [
    "##17 hashes are duplicated i.e, have duplicate URLs and have been downled as a same report with same TTPs as two different hashes -> 120 - 17 is the true ground truth\n",
    "# Function to find unique hashes with URLs and track removed hashes\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "393ddb2c-ab87-4721-a441-e9cfe70347e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of URLs: 1372\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of URLs in the file\n",
    "def count_urls(file_path):\n",
    "    url_count = 0\n",
    "    \n",
    "    # Open and read the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Count non-empty lines (assuming each line contains one URL)\n",
    "            if line.strip():  # Strip any whitespace and check if the line is not empty\n",
    "                url_count += 1\n",
    "                \n",
    "    return url_count\n",
    "\n",
    "file_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\downloads\\downloads\\20240930_urls.txt\"\n",
    "\n",
    "# Call the function and print the result\n",
    "print(f'Total number of URLs: {count_urls(file_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9a0b61c8-5f29-4bae-83b4-929168727b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_jsonl_to_dict(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and converts it into a list of dictionaries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the JSONL file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list of dict\n",
    "        A list of dictionaries where each dictionary represents one line in the JSONL file.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            # Parse each line into a dictionary and append to the list\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fc90ac44-b174-495c-8cf8-795fe2fea912",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries with duplicate URLs: 395\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def find_duplicate_urls(merged_data):\n",
    "    \"\"\"\n",
    "    Identifies entries with duplicate URLs and returns their associated download_sha256 values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    merged_data : list of dict\n",
    "        The merged JSONL data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where each key is a URL and the value is a list of download_sha256\n",
    "        values for entries with the same URL.\n",
    "    \"\"\"\n",
    "    # Dictionary to store URLs as keys and lists of download_sha256 as values\n",
    "    url_to_sha256 = defaultdict(list)\n",
    "    \n",
    "    # Iterate through all entries in the merged data\n",
    "    for entry in merged_data:\n",
    "        url = entry.get('url')  # Safely get the 'url', if it exists\n",
    "        download_sha256 = entry.get('download_sha256')  # Safely get the 'download_sha256'\n",
    "        \n",
    "        # Only proceed if both url and download_sha256 are present\n",
    "        if url and download_sha256:\n",
    "            # Add the download_sha256 to the list corresponding to the URL\n",
    "            url_to_sha256[url].append(download_sha256)\n",
    "    \n",
    "    # Filter to keep only URLs with more than one associated download_sha256 (duplicate URLs)\n",
    "    duplicate_urls = {url: sha256s for url, sha256s in url_to_sha256.items() if len(sha256s) > 1}\n",
    "    \n",
    "    return duplicate_urls\n",
    "\n",
    "\n",
    "# Find and print duplicate URLs and their download_sha256 values\n",
    "duplicate_urls = find_duplicate_urls(data)\n",
    "\n",
    "if duplicate_urls:\n",
    "    #print(\"Entries with duplicate URLs:\")\n",
    "    print(f\"Total entries with duplicate URLs: {len(duplicate_urls)}\")\n",
    "    #for url, sha256s in duplicate_urls.items():\n",
    "     #   print(f\"URL: {url}\")\n",
    "     #   print(f\"Download SHA256 values: {sha256s}\\n\")\n",
    "else:\n",
    "    print(\"No duplicate URLs found.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "42c20d1b-8153-4bc3-ac09-948b66b53623",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total MITRE JSONL entries: 1392\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "file_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\downloads\\downloads\\20241008_downloads.jsonl\"\n",
    "data = read_jsonl_to_dict(file_path)\n",
    "print(f\"Total MITRE JSONL entries: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "960e8fc3-4134-4093-a316-6f859bd32816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 0252a19e0a9ba187823828530c0117a4be0a99717ff643c72f54b8db09a63b68.download not found in the folder i.e., not downloaded\n",
      "File 0252a19e0a9ba187823828530c0117a4be0a99717ff643c72f54b8db09a63b68.download not found in the folder i.e., not downloaded\n",
      "Could not find exactly two matching files or more that two files for http://cdn0.vox-cdn.com/assets/4589853/crowdstrike-intelligence-report-putter-panda.original.pdf.\n",
      "Could not find exactly two matching files or more that two files for https://www.mandiant.com/sites/default/files/2022-02/rt-apt41-dual-operation.pdf.\n",
      "Could not find exactly two matching files or more that two files for https://www.secureworks.com/research/threat-profiles/iron-viking.\n",
      "Could not find exactly two matching files or more that two files for https://blog.talosintelligence.com/2020/03/bisonal-10-years-of-play.html.\n",
      "\n",
      "Total identical files found: 266\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def compare_files_content(file_path1, file_path2):\n",
    "    \"\"\"\n",
    "    Compare the contents of two files to see if they are identical.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path1 : str\n",
    "        Path to the first file.\n",
    "    file_path2 : str\n",
    "        Path to the second file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    bool\n",
    "        True if the files have identical content, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path1, 'rb') as file1, open(file_path2, 'rb') as file2:\n",
    "            # Compare the files byte by byte\n",
    "            return file1.read() == file2.read()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"File not found: {file_path1} or {file_path2}\")\n",
    "        return False\n",
    "\n",
    "def check_duplicate_files_content(folder_path, duplicate_urls):\n",
    "    \"\"\"\n",
    "    Check if the downloaded files corresponding to the given SHA256 hashes have the same content.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        The primary folder containing the downloaded files.\n",
    "    duplicate_urls : dict\n",
    "        A dictionary with URLs as keys and lists of SHA256 hashes as values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    identical_files_count = 0\n",
    "    \n",
    "    for url, sha256_list in duplicate_urls.items():\n",
    "        #print(f\"Checking duplicate files for URL: {url}\")\n",
    "        \n",
    "        matched_files = []\n",
    "   \n",
    "        \n",
    "        # Loop through the folder and find files that match the SHA256 hashes\n",
    "        for sha256_hash in sha256_list:\n",
    "            file_name = f\"{sha256_hash}.download\"  # Assuming the file name is based on the hash\n",
    "            file_path_primary = os.path.join(folder_path, file_name)\n",
    "            \n",
    "            # Check if the file exists in the primary folder\n",
    "            if os.path.exists(file_path_primary):\n",
    "                matched_files.append(file_path_primary)\n",
    "\n",
    "            else:\n",
    "                print(f\"File {file_name} not found in the folder i.e., not downloaded\")\n",
    "        \n",
    "        # If there are exactly two matched files, compare their content\n",
    "        if len(matched_files) == 2:\n",
    "            file1, file2 = matched_files\n",
    "            #print(f\"Comparing {file1} and {file2}\")\n",
    "            if compare_files_content(file1, file2):\n",
    "                identical_files_count += 1\n",
    "                ##print(f\"Files {file1} and {file2} have identical content.\")\n",
    "            #else:\n",
    "            #    print(f\"Files {file1} and {file2} have different content.\")\n",
    "        else:\n",
    "            print(f\"Could not find exactly two matching files or more that two files for {url}.\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    print(f\"\\nTotal identical files found: {identical_files_count}\")\n",
    "\n",
    "# Example usage:\n",
    "folder_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\downloads\\downloads\\20241008_downloads\"\n",
    "\n",
    "check_duplicate_files_content(folder_path, duplicate_urls)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "47d5c340-c5b7-4bf2-8708-57d4ccc9134e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in both folders: 900\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def count_files_in_folders(folder_path):\n",
    "    \"\"\"\n",
    "    Count the number of files in the given folder paths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        The primary folder containing the files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The total count of files in both folders.\n",
    "    \"\"\"\n",
    "    # Count files in the primary folder\n",
    "    primary_folder_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "\n",
    "    # Calculate the total count of files \n",
    "    total_files_count = len(primary_folder_files) \n",
    "\n",
    "    print(f\"Total number of files in both folders: {total_files_count}\")\n",
    "\n",
    "    return total_files_count\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "#folder_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\downloads\\downloads\\20241008_downloads\"\n",
    "# Get total file count\n",
    "total_files = count_files_in_folders(folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "19ac22b2-77b1-4c04-b9cb-1764cc486260",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in both folders: 120\n"
     ]
    }
   ],
   "source": [
    "iocs_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\downloads\\downloads\\iocs2\"\n",
    "# Get total file count\n",
    "total_files = count_files_in_folders(iocs_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "33522473-2764-47dc-8a4e-e91870d481d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Unique TTPs found: 461\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def count_cumulative_unique_ttps(folder_path):\n",
    "    \"\"\"\n",
    "    Count cumulative unique TTP IDs across all files in the given folder paths.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        Path to the first folder.\n",
    "    secondary_folder_path : str\n",
    "        Path to the second folder.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    set\n",
    "        A set of unique TTP IDs found in both folders.\n",
    "    \"\"\"\n",
    "    unique_ttps = set()\n",
    "\n",
    "    # Regex pattern to capture full TTP IDs (e.g., T1003, T1003.001)\n",
    "    ttp_pattern = re.compile(r'\\bT\\d{4}(?:\\.\\d+)?\\b')\n",
    "\n",
    "    # Loop through both folders\n",
    "    for folder in [folder_path]:\n",
    "        for file_name in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, file_name)\n",
    "\n",
    "            # Skip hidden or backup files\n",
    "            if file_name.startswith('_'):\n",
    "                print(f\"Skipping hidden or backup file: {file_name}\")\n",
    "                continue\n",
    "\n",
    "            if os.path.isfile(file_path):\n",
    "                #print(f\"Reading TTPs from file: {file_name}\")\n",
    "                try:\n",
    "                    # Open the file and check each line for TTP patterns\n",
    "                    with open(file_path, 'r', errors='ignore') as file:\n",
    "                        for line in file:\n",
    "                            # Find all TTP IDs in the line and add them directly to the cumulative set\n",
    "                            matches = ttp_pattern.findall(line)\n",
    "                            unique_ttps.update(matches)  # Add each unique TTP ID found in the line\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_name}: {e}\")\n",
    "    \n",
    "    return unique_ttps\n",
    "\n",
    "# Example usage\n",
    "folder_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\downloads\\downloads\\iocs2\"\n",
    "\n",
    "\n",
    "unique_ttps = count_cumulative_unique_ttps(folder_path)\n",
    "\n",
    "# Display the unique TTPs\n",
    "print(f\"Cumulative Unique TTPs found: {len(unique_ttps)}\")\n",
    "#for ttp in unique_ttps:\n",
    "#   print(ttp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd1a55-1d93-4636-8cd2-6d0cb23b975e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649adb7-db43-49f2-a5e7-f8c00dbb8a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
