{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "299fe012-87e2-4e8d-8be8-ab78bf6a45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "def extract_citations(citations_raw):\n",
    "    \"\"\"\n",
    "    This function takes a raw string of citations, extracts individual citations,\n",
    "    removes duplicates, cleans up irrelevant characters, and ensures no extra spaces or invalid characters.\n",
    "\n",
    "    Args:\n",
    "        citations_raw (str): The raw citation string from which citations are to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique citations.\n",
    "    \"\"\"\n",
    "    #print(citations_raw)\n",
    "    if pd.isna(citations_raw) or citations_raw == '':\n",
    "        return []\n",
    "\n",
    "    # Replace any occurrence of 'Citation: ' to standardize the string\n",
    "    citations_raw = citations_raw.replace(\"Citation: \", \"\")\n",
    "    \n",
    "    # Split citations by both the delimiters '), (' and ')('\n",
    "    citations = re.split(r\"\\)\\s*,?\\s*\\(|\\)\\(\", citations_raw)\n",
    "\n",
    "    # Clean each citation by removing unwanted characters and extra spaces\n",
    "    cleaned_citations = []\n",
    "    for citation in citations:\n",
    "        citation = citation.replace(\"(\", \"\").replace(\")\", \"\").strip(\", \").strip()\n",
    "        \n",
    "        # Use regex to extract only the relevant citation portion before any period or embedded sentence\n",
    "        # Here, we match the first part (e.g., \"US-CERT HIDDEN COBRA June 2017\") and ignore the rest\n",
    "        citation = re.sub(r'\\s*\\..*$', '', citation)  # Remove everything after the first period\n",
    "        \n",
    "        # We also use another regex to handle extra commas, which might still linger after cleaning\n",
    "        citation = re.sub(r',\\s*$', '', citation)  # Remove comma at the end of citation\n",
    "        \n",
    "        if citation:  # Only add to the list if there is any valid citation\n",
    "            cleaned_citations.append(citation)\n",
    "    \n",
    "    # Remove duplicates by converting the list to a set and back to a list\n",
    "    cleaned_citations = list(set(cleaned_citations))\n",
    "\n",
    "    #print(cleaned_citations)\n",
    "\n",
    "    return cleaned_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "63c16354-3311-48ad-bf4e-4759da5165c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enterprise Groups File Path: ..\\ATTACK Excel sheets\\enterprise-attack-v15.1-groups.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Load configuration from the JSON file located outside the current directory\n",
    "config_file_path = os.path.join(\"..\", \"Malpedia Bib files Analysis\", \"config.json\")\n",
    "\n",
    "with open(config_file_path, \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Step 2: Extract the data directory and file paths for groups and software from the configuration\n",
    "data_directory = config[\"data_directory\"]\n",
    "groups_file_paths = {key: os.path.normpath(os.path.join(data_directory, value)) for key, value in config[\"file_paths_groups_v15\"].items()}\n",
    "\n",
    "# Example to access one of the group files (you can select enterprise, ics, or mobile)\n",
    "enterprise_groups_file = groups_file_paths['enterprise']\n",
    "print(f\"Enterprise Groups File Path: {enterprise_groups_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "07fd6e24-3d8a-47bf-ae90-7af77a711d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load Excel sheets for groups and citations\n",
    "df_groups = pd.read_excel(enterprise_groups_file, sheet_name='groups')\n",
    "df_citations = pd.read_excel(enterprise_groups_file, sheet_name='citations')\n",
    "\n",
    "# Step 4: Extract citation key and URL mapping from 'citations' sheet\n",
    "citation_map = dict(zip(df_citations['reference'], df_citations['url']))\n",
    "\n",
    "# Step 5: Initialize a dictionary to store the final data structure\n",
    "group_citations_map = {}\n",
    "\n",
    "# Process each group in the 'groups' sheet\n",
    "for index, row in df_groups.iterrows():\n",
    "    group_id = row['ID']\n",
    "    \n",
    "    # Extract and clean associated groups citations\n",
    "    associated_citations_raw = row['associated groups citations']\n",
    "    associated_citations = extract_citations(associated_citations_raw) if pd.notna(associated_citations_raw) else []\n",
    "    \n",
    "    # Extract and clean relationship citations\n",
    "    relationship_citations_raw = row['relationship citations']\n",
    "    relationship_citations = extract_citations(relationship_citations_raw) if pd.notna(relationship_citations_raw) else []\n",
    "    \n",
    "    # Map citations to URLs with uniqueness check\n",
    "    associated_citations_with_urls = {}\n",
    "    for citation in associated_citations:\n",
    "        url = citation_map.get(citation, \"URL not found\")\n",
    "        # Only add to map if citation and URL are not already present\n",
    "        if citation not in associated_citations_with_urls:\n",
    "            associated_citations_with_urls[citation] = url\n",
    "\n",
    "    relationship_citations_with_urls = {}\n",
    "    for citation in relationship_citations:\n",
    "        url = citation_map.get(citation, \"URL not found\")\n",
    "        if citation not in relationship_citations_with_urls:\n",
    "            relationship_citations_with_urls[citation] = url\n",
    "    \n",
    "    # Store the data in the final structure\n",
    "    group_citations_map[group_id] = {\n",
    "        \"associated_groups_citations\": associated_citations_with_urls,\n",
    "        \"relationship_citations\": relationship_citations_with_urls\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "75073e71-2433-49d3-866c-0578170a021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of URLs across the whole dictionary: 1176\n",
      "Totak number of groups: 148\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "total_urls = 0  # Total number of URLs across both associated_groups_citations and relationship_citations\n",
    "\n",
    "# Iterate through each group in the dictionary\n",
    "for group_id, group_data in group_citations_map.items():\n",
    "    # Count URLs in 'associated_groups_citations'\n",
    "    associated_urls = group_data.get('associated_groups_citations', {})\n",
    "    total_urls += len(associated_urls)  # Add the number of URLs in associated_groups_citations\n",
    "    \n",
    "    # Count URLs in 'relationship_citations'\n",
    "    relationship_urls = group_data.get('relationship_citations', {})\n",
    "    total_urls += len(relationship_urls)  # Add the number of URLs in relationship_citations\n",
    "\n",
    "# Output the total number of URLs\n",
    "print(f\"Total number of URLs across the whole dictionary: {total_urls}\")\n",
    "num_groups = len(group_citations_map)  \n",
    "print(f\"Totak number of groups: {num_groups}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "54a01859-582c-41ea-9bc6-6a01bb8fb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load JSONL data\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Loads a JSONL file into a list of dictionaries.\n",
    "    Each line of the file is parsed as a JSON object.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = [json.loads(line) for line in file]\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f3b3f510-4d65-4326-a3ff-93e53d786aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_file_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\downloads\\downloads\\20241008_downloads.jsonl\"\n",
    "\n",
    "# Step 1: Load the JSONL data\n",
    "jsonl_data = load_jsonl(jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "fe100250-bb9b-4725-b761-68d96a618d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_hash_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extracts the file hash from a given filename.\n",
    "    \n",
    "    - If the filename ends with '.download.iocs', it removes this suffix.\n",
    "    - Otherwise, if the filename ends with '.iocs', it removes only '.iocs'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The input filename.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or None\n",
    "        The extracted file hash, or None if the filename is invalid.\n",
    "    \"\"\"\n",
    "    if filename.endswith('.download.iocs'):\n",
    "        return filename[:-14]  # Remove '.download.iocs' (14 characters)\n",
    "\n",
    "    if filename.endswith('.iocs'):\n",
    "        return filename[:-5]  # Remove '.iocs'\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ad4d217e-5c36-4811-9c74-52f99b5c0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where the CVE files are stored\n",
    "folder_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\CVEs\"\n",
    "\n",
    "# Folder where the TTP files are stored\n",
    "#folder_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\downloads\\downloads\\iocs2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fda59e75-205b-47b3-81a5-36b93831a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store file hash -> URL mapping\n",
    "hash_to_url_map = {}\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Extract file hash from the filename\n",
    "    file_hash = extract_file_hash_from_filename(filename)\n",
    "    if file_hash:\n",
    "        # Look for the matching download_sha256 in the JSONL data\n",
    "        for entry in jsonl_data:\n",
    "            if entry.get('download_sha256') == file_hash:\n",
    "                # If the hashes match, store the URL\n",
    "                hash_to_url_map[file_hash] = entry.get('url')\n",
    "                break  # Exit loop once a match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "cc763645-8ae4-4326-9202-e023567e519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "263\n"
     ]
    }
   ],
   "source": [
    "print(len(hash_to_url_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fe85b757-6ade-4f99-a659-aae5532dc9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary to store the hash, group_id, and URL\n",
    "hash_group_url_map = {}\n",
    "\n",
    "# Iterate through each hash, url pair in hash_to_url_map\n",
    "for file_hash, url in hash_to_url_map.items():\n",
    "    # Iterate through each group in group_citations_map\n",
    "    for group_id, group_data in group_citations_map.items():\n",
    "        # Search for the URL in associated_groups_citations\n",
    "        if url in group_data.get('associated_groups_citations', {}).values():\n",
    "            # If the URL is found, add the file_hash, group_id, and URL to the hash_group_url_map\n",
    "            hash_group_url_map[file_hash] = {'group_id': group_id, 'url': url}\n",
    "            break  # Exit once we find the match (no need to check further in this group)\n",
    "\n",
    "        # Search for the URL in relationship_citations\n",
    "        if url in group_data.get('relationship_citations', {}).values():\n",
    "            # If the URL is found, add the file_hash, group_id, and URL to the hash_group_url_map\n",
    "            hash_group_url_map[file_hash] = {'group_id': group_id, 'url': url}\n",
    "            break  # Exit once we find the match (no need to check further in this group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5edd10df-eaec-4272-9a1f-825083b380f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "255"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hash_group_url_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "a42c57e3-fe8b-45ca-bea1-d6527ba9e206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash: 05bc7a68fdfe54c95ba1fb7360f2cb73bbfbdbbe939c29d764abf59f975a6a3a, URL: https://www.infosecurity-magazine.com/news/microsoft-zero-day-traced-russian/\n",
      "Hash: 2c9e582e0194bacc4e4bbb37ffe61ed7e89af5cc5748fdc001e9dd65ddfaa32f, URL: https://securelist.com/apt-trends-report-q1-2018/85280/\n",
      "Hash: 5b6328ed41cb49229d8d47046caabae1fdb90045c467d6509ae1f459a9b5b518, URL: https://www.intezer.com/wp-content/uploads/2021/09/TeamTNT-Cryptomining-Explosion.pdf\n",
      "Hash: 72beb22ceed285d666ec7912dfcb95e7107c4232e622026915ef1bcd3c593490, URL: https://unit42.paloaltonetworks.com/ukraine-targeted-outsteel-saintbot/\n",
      "Hash: 73eac7a13e4c15ce849d7a12a8d56eb3d831b6b442bf9ce7bc43afc1caafde9c, URL: https://www.us-cert.gov/ncas/alerts/TA17-164A\n",
      "Hash: cabd66802a057829a0113bc5e53ac0c2c48f91142e8a40e10aac0d9d6aebbe98, URL: https://www.bleepingcomputer.com/news/security/ukraine-links-members-of-gamaredon-hacker-group-to-russian-fsb/\n",
      "Hash: e2f84d3c77547f31ba782c0bb5525980059f651931e2b1dbbcd0a81f4430a1db, URL: https://securelist.com/the-naikon-apt/69953/\n",
      "Hash: e6af21bf6c751aabe190006f4f60b2b2a4164c6cb1bf34cc461b4f116eaf479d, URL: https://www.cyberscoop.com/middle-eastern-hacking-group-using-finfisher-malware-conduct-international-espionage/\n"
     ]
    }
   ],
   "source": [
    "# Find the hashes that are in hash_to_url_map but not in hash_group_url_map\n",
    "missing_hashes = {key: value for key, value in hash_to_url_map.items() if key not in hash_group_url_map}\n",
    "\n",
    "# Print the list of hashes and URLs that are missing in hash_group_url_map\n",
    "for hash_key, url in missing_hashes.items():\n",
    "    print(f\"Hash: {hash_key}, URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b59185ef-1fab-4378-8336-d736bd3f2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cves_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all the files in a specified folder with .iocs extension and creates a dictionary\n",
    "    with the file hash (from the filename) as the key and the list of CVEs as the value.\n",
    "    It also removes the 'cve' prefix from each CVE and handles the file content appropriately.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the .iocs files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the key is the file hash and the value is a list of CVEs.\n",
    "    \"\"\"\n",
    "    iocs_dict = {}\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a '.iocs' extension\n",
    "        if filename.endswith(\".iocs\"):\n",
    "            file_hash = filename.split(\".\")[0]  # Extract file hash from the filename\n",
    "            \n",
    "            # Construct the full path to the file\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            try:\n",
    "                # Open the file and read its content\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read().strip()  # Read and strip any unwanted whitespace\n",
    "\n",
    "                    # Remove \"cve\" prefix and the tab character, and store CVEs in a list\n",
    "                    content_lines = content.split('\\n')  # Split content by lines\n",
    "                    cleaned_content = []\n",
    "                    for line in content_lines:\n",
    "                        if line.startswith(\"cve\"):\n",
    "                            # Remove 'cve' and the tab character '\\t'\n",
    "                            cleaned_line = line[4:].strip()  # Strip the 'cve' prefix and any leading/trailing spaces\n",
    "                            cleaned_content.append(cleaned_line)\n",
    "\n",
    "                # Store the file hash and list of CVEs in the dictionary\n",
    "                iocs_dict[file_hash] = cleaned_content\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {filename}: {e}\")\n",
    "\n",
    "    return iocs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0ed43b1b-9b1b-4afd-9680-ebd322bc170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\CVEs\"  \n",
    "iocs_data = read_cves_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "b64a82b4-03b4-4621-be1a-4d22bbf89221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ttps_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all .download.iocs files in a specified folder and extracts TTPs (Technique IDs).\n",
    "    \n",
    "    - Extracts the file hash from the filename.\n",
    "    - Parses the file to extract TTPs (entries starting with 'ttp\\tT').\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        The path to the folder containing .download.iocs files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where keys are file hashes and values are lists of extracted TTPs.\n",
    "    \"\"\"\n",
    "    ttps_dict = {}\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".download.iocs\"):\n",
    "            file_hash = extract_file_hash_from_filename(filename)\n",
    "            \n",
    "            if not file_hash:\n",
    "                print(f\"Skipping invalid filename: {filename}\")\n",
    "                continue  # Skip files that don't match expected format\n",
    "            \n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    # Extract TTPs from each line that starts with 'ttp\\tT'\n",
    "                    ttps = [\n",
    "                        line.split('\\t')[1].split(' ')[0]  # Extract only the TTP ID (e.g., T1204)\n",
    "                        for line in file.read().strip().split('\\n')\n",
    "                        if line.startswith(\"ttp\\tT\")\n",
    "                    ]\n",
    "\n",
    "                # Store results in dictionary\n",
    "                ttps_dict[file_hash] = ttps\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {filename}: {e}\")\n",
    "    \n",
    "    return ttps_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0bd7deb9-946e-43c2-b1b6-ce1f1778039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#folder_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\downloads\\downloads\\iocs2\"  \n",
    "#ttps_data = read_ttps_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2585a6de-ebca-47b6-919c-329e77224aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_hash_data(iocs_data, hash_group_url_map, data_type='cves'):\n",
    "    \"\"\"\n",
    "    Combines hash-based IOC data with group and URL information.\n",
    "    \n",
    "    Args:\n",
    "        iocs_data (dict): Dictionary where keys are file hashes and values are lists of CVEs or TTPs.\n",
    "        hash_group_url_map (dict): Dictionary mapping file hashes to group information (group_id, URL).\n",
    "        data_type (str): Type of data to process ('cves' or 'ttps').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are group IDs and values contain hashes and associated data.\n",
    "    \"\"\"\n",
    "    hash_data_map = {}\n",
    "    \n",
    "    # Step 1: Iterate through the hashes in iocs_data\n",
    "    for file_hash, data_list in iocs_data.items():\n",
    "        # Check if the file_hash exists in hash_group_url_map\n",
    "        if file_hash in hash_group_url_map:\n",
    "            group_info = hash_group_url_map[file_hash]\n",
    "            \n",
    "            # Combine the data: CVEs/TTPs, group_id, and URL\n",
    "            hash_data_map[file_hash] = {\n",
    "                data_type: data_list,  # 'cves' or 'ttps'\n",
    "                'group_id': group_info['group_id'],\n",
    "                'url': group_info['url']\n",
    "            }\n",
    "    \n",
    "    # Step 2: Transform hash_data_map into a group-based structure\n",
    "    group_data_map = {}\n",
    "    \n",
    "    for hash_val, data in hash_data_map.items():\n",
    "        group_id = data['group_id']\n",
    "        extracted_data = data[data_type]\n",
    "        url = data['url']\n",
    "        \n",
    "        # Initialize group entry if not present\n",
    "        if group_id not in group_data_map:\n",
    "            group_data_map[group_id] = {\n",
    "                'hashes': [],\n",
    "                'url': url,\n",
    "            }\n",
    "        \n",
    "        # Append hash and extracted data (CVEs or TTPs)\n",
    "        group_data_map[group_id]['hashes'].append({\n",
    "            'hash': hash_val,\n",
    "            data_type: extracted_data,\n",
    "        })\n",
    "    \n",
    "    return group_data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a1555d7b-772a-4295-b731-0121247a4ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CVEs\n",
    "group_cve_map = combine_hash_data(iocs_data, hash_group_url_map, data_type='cves')\n",
    "\n",
    "# For TTPs\n",
    "#group_ttp_map = combine_hash_data(ttps_data, hash_group_url_map, data_type='ttps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "55c20676-7562-44e6-a2b6-532b94ec5e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(group_cve_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "d3f48e6d-901b-4cf2-9900-093fa642a415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups in TTP data: 85\n"
     ]
    }
   ],
   "source": [
    "def dump_and_count_data(data, data_type='cves', file_prefix='MITRE'):\n",
    "    \"\"\"\n",
    "    Dumps the provided data to a JSON file and counts the number of keys.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The data dictionary to be saved.\n",
    "        data_type (str): The type of data ('cves' or 'ttps').\n",
    "        file_prefix (str): Prefix for the filename.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of keys in the JSON file.\n",
    "    \"\"\"\n",
    "    # Define file name based on data type\n",
    "    file_path = f'{file_prefix}_{data_type}_group_analysis.json'\n",
    "    \n",
    "    # Dump data to JSON file\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    \n",
    "    # Read and count the number of keys in the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        loaded_data = json.load(file)\n",
    "    \n",
    "    return len(loaded_data)\n",
    "\n",
    "# Example usage:\n",
    "num_cve_keys = dump_and_count_data(group_cve_map, data_type='cves')\n",
    "#num_ttp_keys = dump_and_count_data(group_ttp_map, data_type='ttps')\n",
    "print(f\"Number of groups in TTP data: {num_cve_keys}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "869be641-8827-45fa-8ecd-706c082d25ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of CVES: 955\n",
      "Total number of unique CVES: 322\n",
      "Top 10 most common CVES:\n",
      "CVE-2012-0158: 29\n",
      "CVE-2017-11882: 22\n",
      "CVE-2017-0199: 21\n",
      "CVE-2022-38028: 15\n",
      "CVE-2024-3400: 12\n",
      "CVE-2021-26855: 11\n",
      "CVE-2021-27065: 11\n",
      "CVE-2010-3333: 10\n",
      "CVE-2014-6332: 10\n",
      "CVE-2018-13379: 10\n",
      "Number of groups with unique CVES: 47\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_data(group_data_map, data_type=\"cves\"):\n",
    "    \"\"\"\n",
    "    Analyzes the given group data (CVE or TTP) and provides statistical insights.\n",
    "\n",
    "    Args:\n",
    "        group_data_map (dict): Dictionary where keys are group IDs and values contain hashes and data lists.\n",
    "        data_type (str): Either \"cves\" or \"ttps\" to specify the type of data being analyzed.\n",
    "\n",
    "    Returns:\n",
    "        dict: Analysis results including total count, unique count, top 10 common items, and unique items per group.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store all items (CVE/TTPs) across groups\n",
    "    all_items = []\n",
    "    \n",
    "    # Extract all CVEs or TTPs from the data\n",
    "    for group_id, group_data in group_data_map.items():\n",
    "        for hash_data in group_data['hashes']:\n",
    "            all_items.extend(hash_data[data_type])\n",
    "    \n",
    "    # Calculate total and unique counts\n",
    "    total_items = len(all_items)\n",
    "    unique_items = set(all_items)\n",
    "    total_unique_items = len(unique_items)\n",
    "    \n",
    "    # Identify the top 10 most common CVEs or TTPs\n",
    "    item_counter = Counter(all_items)\n",
    "    top_10_items = item_counter.most_common(10)\n",
    "    \n",
    "    # Identify unique items per group\n",
    "    unique_items_per_group = {}\n",
    "    for group_id, data in group_data_map.items():\n",
    "        group_items = set()\n",
    "        for hash_entry in data['hashes']:\n",
    "            group_items.update(hash_entry[data_type])\n",
    "\n",
    "        # Find items unique to this group\n",
    "        other_groups_items = set()\n",
    "        for other_group_id, other_data in group_data_map.items():\n",
    "            if other_group_id != group_id:\n",
    "                for other_hash_entry in other_data['hashes']:\n",
    "                    other_groups_items.update(other_hash_entry[data_type])\n",
    "\n",
    "        unique_items = group_items - other_groups_items\n",
    "        if unique_items:\n",
    "            unique_items_per_group[group_id] = list(unique_items)\n",
    "    \n",
    "    # Print analysis results\n",
    "    print(f\"Total number of {data_type.upper()}: {total_items}\")\n",
    "    print(f\"Total number of unique {data_type.upper()}: {total_unique_items}\")\n",
    "    print(f\"Top 10 most common {data_type.upper()}:\")\n",
    "    for item, count in top_10_items:\n",
    "        print(f\"{item}: {count}\")\n",
    "    print(f\"Number of groups with unique {data_type.upper()}: {len(unique_items_per_group)}\")\n",
    "    \n",
    "    # Return analysis results as a dictionary\n",
    "    return {\n",
    "        \"total_count\": total_items,\n",
    "        \"unique_count\": total_unique_items,\n",
    "        \"top_10\": top_10_items,\n",
    "        \"unique_per_group\": unique_items_per_group\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "cve_analysis = analyze_data(group_cve_map, data_type=\"cves\")\n",
    "#ttp_analysis = analyze_data(group_ttp_map, data_type=\"ttps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071f05d-dfca-43d7-9a64-877c83b281f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
