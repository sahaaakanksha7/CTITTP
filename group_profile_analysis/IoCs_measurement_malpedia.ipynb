{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d2c4155-7a94-415b-8432-4678a107c320",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from statistics import mode\n",
    "from collections import Counter, defaultdict\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37145c57-eedb-4023-955d-b5804baaaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    # Get the absolute path of the project root (one directory up)\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "    # Normalize the project_root to ensure it's correctly formatted\n",
    "    project_root = os.path.normpath(project_root)\n",
    "    \n",
    "    config_path = os.path.join(project_root, 'config.json')\n",
    "\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Config file not found at expected location: {config_path}\")\n",
    "\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    return config, project_root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6871ff8-6e83-4068-9b42-4a7ea557e096",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, project_root = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de0429cd-4d6a-4187-a66e-c8964be68725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ricewater\\\\Documents\\\\CTITTP'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "project_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89acccec-3cec-4af5-9fd3-015f6f07bf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read JSONL file\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns a list of JSON objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the JSONL file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of JSON objects.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c51e32e-0925-484b-8ea6-4927c0c595f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 JSONL path: C:\\Users\\ricewater\\Documents\\CTIDownloads\\malpedia_20220718\\malpedia_20220718\\malpedia-db_2022-07-18_downloader.jsonl\n",
      "2024 JSONL path: C:\\Users\\ricewater\\Documents\\CTIDownloads\\20241204_malpedia_downloads\\20241204_downloads.jsonl\n",
      "Total entries combined: 15768\n",
      "First entry:\n",
      "{'author': 'CERT Division', 'date': '2000', 'download_mime': 'text/html', 'download_redirects': ['https://resources.sei.cmu.edu/library/asset-view.cfm?assetID=496186'], 'download_sha256': 'f2c405b383ebaf4d0793f8d5162841b953d06947a711f7d34242faa20e285a04', 'download_size': 41745, 'download_status': 200, 'download_ts': '2022-07-19 12:43:41.400938+00:00', 'language': 'English', 'organization': 'Carnegie Mellon University', 'origin': ['malpedia:CarnegieMellonUniversity'], 'title': '2000 CERT Advisories', 'url': 'https://resources.sei.cmu.edu/library/asset-view.cfm?assetID=496186'}\n"
     ]
    }
   ],
   "source": [
    "# Construct full paths to the JSONL files using the config\n",
    "malpedia_2022_path = os.path.normpath(os.path.join(project_root, config['jsonl_files']['Malpedia_2022']))\n",
    "malpedia_2024_path = os.path.normpath(os.path.join(project_root, config['jsonl_files']['Malpedia_2024']))\n",
    "\n",
    "# Print to verify\n",
    "print(f\"2022 JSONL path: {malpedia_2022_path}\")\n",
    "print(f\"2024 JSONL path: {malpedia_2024_path}\")\n",
    "\n",
    "# Read the JSONL files\n",
    "malpedia_2022_data = read_jsonl(malpedia_2022_path)\n",
    "malpedia_2024_data = read_jsonl(malpedia_2024_path)\n",
    "\n",
    "# Combine the entries\n",
    "combined_data = malpedia_2022_data + malpedia_2024_data\n",
    "\n",
    "# Output summary\n",
    "print(f\"Total entries combined: {len(combined_data)}\")\n",
    "print(\"First entry:\")\n",
    "print(combined_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a2d91794-3051-4577-981e-a7a6cb6b9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hashes(directories):\n",
    "    \"\"\"\n",
    "    Extracts file hashes from filenames in the given directories.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directories : list\n",
    "        A list of directory paths containing the files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of file hashes extracted from the filenames.\n",
    "    \"\"\"\n",
    "    file_hashes = []\n",
    "    \n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".iocs\"):\n",
    "                # Extract the hash from the filename (everything before '.iocs')\n",
    "                file_hash = filename.split('.')[0]\n",
    "                file_hashes.append(file_hash)\n",
    "\n",
    "            elif filename.endswith(\".download.iocs\"):\n",
    "                # Remove the '.download.iocs' suffix from the filename\n",
    "                file_hash = filename.removesuffix(\".download.iocs\")\n",
    "                file_hashes.append(file_hash)\n",
    "\n",
    "    # Remove empty values or invalid hashes\n",
    "    #file_hashes = [h for h in file_hashes if h.strip()]\n",
    "    \n",
    "    return file_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "00b63530-a006-4fbd-b4d0-1b4de9322862",
   "metadata": {},
   "outputs": [],
   "source": [
    "### use CVE_Malpedia if querying CVE and TTP_Malpedia\n",
    "# Get and normalize directories from config\n",
    "directories = [\n",
    "    os.path.normpath(os.path.join(project_root, config[\"directory_paths_ioc\"][\"CVE_Malpedia_2022\"])),\n",
    "    os.path.normpath(os.path.join(project_root, config[\"directory_paths_ioc\"][\"CVE_Malpedia_2024\"]))\n",
    "]\n",
    "\n",
    "# Extract file hashes from the directory\n",
    "file_hashes = get_file_hashes(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5728c597-1ddd-439d-9650-6983274dbfc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2837"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(file_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc687316-3c0d-43cf-b5ae-d2ce21f3097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map file hash to url\n",
    "def create_hash_to_url_map(file_hashes, combined_data):\n",
    "    \"\"\"\n",
    "    Maps file hashes to URLs based on the download_sha256 field in the combined_data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_hashes : list\n",
    "        A list of file hashes.\n",
    "    combined_data : list\n",
    "        A list of dictionaries (combined JSONL data) with download_sha256 and url fields.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary mapping file hashes to URLs.\n",
    "    \"\"\"\n",
    "    hash_to_url = {}\n",
    "    for file_hash in file_hashes:\n",
    "        for entry in combined_data:\n",
    "            if entry.get(\"download_sha256\") == file_hash:\n",
    "                hash_to_url[file_hash] = entry.get(\"url\")\n",
    "                break  # Stop searching once we find the match for the hash\n",
    "    return hash_to_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b022e619-2036-455a-8950-1e896dd95cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000c1ee0c1bab222569623c47397f634b77c4124ad8bb0b0d2533ec98fcc6a16 https://attack.mitre.org/groups/G0046/\n"
     ]
    }
   ],
   "source": [
    "# Create the mapping of file hashes to URLs\n",
    "hash_to_url_map = create_hash_to_url_map(file_hashes, combined_data)\n",
    "\n",
    "first_key, first_value = next(iter(hash_to_url_map.items()))\n",
    "print(first_key, first_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b5935f3f-8949-4b19-95d4-8c540e702211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.clearskysec.com/wp-content/uploads/2021/01/Lebanese-Cedar-APT.pdf'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_to_url_map.get(\"cb10915f45e3c27ccd203dd3f69aad162802d8db568c9010ee696ff631caa41e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b07c37b6-cd5f-457a-9c92-e957998a4c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique file hashes: 2834\n",
      "Total URLs: 2834\n",
      "Total unique URLs: 2813\n",
      "There are 21 duplicate URLs.\n"
     ]
    }
   ],
   "source": [
    "##Common file hashes found: {'bd90e5d64d43cd326049d739d519c270d9f2856db6c1d140569f152b0fa3b757', \n",
    "##'acd626acf50af8e30a681ccf88662b2bcecd5ec6053c18d6b460a42d9d726764', \n",
    "##'a71555ff127721ad3f47e0427411dde35ec792889c2778ba43571d3a4b3f5cca'}\n",
    "unique_hashes = len(hash_to_url_map)\n",
    "print(f\"Total unique file hashes: {unique_hashes}\")\n",
    "\n",
    "# Check if the URLs are unique\n",
    "urls = list(hash_to_url_map.values())\n",
    "unique_urls = len(set(urls))\n",
    "\n",
    "print(f\"Total URLs: {len(urls)}\")\n",
    "print(f\"Total unique URLs: {unique_urls}\")\n",
    "\n",
    "# Check if there are any duplicate URLs\n",
    "if len(urls) == unique_urls:\n",
    "    print(\"All URLs are unique.\")\n",
    "else:\n",
    "    print(f\"There are {len(urls) - unique_urls} duplicate URLs.\")\n",
    "\n",
    "    # Find and print duplicate URLs\n",
    "    url_counts = {}\n",
    "    \n",
    "    # Count the occurrences of each URL\n",
    "    for url in urls:\n",
    "        if url in url_counts:\n",
    "            url_counts[url] += 1\n",
    "        else:\n",
    "            url_counts[url] = 1\n",
    "\n",
    "    # Filter out the duplicate URLs (those that appear more than once)\n",
    "    duplicate_urls = {url: count for url, count in url_counts.items() if count > 1}\n",
    "\n",
    "    # print(\"Duplicate URLs and their occurrences:\")\n",
    "    # for url, count in duplicate_urls.items():\n",
    "    #     print(f\"URL: {url} -> Occurrences: {count}\")\n",
    "\n",
    "    # #Find the hashes associated with the duplicate URLs\n",
    "    # print(\"\\nHashes associated with duplicate URLs:\")\n",
    "    # for hash_key, url in hash_to_url_map.items():\n",
    "    #     if url in duplicate_urls:\n",
    "    #         print(f\"Hash: {hash_key} -> URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "292cf82f-9d72-4a40-b38d-b42b59954669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the actors data from the actors_data.json file and malware families data from malware_families.json\n",
    "# Load JSON data\n",
    "def load_json(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63dc9150-71e3-4a06-9b2d-b6142a30f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize group names\n",
    "def normalize_group_name(name):\n",
    "    # Convert to lowercase for case-insensitive comparison\n",
    "    name = name.lower().strip()\n",
    "\n",
    "    # Remove 'team' from names like 'Sandworm Team'\n",
    "    if name.endswith(' team'):\n",
    "        name = name.replace(' team', '')\n",
    "\n",
    "    # Replace 'threat group-' with 'tg-' (e.g., 'Threat Group-1314' -> 'TG-1314')\n",
    "    name = re.sub(r'threat group[- ]', 'tg-', name)\n",
    "\n",
    "    # Remove 'temp.' or similar prefixes (e.g., 'Temp.Pittytiger' -> 'Pittytiger')\n",
    "    name = re.sub(r'^temp[\\. ]+', '', name)\n",
    "\n",
    "    # Normalize spaces and dots (e.g., 'pitty tiger' == 'pitty.tiger')\n",
    "    name = re.sub(r'[\\. ]+', ' ', name)\n",
    "\n",
    "    # Remove common suffixes like 'framework' or 'group' (e.g., 'Inception Framework' -> 'Inception')\n",
    "    name = re.sub(r' (framework|group)$', '', name)\n",
    "\n",
    "    # Standardize 'Confucius' and 'Confucious' to 'confucius'\n",
    "    name = re.sub(r'confucious', 'confucius', name)\n",
    "\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a665e81-2396-4e48-927a-5790d6dc7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url_to_group_map(actors_data, families_data):\n",
    "    \"\"\"\n",
    "    Creates a mapping from URLs to group names based on the actors and families data,\n",
    "    and records the origin of each mapping: 'actor', 'attribution', or 'family'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary mapping each URL to a dict with 'group' and 'source'.\n",
    "    \"\"\"\n",
    "    url_to_group_map = {}\n",
    "\n",
    "    # First: process actors_data\n",
    "    for group_name, group_info in actors_data.items():\n",
    "        refs = group_info.get(\"meta\", {}).get(\"refs\", [])\n",
    "        normalized_name = normalize_group_name(group_name)\n",
    "\n",
    "        for url in refs:\n",
    "            if url not in url_to_group_map:\n",
    "                url_to_group_map[url] = {\n",
    "                    \"group\": normalized_name,\n",
    "                    \"source\": \"actor\"\n",
    "                }\n",
    "            else:\n",
    "                # Support multiple mappings from different sources\n",
    "                existing = url_to_group_map[url]\n",
    "                if isinstance(existing[\"group\"], list):\n",
    "                    existing[\"group\"].append(normalized_name)\n",
    "                else:\n",
    "                    existing[\"group\"] = [existing[\"group\"], normalized_name]\n",
    "                # Keep the first source, or you can also store all sources as a list if needed\n",
    "\n",
    "    # Then: process families_data\n",
    "    for family_name, family_info in families_data.items():\n",
    "        for url in family_info.get(\"urls\", []):\n",
    "            if url in url_to_group_map:\n",
    "                continue  # already mapped via actor\n",
    "\n",
    "            attribution = family_info.get(\"attribution\", [])\n",
    "            if attribution:\n",
    "                mapped_names = [normalize_group_name(attr) for attr in attribution]\n",
    "                source_type = \"attribution\"\n",
    "            else:\n",
    "                mapped_names = [family_name]\n",
    "                source_type = \"family\"\n",
    "\n",
    "            group_value = mapped_names if len(mapped_names) > 1 else mapped_names[0]\n",
    "            url_to_group_map[url] = {\n",
    "                \"group\": group_value,\n",
    "                \"source\": source_type\n",
    "            }\n",
    "\n",
    "    return url_to_group_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "869c3f0c-0a8e-4413-a49b-1c05449ffdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define base path relative to current working directory\n",
    "base_dir = os.getcwd()\n",
    "api_data_dir = os.path.join(base_dir, \"malpedia_api_responses\")\n",
    "\n",
    "# Construct full paths\n",
    "actors_data_file = os.path.join(api_data_dir, \"actors_data.json\")\n",
    "families_data_file = os.path.join(api_data_dir, \"malware_families.json\")\n",
    "\n",
    "# # Load the data using helper\n",
    "actors_data = load_json(actors_data_file)\n",
    "families_data = load_json(families_data_file)\n",
    "\n",
    "# # Create URL-to-group mapping\n",
    "url_to_group_map = create_url_to_group_map(actors_data, families_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1a660966-b97a-4d21-ba3d-b0bef0a0b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_counter = Counter()\n",
    "for entry in url_to_group_map.values():\n",
    "    source = entry.get(\"source\", \"unknown\")\n",
    "    source_counter[source] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a49b522c-1140-4fbc-9fd1-36b0b5e17393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'family': 6640, 'attribution': 5104, 'actor': 2557})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6b37d303-1530-4c7e-9592-c7a70efd7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_group_to_hash_url_map(hash_to_url_map, url_to_group_map):\n",
    "    \"\"\"\n",
    "    Updates the group-to-hash-and-url map with group names as keys.\n",
    "    Each entry includes:\n",
    "      - list of file hashes\n",
    "      - list of URLs with their source attribution ('actor', 'attribution', 'family')\n",
    "\n",
    "    If a URL is associated with multiple groups, it's assigned to 'Unknown'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hash_to_url_map : dict\n",
    "        A dictionary mapping file hashes to URLs.\n",
    "\n",
    "    url_to_group_map : dict\n",
    "        A dictionary mapping URLs to dicts with keys 'group' and 'source'.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where group names are keys and values include lists of hashes and URL-source pairs.\n",
    "    \"\"\"\n",
    "    group_to_hash_url_map = {}\n",
    "\n",
    "    for file_hash, url in hash_to_url_map.items():\n",
    "        group_entry = url_to_group_map.get(url)\n",
    "\n",
    "        if not group_entry:\n",
    "            group_name = \"Unknown\"\n",
    "            source = \"unknown\"\n",
    "        else:\n",
    "            group_info = group_entry.get(\"group\", \"Unknown\")\n",
    "            source = group_entry.get(\"source\", \"unknown\")\n",
    "\n",
    "            if isinstance(group_info, list):\n",
    "                group_name = \"Unknown\" if len(group_info) > 1 else group_info[0]\n",
    "            else:\n",
    "                group_name = group_info\n",
    "\n",
    "        # Initialize if not yet present\n",
    "        if group_name not in group_to_hash_url_map:\n",
    "            group_to_hash_url_map[group_name] = {\n",
    "                \"hashes\": [],\n",
    "                \"urls\": []\n",
    "            }\n",
    "\n",
    "        group_to_hash_url_map[group_name][\"hashes\"].append(file_hash)\n",
    "        group_to_hash_url_map[group_name][\"urls\"].append({\n",
    "            \"url\": url,\n",
    "            \"source\": source\n",
    "        })\n",
    "\n",
    "    return group_to_hash_url_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a2e29a7e-e580-4553-8d19-7e4593c4794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "849\n"
     ]
    }
   ],
   "source": [
    "group_to_hash_url_map = update_group_to_hash_url_map(hash_to_url_map, url_to_group_map)\n",
    "print(len(group_to_hash_url_map))\n",
    "#first_key, first_value = next(iter(group_to_hash_url_map.items()))\n",
    "#print(first_key, first_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "355bab6a-1ef3-4d3e-af08-2ee15c0a0a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics(group_map):\n",
    "    \"\"\"\n",
    "    Calculate statistics for the group-to-hash-url map.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    group_map : dict\n",
    "        A dictionary where group names are keys. Each value is a dict with:\n",
    "        - 'hashes': list of hashes\n",
    "        - 'urls': list of dicts with keys 'url' and 'source'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with:\n",
    "        - num_unique_groups\n",
    "        - num_unique_families\n",
    "        - num_hashes\n",
    "        - num_unique_urls\n",
    "        - unknown_groups_count\n",
    "    \"\"\"\n",
    "    unique_urls = set()\n",
    "    num_hashes = 0\n",
    "    unknown_groups_count = 0\n",
    "\n",
    "    unique_group_names = set()\n",
    "    unique_family_names = set()\n",
    "\n",
    "    for group_name, data in group_map.items():\n",
    "        urls = data.get(\"urls\", [])\n",
    "        hashes = data.get(\"hashes\", [])\n",
    "\n",
    "        num_hashes += len(hashes)\n",
    "        unique_urls.update(url_entry['url'] for url_entry in urls)\n",
    "\n",
    "        if group_name.lower() == \"unknown\":\n",
    "            unknown_groups_count += len(hashes)\n",
    "            continue\n",
    "\n",
    "        # Determine source types\n",
    "        for url_entry in urls:\n",
    "            source = url_entry.get(\"source\", \"unknown\")\n",
    "            if source == \"family\":\n",
    "                unique_family_names.add(group_name)\n",
    "            else:  # actor or attribution\n",
    "                unique_group_names.add(group_name)\n",
    "\n",
    "    return {\n",
    "        \"num_unique_groups\": len(unique_group_names),\n",
    "        \"num_unique_families\": len(unique_family_names),\n",
    "        \"num_hashes\": num_hashes,\n",
    "        \"num_unique_urls\": len(unique_urls),\n",
    "        \"unknown_groups_count\": unknown_groups_count\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94897ff4-0084-4904-a1ae-257d0d300da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique groups (actor/attribution): 278\n",
      "Number of unique families: 570\n",
      "Total number of hashes: 2834\n",
      "Total unique URLs: 2813\n",
      "Number of hashes with 'Unknown' group: 841\n"
     ]
    }
   ],
   "source": [
    "stats = calculate_statistics(group_to_hash_url_map)\n",
    "\n",
    "print(f\"Number of unique groups (actor/attribution): {stats['num_unique_groups']}\")\n",
    "print(f\"Number of unique families: {stats['num_unique_families']}\")\n",
    "print(f\"Total number of hashes: {stats['num_hashes']}\")\n",
    "print(f\"Total unique URLs: {stats['num_unique_urls']}\")\n",
    "print(f\"Number of hashes with 'Unknown' group: {stats['unknown_groups_count']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ff1ecd6d-2b40-4c3e-830e-d3b8b9c72dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hashes_and_indicators(directories, indicator_type=\"cves\"):\n",
    "    \"\"\"\n",
    "    Extracts file hashes from filenames and reads the content of the files to\n",
    "    extract either CVEs or TTPs associated with each hash, based on the indicator_type parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directories : list\n",
    "        A list of directory paths containing the files.\n",
    "    indicator_type : str, optional\n",
    "        The type of indicators to extract. Accepts \"cve\" or \"ttp\". Default is \"cve\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where the key is the file hash, and the value is a list\n",
    "        of CVEs or TTPs associated with that hash.\n",
    "    \"\"\"\n",
    "    hash_to_indicators = {}\n",
    "    \n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            if indicator_type == \"cves\" and filename.endswith(\".iocs\"):\n",
    "                file_hash = filename.split('.')[0]\n",
    "                indicators = []\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    for line in file:\n",
    "                        parts = line.strip().split(\"\\t\")\n",
    "                        if parts[0] == \"cve\" and len(parts) > 1:\n",
    "                            indicators.append(parts[1])\n",
    "                \n",
    "                if indicators:\n",
    "                    hash_to_indicators[file_hash] = indicators\n",
    "\n",
    "            elif indicator_type == \"ttps\" and filename.endswith(\".download.iocs\"):\n",
    "                # Remove the '.download.iocs' suffix from the filename\n",
    "                file_hash = filename.removesuffix(\".download.iocs\")\n",
    "                file_hashes.append(file_hash) # Remove \".download.iocs\" from filename\n",
    "                indicators = []\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                    for line in file:\n",
    "                        parts = line.strip().split(\"\\t\")\n",
    "                        if len(parts) > 1 and parts[0] == \"ttp\":\n",
    "                            indicators.append(parts[1])\n",
    "                \n",
    "                if indicators:\n",
    "                    hash_to_indicators[file_hash] = indicators\n",
    "    \n",
    "    return hash_to_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e4a2efa2-b7f2-4334-b5eb-a468480b3a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2834\n"
     ]
    }
   ],
   "source": [
    "cve_data = get_file_hashes_and_indicators(directories, \"cves\")\n",
    "#ttp_data = get_file_hashes_and_indicators(directories, \"ttps\")\n",
    "\n",
    "\n",
    "print(len(cve_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d85f3242-1f09-4383-9f4c-6d7375a92e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_group_hash_with_data(group_hash_to_url_map, data, data_type):\n",
    "    \"\"\"\n",
    "    Updates the group_hash_to_url_map by adding TTPs or CVEs from the respective data dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group_hash_to_url_map : dict\n",
    "        A dictionary where each group contains hashes and URLs.\n",
    "    data : dict\n",
    "        A dictionary where the keys are hashes and the values are lists of TTPs or CVEs.\n",
    "    data_type : str\n",
    "        The type of data being processed ('cve' or 'ttp').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The updated group_hash_to_url_map with TTPs or CVEs added for each hash.\n",
    "    \"\"\"\n",
    "    if data_type not in ('cves', 'ttps'):\n",
    "        raise ValueError(\"Invalid data_type. Must be 'cve' or 'ttp'.\")\n",
    "    \n",
    "    for group_name, group_data in group_hash_to_url_map.items():\n",
    "        updated_hashes = []\n",
    "        \n",
    "        for hash_value in group_data['hashes']:\n",
    "            if hash_value in data:\n",
    "                updated_hashes.append({\n",
    "                    \"hash\": hash_value,\n",
    "                    data_type: data[hash_value]  # Dynamically set key as 'cve' or 'ttp'\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Hash {hash_value} not found in {data_type}_data.\")\n",
    "                updated_hashes.append({\n",
    "                    \"hash\": hash_value,\n",
    "                    data_type: []\n",
    "                })\n",
    "        \n",
    "        group_hash_to_url_map[group_name]['hashes'] = updated_hashes\n",
    "    \n",
    "    return group_hash_to_url_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d8ea5a3d-f469-4898-b097-a10258ae6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to update the map  --> May need to restart it if already ran once or else it flashes error\n",
    "\n",
    "#updated_data = update_group_hash_with_data(group_to_hash_url_map, ttp_data, \"ttps\")\n",
    "\n",
    "updated_data = update_group_hash_with_data(group_to_hash_url_map, cve_data, \"cves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5f2fb46-42fb-4193-9581-d03ae9970505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(data, data_type, file_path):\n",
    "    \"\"\"\n",
    "    Saves the given data to a JSON file with a dynamic filename based on the data type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        The dictionary data to be saved.\n",
    "    data_type : str\n",
    "        The type of data being saved, either 'cve' or 'ttp'.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    return file_path  # Return filename for further use\n",
    "\n",
    "def count_json_keys(json_file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file and counts the number of keys in it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    json_file_path : str\n",
    "        The path to the JSON file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of keys in the JSON file.\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    return len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e8067dbe-73ef-4181-9fe8-3587d24a846a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys in the JSON file: 849\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "\n",
    "base_dir = os.getcwd()\n",
    "json_output_data_dir = os.path.join(base_dir, \"group_analysis_json_outputs\")\n",
    "\n",
    "\n",
    "data_type = \"cve\"\n",
    "#data_type = \"ttp\" # Change to \"cve\" when dealing with CVE data\n",
    "filename = f\"Malpedia_{data_type}_group_analysis.json\"\n",
    "file_path = os.path.join(json_output_data_dir, filename)\n",
    "\n",
    "file_path = save_json(updated_data, data_type, file_path)\n",
    "\n",
    "num_keys = count_json_keys(file_path)\n",
    "print(f\"Number of keys in the JSON file: {num_keys}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0efaaf15-f4c1-4708-a6c5-31476e67e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data(group_data_map, data_type=\"cves\"):\n",
    "    \"\"\"\n",
    "    Analyzes CVEs or TTPs across groups with breakdowns for actor/attribution and family sources.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    group_data_map : dict\n",
    "        Dictionary with group names as keys. Each value contains:\n",
    "        - 'hashes': list of dicts with 'hash' and data_type (e.g., 'cves')\n",
    "        - 'urls': list of dicts with 'url' and 'source'\n",
    "\n",
    "    data_type : str\n",
    "        Type of data to analyze: 'cves' or 'ttps'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Dictionary with analysis statistics.\n",
    "    \"\"\"\n",
    "    all_items = []\n",
    "    group_item_map = defaultdict(set)\n",
    "    group_source_type = {}  # group_id -> \"family\" or \"group\" (actor/attribution)\n",
    "\n",
    "    for group_id, group_data in group_data_map.items():\n",
    "        # Determine if group is family-based or actor/attribution\n",
    "        sources = {entry[\"source\"] for entry in group_data.get(\"urls\", [])}\n",
    "        if sources == {\"family\"}:\n",
    "            group_source_type[group_id] = \"family\"\n",
    "        else:\n",
    "            group_source_type[group_id] = \"group\"\n",
    "\n",
    "        # Collect data items\n",
    "        for hash_data in group_data[\"hashes\"]:\n",
    "            items = hash_data.get(data_type, [])\n",
    "            group_item_map[group_id].update(items)\n",
    "            all_items.extend(items)\n",
    "\n",
    "    # === Global stats ===\n",
    "    total_items = len(all_items)\n",
    "    unique_items = set(all_items)\n",
    "    total_unique_items = len(unique_items)\n",
    "    item_counter = Counter(all_items)\n",
    "    top_10_items = item_counter.most_common(10)\n",
    "\n",
    "\n",
    "    # === Per-group unique items (actor/attribution only, excluding \"Unknown\") ===\n",
    "    unique_items_per_group = {}\n",
    "    for group_id, items in group_item_map.items():\n",
    "        if group_source_type.get(group_id) != \"group\" or group_id.lower() == \"unknown\":\n",
    "            continue  # Skip family and 'Unknown' groups\n",
    "    \n",
    "        other_items = set()\n",
    "        for other_group, other_group_items in group_item_map.items():\n",
    "            if other_group != group_id:\n",
    "                other_items.update(other_group_items)\n",
    "    \n",
    "        unique = items - other_items\n",
    "        if unique:\n",
    "            unique_items_per_group[group_id] = list(unique)\n",
    "\n",
    "\n",
    "    # === Per-item group appearances ===\n",
    "    item_group_count = Counter()\n",
    "    for item in unique_items:\n",
    "        for group_id, items in group_item_map.items():\n",
    "            if item in items:\n",
    "                item_group_count[item] += 1\n",
    "    top_10_common_across_groups = item_group_count.most_common(10)\n",
    "\n",
    "    # === Source-aware stats ===\n",
    "    group_items = []\n",
    "    family_items = []\n",
    "\n",
    "    for group_id, items in group_item_map.items():\n",
    "        if group_source_type[group_id] == \"group\":\n",
    "            group_items.extend(items)\n",
    "        elif group_source_type[group_id] == \"family\":\n",
    "            family_items.extend(items)\n",
    "\n",
    "    top_10_group_only = Counter(group_items).most_common(10)\n",
    "    top_10_family_only = Counter(family_items).most_common(10)\n",
    "\n",
    "    # === Output summary ===\n",
    "    print(f\"Total number of {data_type.upper()}: {total_items}\")\n",
    "    print(f\"Total number of unique {data_type.upper()}: {total_unique_items}\")\n",
    "\n",
    "    print(f\"\\nTop 10 most common {data_type.upper()} (overall occurrences):\")\n",
    "    for item, count in top_10_items:\n",
    "        print(f\"{item}: {count}\")\n",
    "\n",
    "    print(f\"\\nTop 10 most common {data_type.upper()} (across groups):\")\n",
    "    for item, count in top_10_common_across_groups:\n",
    "        print(f\"{item}: {count} groups\")\n",
    "\n",
    "    print(f\"\\nTop 10 {data_type.upper()} in groups (actor/attribution):\")\n",
    "    for item, count in top_10_group_only:\n",
    "        print(f\"{item}: {count}\")\n",
    "\n",
    "    print(f\"\\nTop 10 {data_type.upper()} in families:\")\n",
    "    for item, count in top_10_family_only:\n",
    "        print(f\"{item}: {count}\")\n",
    "\n",
    "    print(f\"\\n{data_type.upper()} count per group:\")\n",
    "    # Uncomment below if you want a full printout\n",
    "    # for group_id, count in count_per_group.items():\n",
    "    #     print(f\"{group_id}: {count}\")\n",
    "\n",
    "    print(f\"\\nNumber of groups with unique {data_type.upper()}: {len(unique_items_per_group)}\")\n",
    "\n",
    "    # Show a few example groups and their unique items\n",
    "    print(f\"\\nExample groups with unique {data_type.upper()}:\")\n",
    "    for group_id, uniques in list(unique_items_per_group.items())[:10]:  # Show first 5\n",
    "        print(f\"  Group: {group_id}\")\n",
    "        for item in uniques[:5]:  # Show up to 5 CVEs/TTPs per group\n",
    "            print(f\"    - {item}\")\n",
    "        if len(uniques) > 5:\n",
    "            print(f\"    ... and {len(uniques) - 5} more\")\n",
    "\n",
    "\n",
    "\n",
    "    return {\n",
    "        \"total_count\": total_items,\n",
    "        \"unique_count\": total_unique_items,\n",
    "        \"top_10_overall\": top_10_items,\n",
    "        \"top_10_across_groups\": top_10_common_across_groups,\n",
    "        \"top_10_groups_only\": top_10_group_only,\n",
    "        \"top_10_families_only\": top_10_family_only,\n",
    "        \"unique_per_group\": unique_items_per_group,\n",
    "        #\"count_per_group\": count_per_group\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4e2420c-4811-4f2a-83fa-8cbbcdef124c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of CVES: 11340\n",
      "Total number of unique CVES: 2020\n",
      "\n",
      "Top 10 most common CVES (overall occurrences):\n",
      "CVE-2017-11882: 263\n",
      "CVE-2021-44228: 183\n",
      "CVE-2017-0199: 156\n",
      "CVE-2022-30190: 152\n",
      "CVE-2012-0158: 135\n",
      "CVE-2021-26855: 124\n",
      "CVE-2021-27065: 103\n",
      "CVE-2020-1472: 85\n",
      "CVE-2019-19781: 84\n",
      "CVE-2022-26134: 81\n",
      "\n",
      "Top 10 most common CVES (across groups):\n",
      "CVE-2017-11882: 131 groups\n",
      "CVE-2021-44228: 91 groups\n",
      "CVE-2022-30190: 85 groups\n",
      "CVE-2012-0158: 84 groups\n",
      "CVE-2017-0199: 56 groups\n",
      "CVE-2022-26134: 54 groups\n",
      "CVE-2014-4114: 53 groups\n",
      "CVE-2010-2883: 51 groups\n",
      "CVE-2022-22965: 50 groups\n",
      "CVE-2009-3129: 49 groups\n",
      "\n",
      "Top 10 CVES in groups (actor/attribution):\n",
      "CVE-2017-11882: 48\n",
      "CVE-2012-0158: 44\n",
      "CVE-2017-0199: 34\n",
      "CVE-2021-44228: 30\n",
      "CVE-2022-30190: 29\n",
      "CVE-2022-26134: 24\n",
      "CVE-2019-19781: 20\n",
      "CVE-2020-1472: 19\n",
      "CVE-2018-0802: 19\n",
      "CVE-2023-38831: 18\n",
      "\n",
      "Top 10 CVES in families:\n",
      "CVE-2017-11882: 83\n",
      "CVE-2021-44228: 61\n",
      "CVE-2022-30190: 56\n",
      "CVE-2014-4114: 41\n",
      "CVE-2010-2883: 41\n",
      "CVE-2012-4681: 41\n",
      "CVE-2009-3129: 40\n",
      "CVE-2012-0158: 40\n",
      "CVE-2011-0611: 39\n",
      "CVE-2011-2462: 39\n",
      "\n",
      "CVES count per group:\n",
      "\n",
      "Number of groups with unique CVES: 87\n",
      "\n",
      "Example groups with unique CVES:\n",
      "  Group: hafnium\n",
      "    - CVE-2022-27511\n",
      "  Group: lazarus\n",
      "    - CVE-2015-6585\n",
      "    - CVE-2018-15133\n",
      "    - CVE-2018-9375\n",
      "    - CVE-2023-29059\n",
      "    - CVE-2018-8165\n",
      "    ... and 8 more\n",
      "  Group: ta505\n",
      "    - CVE-2020-14002\n",
      "    - CVE-2023-35036\n",
      "    - CVE-2020-12061\n",
      "  Group: wildneutron\n",
      "    - CVE-2012-3213\n",
      "    - CVE-2022-29972\n",
      "  Group: red menshen\n",
      "    - CVE-2020-8835\n",
      "    - CVE-2019-3010\n",
      "  Group: pinchy spider\n",
      "    - CVE-2021-30117\n",
      "    - CVE-2014-0552\n",
      "    - CVE-2013-4660\n",
      "    - CVE-2016-3082\n",
      "    - CVE-2010-0112\n",
      "    ... and 5 more\n",
      "  Group: apt17\n",
      "    - CVE-2013-1493\n",
      "    - CVE-2020-10198\n",
      "  Group: gold cabin\n",
      "    - CVE-2007-0015\n",
      "    - CVE-2021-26427\n",
      "    - CVE-2007-4673\n",
      "    - CVE-2022-41049\n",
      "  Group: sweed\n",
      "    - CVE-2020-10786\n",
      "    - CVE-2020-10787\n",
      "  Group: operation c-major\n",
      "    - CVE-2017-12815\n",
      "    - CVE-2018-100012\n",
      "    - CVE-2016-0898\n",
      "    - CVE-2012-1876\n",
      "    - CVE-2018-4091\n",
      "    ... and 1 more\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "cve_analysis = analyze_data(updated_data, data_type=\"cves\")\n",
    "#ttp_analysis = analyze_data(updated_data, data_type=\"ttps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1df22-02e3-4f30-a820-68593dccbae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2c219-e1fe-4aba-b8ea-13248c9e3dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
