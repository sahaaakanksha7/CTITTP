{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09b150f7-dc57-43e3-a304-f572ff1c89c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from statistics import mode\n",
    "from collections import Counter, defaultdict\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7278c6c-22cb-413a-a0c6-ebf14d1c92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_config():\n",
    "    # Get the absolute path of the project root (one directory up)\n",
    "    project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "    # Normalize the project_root to ensure it's correctly formatted\n",
    "    project_root = os.path.normpath(project_root)\n",
    "    \n",
    "    config_path = os.path.join(project_root, 'config.json')\n",
    "\n",
    "    if not os.path.exists(config_path):\n",
    "        raise FileNotFoundError(f\"Config file not found at expected location: {config_path}\")\n",
    "\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = json.load(f)\n",
    "\n",
    "    return config, project_root\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8394d0d-a0af-40d3-af70-26bfcc448d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "config, project_root = load_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "299fe012-87e2-4e8d-8be8-ab78bf6a45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_citations(citations_raw):\n",
    "    \"\"\"\n",
    "    This function takes a raw string of citations, extracts individual citations,\n",
    "    removes duplicates, cleans up irrelevant characters, and ensures no extra spaces or invalid characters.\n",
    "\n",
    "    Args:\n",
    "        citations_raw (str): The raw citation string from which citations are to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique citations.\n",
    "    \"\"\"\n",
    "    #print(citations_raw)\n",
    "    if pd.isna(citations_raw) or citations_raw == '':\n",
    "        return []\n",
    "\n",
    "    # Replace any occurrence of 'Citation: ' to standardize the string\n",
    "    citations_raw = citations_raw.replace(\"Citation: \", \"\")\n",
    "    \n",
    "    # Split citations by both the delimiters '), (' and ')('\n",
    "    citations = re.split(r\"\\)\\s*,?\\s*\\(|\\)\\(\", citations_raw)\n",
    "\n",
    "    # Clean each citation by removing unwanted characters and extra spaces\n",
    "    cleaned_citations = []\n",
    "    for citation in citations:\n",
    "        citation = citation.replace(\"(\", \"\").replace(\")\", \"\").strip(\", \").strip()\n",
    "        \n",
    "        # Use regex to extract only the relevant citation portion before any period or embedded sentence\n",
    "        # Here, we match the first part (e.g., \"US-CERT HIDDEN COBRA June 2017\") and ignore the rest\n",
    "        citation = re.sub(r'\\s*\\..*$', '', citation)  # Remove everything after the first period\n",
    "        \n",
    "        # We also use another regex to handle extra commas, which might still linger after cleaning\n",
    "        citation = re.sub(r',\\s*$', '', citation)  # Remove comma at the end of citation\n",
    "        \n",
    "        if citation:  # Only add to the list if there is any valid citation\n",
    "            cleaned_citations.append(citation)\n",
    "    \n",
    "    # Remove duplicates by converting the list to a set and back to a list\n",
    "    cleaned_citations = list(set(cleaned_citations))\n",
    "\n",
    "    #print(cleaned_citations)\n",
    "\n",
    "    return cleaned_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07fd6e24-3d8a-47bf-ae90-7af77a711d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_group_citation_map(excel_file):\n",
    "    \"\"\"\n",
    "    Build a group citation map from a given MITRE ATT&CK group Excel file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    excel_file : str\n",
    "        Path to the Excel file containing 'groups' and 'citations' sheets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A mapping of group ID to associated and relationship citations (with URLs).\n",
    "    \"\"\"\n",
    "    df_groups = pd.read_excel(excel_file, sheet_name='groups')\n",
    "    df_citations = pd.read_excel(excel_file, sheet_name='citations')\n",
    "\n",
    "    citation_map = dict(zip(df_citations['reference'], df_citations['url']))\n",
    "    group_citations = {}\n",
    "\n",
    "    for _, row in df_groups.iterrows():\n",
    "        group_id = row['ID']\n",
    "\n",
    "        associated_raw = row.get('associated groups citations')\n",
    "        relationship_raw = row.get('relationship citations')\n",
    "\n",
    "        associated = extract_citations(associated_raw) if pd.notna(associated_raw) else []\n",
    "        relationship = extract_citations(relationship_raw) if pd.notna(relationship_raw) else []\n",
    "\n",
    "        associated_with_urls = {\n",
    "            c: citation_map.get(c, \"URL not found\") for c in associated\n",
    "        }\n",
    "        relationship_with_urls = {\n",
    "            c: citation_map.get(c, \"URL not found\") for c in relationship\n",
    "        }\n",
    "\n",
    "        group_citations[group_id] = {\n",
    "            \"associated_groups_citations\": associated_with_urls,\n",
    "            \"relationship_citations\": relationship_with_urls\n",
    "        }\n",
    "\n",
    "    return group_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21281719-1905-4562-8384-ac29810b56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_group_citation_maps(group_citations_map, new_data):\n",
    "    \"\"\"\n",
    "    Merge new group citation data into the existing map.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    group_citations_map : dict\n",
    "        The existing group citation map.\n",
    "    new_data : dict\n",
    "        New citation data to merge in.\n",
    "    \"\"\"\n",
    "    for group_id, data in new_data.items():\n",
    "        if group_id not in group_citations_map:\n",
    "            group_citations_map[group_id] = data\n",
    "        else:\n",
    "            # Merge associated citations\n",
    "            group_citations_map[group_id][\"associated_groups_citations\"].update(\n",
    "                data[\"associated_groups_citations\"]\n",
    "            )\n",
    "            # Merge relationship citations\n",
    "            group_citations_map[group_id][\"relationship_citations\"].update(\n",
    "                data[\"relationship_citations\"]\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8c2601e-fa1f-43da-995e-24931dcc761c",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_paths = [\n",
    "    os.path.normpath(os.path.join(project_root, config[\"data_directory\"], config[\"file_paths_groups_v15\"][\"enterprise\"])),\n",
    "    os.path.normpath(os.path.join(project_root, config[\"data_directory\"], config[\"file_paths_groups_v15\"][\"mobile\"])),\n",
    "    os.path.normpath(os.path.join(project_root, config[\"data_directory\"], config[\"file_paths_groups_v15\"][\"ics\"]))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e60a50be-f7b0-463b-b132-71a8393d43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_citations_map = {}\n",
    "\n",
    "for file_path in group_paths:\n",
    "    new_data = build_group_citation_map(file_path)\n",
    "    merge_group_citation_maps(group_citations_map, new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ff126d-1f60-4b7b-bc0c-6aa783fa1ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(group_citations_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75073e71-2433-49d3-866c-0578170a021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of URLs across the whole dictionary: 1212\n",
      "Totak number of groups: 152\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "total_urls = 0  # Total number of URLs across both associated_groups_citations and relationship_citations\n",
    "\n",
    "# Iterate through each group in the dictionary\n",
    "for group_id, group_data in group_citations_map.items():\n",
    "    # Count URLs in 'associated_groups_citations'\n",
    "    associated_urls = group_data.get('associated_groups_citations', {})\n",
    "    total_urls += len(associated_urls)  # Add the number of URLs in associated_groups_citations\n",
    "    \n",
    "    # Count URLs in 'relationship_citations'\n",
    "    relationship_urls = group_data.get('relationship_citations', {})\n",
    "    total_urls += len(relationship_urls)  # Add the number of URLs in relationship_citations\n",
    "\n",
    "# Output the total number of URLs\n",
    "print(f\"Total number of URLs across the whole dictionary: {total_urls}\")\n",
    "num_groups = len(group_citations_map)  \n",
    "print(f\"Totak number of groups: {num_groups}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "54a01859-582c-41ea-9bc6-6a01bb8fb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load JSONL data\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Loads a JSONL file into a list of dictionaries.\n",
    "    Each line of the file is parsed as a JSON object.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = [json.loads(line) for line in file]\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3b3f510-4d65-4326-a3ff-93e53d786aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mitre_jsonl_path = os.path.normpath(os.path.join(project_root, config['jsonl_files']['MITRE_enterprise']))\n",
    "\n",
    "# Step 1: Load the JSONL data\n",
    "jsonl_data = load_jsonl(mitre_jsonl_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a70f22a2-5beb-4290-8f98-e510970ef217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1417"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jsonl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fe100250-bb9b-4725-b761-68d96a618d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_hash_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extracts the file hash from a given filename.\n",
    "    \n",
    "    - If the filename ends with '.download.iocs', it removes this suffix.\n",
    "    - Otherwise, if the filename ends with '.iocs', it removes only '.iocs'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The input filename.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or None\n",
    "        The extracted file hash, or None if the filename is invalid.\n",
    "    \"\"\"\n",
    "    if filename.endswith('.download.iocs'):\n",
    "        return filename[:-14]  # Remove '.download.iocs' (14 characters)\n",
    "\n",
    "    if filename.endswith('.iocs'):\n",
    "        return filename[:-5]  # Remove '.iocs'\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ad4d217e-5c36-4811-9c74-52f99b5c0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where the CVE files are stored\n",
    "folder_path = os.path.normpath(os.path.join(project_root, config['directory_paths_ioc']['CVE_MITRE']))\n",
    "\n",
    "# Folder where the TTP files are stored\n",
    "#folder_path = os.path.normpath(os.path.join(project_root, config['directory_paths_ioc']['TTP_MITRE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fda59e75-205b-47b3-81a5-36b93831a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store file hash -> URL mapping\n",
    "hash_to_url_map = {}\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Extract file hash from the filename\n",
    "    file_hash = extract_file_hash_from_filename(filename)\n",
    "    if file_hash:\n",
    "        # Look for the matching download_sha256 in the JSONL data\n",
    "        for entry in jsonl_data:\n",
    "            if entry.get('download_sha256') == file_hash:\n",
    "                # If the hashes match, store the URL\n",
    "                hash_to_url_map[file_hash] = entry.get('url')\n",
    "                break  # Exit loop once a match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cc763645-8ae4-4326-9202-e023567e519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266\n"
     ]
    }
   ],
   "source": [
    "print(len(hash_to_url_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8789780-0db0-4adb-901e-5eed536bb0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.dragos.com/wp-content/uploads/CRASHOVERRIDE2018.pdf'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_to_url_map.get(\"e3394ddac9c861e5be26eb67123da606de98786a1f1c6cc06cfeaf151bc67ac8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fe85b757-6ade-4f99-a659-aae5532dc9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary to store the hash, group_id, and URL\n",
    "hash_group_url_map = {}\n",
    "\n",
    "# Iterate through each hash, url pair in hash_to_url_map\n",
    "for file_hash, url in hash_to_url_map.items():\n",
    "    # Iterate through each group in group_citations_map\n",
    "    for group_id, group_data in group_citations_map.items():\n",
    "        # Search for the URL in associated_groups_citations\n",
    "        if url in group_data.get('associated_groups_citations', {}).values():\n",
    "            # If the URL is found, add the file_hash, group_id, and URL to the hash_group_url_map\n",
    "            hash_group_url_map[file_hash] = {'group_id': group_id, 'url': url}\n",
    "            break  # Exit once we find the match (no need to check further in this group)\n",
    "\n",
    "        # Search for the URL in relationship_citations\n",
    "        if url in group_data.get('relationship_citations', {}).values():\n",
    "            # If the URL is found, add the file_hash, group_id, and URL to the hash_group_url_map\n",
    "            hash_group_url_map[file_hash] = {'group_id': group_id, 'url': url}\n",
    "            break  # Exit once we find the match (no need to check further in this group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5edd10df-eaec-4272-9a1f-825083b380f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'group_id': 'G0034',\n",
       " 'url': 'https://www.dragos.com/wp-content/uploads/CRASHOVERRIDE2018.pdf'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_group_url_map.get(\"e3394ddac9c861e5be26eb67123da606de98786a1f1c6cc06cfeaf151bc67ac8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a42c57e3-fe8b-45ca-bea1-d6527ba9e206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hash: 05bc7a68fdfe54c95ba1fb7360f2cb73bbfbdbbe939c29d764abf59f975a6a3a, URL: https://www.infosecurity-magazine.com/news/microsoft-zero-day-traced-russian/\n",
      "Hash: 2c9e582e0194bacc4e4bbb37ffe61ed7e89af5cc5748fdc001e9dd65ddfaa32f, URL: https://securelist.com/apt-trends-report-q1-2018/85280/\n",
      "Hash: 5b6328ed41cb49229d8d47046caabae1fdb90045c467d6509ae1f459a9b5b518, URL: https://www.intezer.com/wp-content/uploads/2021/09/TeamTNT-Cryptomining-Explosion.pdf\n",
      "Hash: 72beb22ceed285d666ec7912dfcb95e7107c4232e622026915ef1bcd3c593490, URL: https://unit42.paloaltonetworks.com/ukraine-targeted-outsteel-saintbot/\n",
      "Hash: 73eac7a13e4c15ce849d7a12a8d56eb3d831b6b442bf9ce7bc43afc1caafde9c, URL: https://www.us-cert.gov/ncas/alerts/TA17-164A\n",
      "Hash: cabd66802a057829a0113bc5e53ac0c2c48f91142e8a40e10aac0d9d6aebbe98, URL: https://www.bleepingcomputer.com/news/security/ukraine-links-members-of-gamaredon-hacker-group-to-russian-fsb/\n",
      "Hash: e2f84d3c77547f31ba782c0bb5525980059f651931e2b1dbbcd0a81f4430a1db, URL: https://securelist.com/the-naikon-apt/69953/\n",
      "Hash: e6af21bf6c751aabe190006f4f60b2b2a4164c6cb1bf34cc461b4f116eaf479d, URL: https://www.cyberscoop.com/middle-eastern-hacking-group-using-finfisher-malware-conduct-international-espionage/\n"
     ]
    }
   ],
   "source": [
    "# Find the hashes that are in hash_to_url_map but not in hash_group_url_map\n",
    "missing_hashes = {key: value for key, value in hash_to_url_map.items() if key not in hash_group_url_map}\n",
    "\n",
    "# Print the list of hashes and URLs that are missing in hash_group_url_map\n",
    "for hash_key, url in missing_hashes.items():\n",
    "    print(f\"Hash: {hash_key}, URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b59185ef-1fab-4378-8336-d736bd3f2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cves_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all the files in a specified folder with .iocs extension and creates a dictionary\n",
    "    with the file hash (from the filename) as the key and the list of CVEs as the value.\n",
    "    It also removes the 'cve' prefix from each CVE and handles the file content appropriately.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the .iocs files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the key is the file hash and the value is a list of CVEs.\n",
    "    \"\"\"\n",
    "    iocs_dict = {}\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a '.iocs' extension\n",
    "        if filename.endswith(\".iocs\"):\n",
    "            file_hash = filename.split(\".\")[0]  # Extract file hash from the filename\n",
    "            \n",
    "            # Construct the full path to the file\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            try:\n",
    "                # Open the file and read its content\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read().strip()  # Read and strip any unwanted whitespace\n",
    "\n",
    "                    # Remove \"cve\" prefix and the tab character, and store CVEs in a list\n",
    "                    content_lines = content.split('\\n')  # Split content by lines\n",
    "                    cleaned_content = []\n",
    "                    for line in content_lines:\n",
    "                        if line.startswith(\"cve\"):\n",
    "                            # Remove 'cve' and the tab character '\\t'\n",
    "                            cleaned_line = line[4:].strip()  # Strip the 'cve' prefix and any leading/trailing spaces\n",
    "                            cleaned_content.append(cleaned_line)\n",
    "\n",
    "                # Store the file hash and list of CVEs in the dictionary\n",
    "                iocs_dict[file_hash] = cleaned_content\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {filename}: {e}\")\n",
    "\n",
    "    return iocs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ed43b1b-9b1b-4afd-9680-ebd322bc170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cve_data = read_cves_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b64a82b4-03b4-4621-be1a-4d22bbf89221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ttps_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all .download.iocs files in a specified folder and extracts TTPs (Technique IDs).\n",
    "    \n",
    "    - Extracts the file hash from the filename.\n",
    "    - Parses the file to extract TTPs (entries starting with 'ttp\\tT').\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        The path to the folder containing .download.iocs files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where keys are file hashes and values are lists of extracted TTPs.\n",
    "    \"\"\"\n",
    "    ttps_dict = {}\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".download.iocs\"):\n",
    "            file_hash = extract_file_hash_from_filename(filename)\n",
    "            \n",
    "            if not file_hash:\n",
    "                print(f\"Skipping invalid filename: {filename}\")\n",
    "                continue  # Skip files that don't match expected format\n",
    "            \n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    # Extract TTPs from each line that starts with 'ttp\\tT'\n",
    "                    ttps = [\n",
    "                        line.split('\\t')[1].split(' ')[0]  # Extract only the TTP ID (e.g., T1204)\n",
    "                        for line in file.read().strip().split('\\n')\n",
    "                        if line.startswith(\"ttp\\tT\")\n",
    "                    ]\n",
    "\n",
    "                # Store results in dictionary\n",
    "                ttps_dict[file_hash] = ttps\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {filename}: {e}\")\n",
    "    \n",
    "    return ttps_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bd7deb9-946e-43c2-b1b6-ce1f1778039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttps_data = read_ttps_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2585a6de-ebca-47b6-919c-329e77224aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_hash_data(iocs_data, hash_group_url_map, data_type='cves'):\n",
    "    \"\"\"\n",
    "    Combines hash-based IOC data with group and URL information.\n",
    "    \n",
    "    Args:\n",
    "        iocs_data (dict): Dictionary where keys are file hashes and values are lists of CVEs or TTPs.\n",
    "        hash_group_url_map (dict): Dictionary mapping file hashes to group information (group_id, URL).\n",
    "        data_type (str): Type of data to process ('cves' or 'ttps').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are group IDs and values contain hashes and associated data.\n",
    "    \"\"\"\n",
    "    hash_data_map = {}\n",
    "    \n",
    "    # Step 1: Iterate through the hashes in iocs_data\n",
    "    for file_hash, data_list in iocs_data.items():\n",
    "        # Check if the file_hash exists in hash_group_url_map\n",
    "        if file_hash in hash_group_url_map:\n",
    "            group_info = hash_group_url_map[file_hash]\n",
    "            \n",
    "            # Combine the data: CVEs/TTPs, group_id, and URL\n",
    "            hash_data_map[file_hash] = {\n",
    "                data_type: data_list,  # 'cves' or 'ttps'\n",
    "                'group_id': group_info['group_id'],\n",
    "                'url': group_info['url']\n",
    "            }\n",
    "    \n",
    "    # Step 2: Transform hash_data_map into a group-based structure\n",
    "    group_data_map = {}\n",
    "    \n",
    "    for hash_val, data in hash_data_map.items():\n",
    "        group_id = data['group_id']\n",
    "        extracted_data = data[data_type]\n",
    "        url = data['url']\n",
    "        \n",
    "        # Initialize group entry if not present\n",
    "        if group_id not in group_data_map:\n",
    "            group_data_map[group_id] = {\n",
    "                'hashes': [],\n",
    "                'url': url,\n",
    "            }\n",
    "        \n",
    "        # Append hash and extracted data (CVEs or TTPs)\n",
    "        group_data_map[group_id]['hashes'].append({\n",
    "            'hash': hash_val,\n",
    "            data_type: extracted_data,\n",
    "        })\n",
    "    \n",
    "    return group_data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a1555d7b-772a-4295-b731-0121247a4ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CVEs\n",
    "group_cve_map = combine_hash_data(cve_data, hash_group_url_map, data_type='cves')\n",
    "\n",
    "# For TTPs\n",
    "#group_ttp_map = combine_hash_data(ttps_data, hash_group_url_map, data_type='ttps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55c20676-7562-44e6-a2b6-532b94ec5e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "86"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(group_cve_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3f48e6d-901b-4cf2-9900-093fa642a415",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_and_count_data(data, data_type='cve'):\n",
    "    \"\"\"\n",
    "    Dumps the provided data to a JSON file and counts the number of keys.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The data dictionary to be saved.\n",
    "        data_type (str): The type of data ('cves' or 'ttps').\n",
    "        file_prefix (str): Prefix for the filename.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of keys in the JSON file.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Dump data to JSON file\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    \n",
    "    # Read and count the number of keys in the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        loaded_data = json.load(file)\n",
    "    \n",
    "    return len(loaded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b741dc5-ea3a-4fd1-833e-12bf5848ddde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups in data: 86\n"
     ]
    }
   ],
   "source": [
    "base_dir = os.getcwd()\n",
    "json_output_data_dir = os.path.join(base_dir, \"group_analysis_json_outputs\")\n",
    "\n",
    "data_type = \"cve\"\n",
    "#data_type = \"ttp\" \n",
    "# Change to \"cve\" when dealing with CVE data\n",
    "file_name = f'MITRE_{data_type}_group_analysis.json'\n",
    "file_path = os.path.join(json_output_data_dir, file_name)\n",
    "\n",
    "# Example usage:\n",
    "num_cve_keys = dump_and_count_data(group_cve_map, data_type='cve')\n",
    "#num_ttp_keys = dump_and_count_data(group_ttp_map, data_type='ttp')\n",
    "print(f\"Number of groups in data: {num_cve_keys}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "869be641-8827-45fa-8ecd-706c082d25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_data(group_data_map, data_type=\"cves\"):\n",
    "    \"\"\"\n",
    "    Analyzes the given group data (CVE or TTP) and provides statistical insights.\n",
    "\n",
    "    Args:\n",
    "        group_data_map (dict): Dictionary where keys are group IDs and values contain hashes and data lists.\n",
    "        data_type (str): Either \"cves\" or \"ttps\" to specify the type of data being analyzed.\n",
    "\n",
    "    Returns:\n",
    "        dict: Analysis results including total count, unique count, top 10 common items overall,\n",
    "              top 10 by group frequency, and unique items per group.\n",
    "    \"\"\"\n",
    "    all_items = []\n",
    "    overall_counter = Counter()\n",
    "    groupwise_counter = Counter()\n",
    "    unique_items_per_group = {}\n",
    "\n",
    "    for group_id, group_data in group_data_map.items():\n",
    "        group_items = set()\n",
    "        for hash_data in group_data.get(\"hashes\", []):\n",
    "            items = hash_data.get(data_type, [])\n",
    "            all_items.extend(items)\n",
    "            overall_counter.update(items)\n",
    "            group_items.update(items)\n",
    "\n",
    "        # Update groupwise counter (count each item once per group)\n",
    "        groupwise_counter.update(group_items)\n",
    "\n",
    "        # Compute unique items for this group\n",
    "        other_items = set()\n",
    "        for other_id, other_data in group_data_map.items():\n",
    "            if other_id != group_id:\n",
    "                for hash_data in other_data.get(\"hashes\", []):\n",
    "                    other_items.update(hash_data.get(data_type, []))\n",
    "        unique_items = group_items - other_items\n",
    "        if unique_items:\n",
    "            unique_items_per_group[group_id] = list(unique_items)\n",
    "\n",
    "    total_items = len(all_items)\n",
    "    unique_items = set(all_items)\n",
    "    total_unique_items = len(unique_items)\n",
    "\n",
    "    top_10_overall = overall_counter.most_common(10)\n",
    "    top_10_by_group = groupwise_counter.most_common(10)\n",
    "\n",
    "    # Print analysis results\n",
    "    print(f\"Total number of {data_type.upper()}: {total_items}\")\n",
    "    print(f\"Total number of unique {data_type.upper()}: {total_unique_items}\")\n",
    "    print(f\"Top 10 most common {data_type.upper()} (overall occurrences):\")\n",
    "    for item, count in top_10_overall:\n",
    "        print(f\"{item}: {count}\")\n",
    "    print(f\"\\nTop 10 most common {data_type.upper()} (across different groups):\")\n",
    "    for item, count in top_10_by_group:\n",
    "        print(f\"{item}: {count}\")\n",
    "    print(f\"\\nNumber of groups with unique {data_type.upper()}: {len(unique_items_per_group)}\")\n",
    "\n",
    "    print(f\"\\nExample groups with unique {data_type.upper()}:\")\n",
    "    for group_id, uniques in list(unique_items_per_group.items())[:50]:  # Show first 5\n",
    "            print(f\"  Group: {group_id}\")\n",
    "            for item in uniques[:5]:  # Show up to 5 CVEs/TTPs per group\n",
    "                print(f\"    - {item}\")\n",
    "            if len(uniques) > 5:\n",
    "                print(f\"    ... and {len(uniques) - 5} more\")\n",
    "\n",
    "    \n",
    "\n",
    "    return {\n",
    "        \"total_count\": total_items,\n",
    "        \"unique_count\": total_unique_items,\n",
    "        \"top_10_overall\": top_10_overall,\n",
    "        \"top_10_by_group\": top_10_by_group,\n",
    "        \"unique_per_group\": unique_items_per_group\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f071f05d-dfca-43d7-9a64-877c83b281f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of CVES: 959\n",
      "Total number of unique CVES: 325\n",
      "Top 10 most common CVES (overall occurrences):\n",
      "CVE-2012-0158: 29\n",
      "CVE-2017-11882: 22\n",
      "CVE-2017-0199: 21\n",
      "CVE-2022-38028: 15\n",
      "CVE-2024-3400: 12\n",
      "CVE-2021-26855: 11\n",
      "CVE-2021-27065: 11\n",
      "CVE-2010-3333: 10\n",
      "CVE-2014-6332: 10\n",
      "CVE-2018-13379: 10\n",
      "\n",
      "Top 10 most common CVES (across different groups):\n",
      "CVE-2012-0158: 17\n",
      "CVE-2017-0199: 16\n",
      "CVE-2017-11882: 14\n",
      "CVE-2022-38028: 9\n",
      "CVE-2018-13379: 8\n",
      "CVE-2016-4117: 8\n",
      "CVE-2010-3333: 7\n",
      "CVE-2018-0802: 7\n",
      "CVE-2024-37085: 7\n",
      "CVE-2023-3519: 6\n",
      "\n",
      "Number of groups with unique CVES: 48\n",
      "\n",
      "Example groups with unique CVES:\n",
      "  Group: G0029\n",
      "    - CVE-2010-2572\n",
      "    - CVE-2012-4969\n",
      "  Group: G1015\n",
      "    - CVE-2015-2291\n",
      "    - CVE-2021-3490\n",
      "    - CVE-2021-35464\n",
      "  Group: G0121\n",
      "    - CVE-2019-2215\n",
      "    - CVE-2020-0674\n",
      "    - CVE-2024-9284\n",
      "  Group: G1023\n",
      "    - CVE-2022-27518\n",
      "    - CVE-2021-20021\n",
      "    - CVE-2021-20023\n",
      "  Group: G0139\n",
      "    - CVE-2024-6387\n",
      "    - CVE-2019-5736\n",
      "  Group: G0032\n",
      "    - CVE-2021-1647\n",
      "    - CVE-2018-202501\n",
      "  Group: G0069\n",
      "    - CVE-2017-01995\n",
      "  Group: G0027\n",
      "    - CVE-2017-15303\n",
      "    - CVE-2011-3544\n",
      "    - CVE-2010-0738\n",
      "    - CVE-2014-6324\n",
      "    - CVE-2017-0213\n",
      "  Group: G1017\n",
      "    - CVE-2021-27860\n",
      "  Group: G0046\n",
      "    - CVE-2024-47076\n",
      "    - CVE-2024-29847\n",
      "    - CVE-2024-47176\n",
      "    - CVE-2024-47177\n",
      "    - CVE-2024-47175\n",
      "  Group: G0034\n",
      "    - CVE-2014-0751\n",
      "    - CVE-2018-4878\n",
      "    - CVE-2014-3828\n",
      "    - CVE-2015-5374\n",
      "    - CVE-2018-8406\n",
      "    ... and 2 more\n",
      "  Group: G0040\n",
      "    - CVE-2017-8570\n",
      "  Group: G0125\n",
      "    - CVE-2018-18913\n",
      "  Group: G0102\n",
      "    - CVE-2021-34527\n",
      "  Group: G1021\n",
      "    - CVE-2020-8243\n",
      "    - CVE-2021-22894\n",
      "    - CVE-2022-30190\n",
      "    - CVE-2020-10189\n",
      "    - CVE-2021-35247\n",
      "    ... and 11 more\n",
      "  Group: G0016\n",
      "    - CVE-2017-5638\n",
      "    - CVE-2014-8361\n",
      "    - CVE-2019-1653\n",
      "    - CVE-2015-2051\n",
      "    - CVE-2021-36934\n",
      "    ... and 19 more\n",
      "  Group: G0097\n",
      "    - CVE-2025-26633\n",
      "  Group: G0131\n",
      "    - CVE-2020-8468\n",
      "    - CVE-2019-9489\n",
      "    - CVE-2018-8174\n",
      "  Group: G0022\n",
      "    - CVE-2019-100300\n",
      "    - CVE-2018-100060\n",
      "    - CVE-2014-4113\n",
      "    - CVE-2010-3962\n",
      "  Group: G0077\n",
      "    - CVE-2014-0160\n",
      "  Group: G0049\n",
      "    - CVE-2017-1099\n",
      "    - CVE-2020-1350\n",
      "  Group: G0007\n",
      "    - CVE-2016-7255\n",
      "    - CVE-2016-725521\n",
      "    - CVE-2015-2590\n",
      "    - CVE-2015-2424\n",
      "    - CVE-2011-3874\n",
      "    ... and 29 more\n",
      "  Group: G0035\n",
      "    - CVE-2013-2465\n",
      "  Group: G0064\n",
      "    - CVE-2017-11774\n",
      "  Group: G0106\n",
      "    - CVE-2017-3066\n",
      "    - CVE-2017-10271\n",
      "  Group: G0092\n",
      "    - CVE-2021-27102\n",
      "    - CVE-2021-27103\n",
      "    - CVE-2021-27104\n",
      "    - CVE-2021-27101\n",
      "  Group: G0098\n",
      "    - CVE-2017-7269\n",
      "  Group: G0055\n",
      "    - CVE-2015-0072\n",
      "    - CVE-2016-0189\n",
      "    - CVE-2015-1671\n",
      "    - CVE-2008-2551\n",
      "    - CVE-2011-0097\n",
      "    ... and 7 more\n",
      "  Group: G0010\n",
      "    - CVE-2013-5065\n",
      "    - CVE-2008-3431\n",
      "    - CVE-2013-3346\n",
      "  Group: G0117\n",
      "    - CVE-2018-1579\n",
      "  Group: G0060\n",
      "    - CVE-2016-7836\n",
      "  Group: G0066\n",
      "    - CVE-2012-1875\n",
      "    - CVE-2012-0779\n",
      "    - CVE-2012-1535\n",
      "    - CVE-2011-0609\n",
      "    - CVE-2010-0249\n",
      "    ... and 1 more\n",
      "  Group: G0001\n",
      "    - CVE-2013-3163\n",
      "    - CVE-2011-2462\n",
      "    - CVE-2013-3893\n",
      "  Group: G0107\n",
      "    - CVE-2016-0051\n",
      "  Group: G1002\n",
      "    - CVE-2021-28310\n",
      "  Group: G1019\n",
      "    - CVE-2022-27926\n",
      "  Group: G0008\n",
      "    - CVE-2013-3660\n",
      "  Group: G1024\n",
      "    - CVE-2019-6693\n",
      "    - CVE-2022-40684\n",
      "    - CVE-2023-35078\n",
      "  Group: G0067\n",
      "    - CVE-2013-0808\n",
      "    - CVE-2021-26411\n",
      "    - CVE-2016-0147\n",
      "    - CVE-2020-1380\n",
      "    - CVE-2018-8120\n",
      "  Group: G0128\n",
      "    - CVE-2011-3402\n",
      "    - CVE-2017-0005\n",
      "    - CVE-2013-3128\n",
      "  Group: G0082\n",
      "    - CVE-2015-6585\n",
      "  Group: G0068\n",
      "    - CVE-2013-1331\n",
      "    - CVE-2015-2546\n",
      "  Group: G0096\n",
      "    - CVE-2019-3369\n",
      "    - CVE-2021-44207\n",
      "  Group: G0020\n",
      "    - CVE-2012-0159\n",
      "    - CVE-2013-3918\n",
      "  Group: G0123\n",
      "    - CVE-2012-3152\n",
      "    - CVE-2019-11581\n",
      "  Group: G1016\n",
      "    - CVE-2010-5326\n",
      "    - CVE-2017-100048\n",
      "    - CVE-2001-0507\n",
      "    - CVE-2015-7450\n",
      "  Group: G0087\n",
      "    - CVE-2019-13720\n",
      "  Group: G0063\n",
      "    - CVE-2017-11292\n"
     ]
    }
   ],
   "source": [
    "cve_analysis = analyze_data(group_cve_map, data_type=\"cves\")\n",
    "#ttp_analysis = analyze_data(group_ttp_map, data_type=\"ttps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ca65d2-d7b1-41f1-a150-91feea8c3bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
