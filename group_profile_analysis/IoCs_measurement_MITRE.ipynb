{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09b150f7-dc57-43e3-a304-f572ff1c89c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7278c6c-22cb-413a-a0c6-ebf14d1c92fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load configuration from the JSON file located outside the current directory\n",
    "config_file_path = os.path.join(\"..\", \"Malpedia Bib files Analysis\", \"config.json\")\n",
    "\n",
    "with open(config_file_path, \"r\") as config_file:\n",
    "    config = json.load(config_file)\n",
    "\n",
    "# Step 2: Extract the data directory and file paths for groups and software from the configuration\n",
    "data_directory = config[\"data_directory\"]\n",
    "groups_file_paths = {key: os.path.normpath(os.path.join(data_directory, value)) for key, value in config[\"file_paths_groups_v15\"].items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "299fe012-87e2-4e8d-8be8-ab78bf6a45ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_citations(citations_raw):\n",
    "    \"\"\"\n",
    "    This function takes a raw string of citations, extracts individual citations,\n",
    "    removes duplicates, cleans up irrelevant characters, and ensures no extra spaces or invalid characters.\n",
    "\n",
    "    Args:\n",
    "        citations_raw (str): The raw citation string from which citations are to be extracted.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique citations.\n",
    "    \"\"\"\n",
    "    #print(citations_raw)\n",
    "    if pd.isna(citations_raw) or citations_raw == '':\n",
    "        return []\n",
    "\n",
    "    # Replace any occurrence of 'Citation: ' to standardize the string\n",
    "    citations_raw = citations_raw.replace(\"Citation: \", \"\")\n",
    "    \n",
    "    # Split citations by both the delimiters '), (' and ')('\n",
    "    citations = re.split(r\"\\)\\s*,?\\s*\\(|\\)\\(\", citations_raw)\n",
    "\n",
    "    # Clean each citation by removing unwanted characters and extra spaces\n",
    "    cleaned_citations = []\n",
    "    for citation in citations:\n",
    "        citation = citation.replace(\"(\", \"\").replace(\")\", \"\").strip(\", \").strip()\n",
    "        \n",
    "        # Use regex to extract only the relevant citation portion before any period or embedded sentence\n",
    "        # Here, we match the first part (e.g., \"US-CERT HIDDEN COBRA June 2017\") and ignore the rest\n",
    "        citation = re.sub(r'\\s*\\..*$', '', citation)  # Remove everything after the first period\n",
    "        \n",
    "        # We also use another regex to handle extra commas, which might still linger after cleaning\n",
    "        citation = re.sub(r',\\s*$', '', citation)  # Remove comma at the end of citation\n",
    "        \n",
    "        if citation:  # Only add to the list if there is any valid citation\n",
    "            cleaned_citations.append(citation)\n",
    "    \n",
    "    # Remove duplicates by converting the list to a set and back to a list\n",
    "    cleaned_citations = list(set(cleaned_citations))\n",
    "\n",
    "    #print(cleaned_citations)\n",
    "\n",
    "    return cleaned_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07fd6e24-3d8a-47bf-ae90-7af77a711d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_group_citation_map(excel_file):\n",
    "    \"\"\"\n",
    "    Build a group citation map from a given MITRE ATT&CK group Excel file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    excel_file : str\n",
    "        Path to the Excel file containing 'groups' and 'citations' sheets.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A mapping of group ID to associated and relationship citations (with URLs).\n",
    "    \"\"\"\n",
    "    df_groups = pd.read_excel(excel_file, sheet_name='groups')\n",
    "    df_citations = pd.read_excel(excel_file, sheet_name='citations')\n",
    "\n",
    "    citation_map = dict(zip(df_citations['reference'], df_citations['url']))\n",
    "    group_citations = {}\n",
    "\n",
    "    for _, row in df_groups.iterrows():\n",
    "        group_id = row['ID']\n",
    "\n",
    "        associated_raw = row.get('associated groups citations')\n",
    "        relationship_raw = row.get('relationship citations')\n",
    "\n",
    "        associated = extract_citations(associated_raw) if pd.notna(associated_raw) else []\n",
    "        relationship = extract_citations(relationship_raw) if pd.notna(relationship_raw) else []\n",
    "\n",
    "        associated_with_urls = {\n",
    "            c: citation_map.get(c, \"URL not found\") for c in associated\n",
    "        }\n",
    "        relationship_with_urls = {\n",
    "            c: citation_map.get(c, \"URL not found\") for c in relationship\n",
    "        }\n",
    "\n",
    "        group_citations[group_id] = {\n",
    "            \"associated_groups_citations\": associated_with_urls,\n",
    "            \"relationship_citations\": relationship_with_urls\n",
    "        }\n",
    "\n",
    "    return group_citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "21281719-1905-4562-8384-ac29810b56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_group_citation_maps(group_citations_map, new_data):\n",
    "    \"\"\"\n",
    "    Merge new group citation data into the existing map.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    group_citations_map : dict\n",
    "        The existing group citation map.\n",
    "    new_data : dict\n",
    "        New citation data to merge in.\n",
    "    \"\"\"\n",
    "    for group_id, data in new_data.items():\n",
    "        if group_id not in group_citations_map:\n",
    "            group_citations_map[group_id] = data\n",
    "        else:\n",
    "            # Merge associated citations\n",
    "            group_citations_map[group_id][\"associated_groups_citations\"].update(\n",
    "                data[\"associated_groups_citations\"]\n",
    "            )\n",
    "            # Merge relationship citations\n",
    "            group_citations_map[group_id][\"relationship_citations\"].update(\n",
    "                data[\"relationship_citations\"]\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d3352ff-b74c-48a7-9e0c-5bab6ed2f3eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enterprise: ..\\ATTACK Excel sheets\\enterprise-attack-v15.1-groups.xlsx\n",
      "ics: ..\\ATTACK Excel sheets\\ics-attack-v15.1-groups.xlsx\n",
      "mobile: ..\\ATTACK Excel sheets\\mobile-attack-v15.1-groups.xlsx\n"
     ]
    }
   ],
   "source": [
    "for domain, path in groups_file_paths.items():\n",
    "    print(f\"{domain}: {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e60a50be-f7b0-463b-b132-71a8393d43bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "group_citations_map = {}\n",
    "\n",
    "for file_path in groups_file_paths.values():\n",
    "    new_data = build_group_citation_map(file_path)\n",
    "    merge_group_citation_maps(group_citations_map, new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0ff126d-1f60-4b7b-bc0c-6aa783fa1ce1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "152"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(group_citations_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75073e71-2433-49d3-866c-0578170a021f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of URLs across the whole dictionary: 1212\n",
      "Totak number of groups: 152\n"
     ]
    }
   ],
   "source": [
    "# Initialize counters\n",
    "total_urls = 0  # Total number of URLs across both associated_groups_citations and relationship_citations\n",
    "\n",
    "# Iterate through each group in the dictionary\n",
    "for group_id, group_data in group_citations_map.items():\n",
    "    # Count URLs in 'associated_groups_citations'\n",
    "    associated_urls = group_data.get('associated_groups_citations', {})\n",
    "    total_urls += len(associated_urls)  # Add the number of URLs in associated_groups_citations\n",
    "    \n",
    "    # Count URLs in 'relationship_citations'\n",
    "    relationship_urls = group_data.get('relationship_citations', {})\n",
    "    total_urls += len(relationship_urls)  # Add the number of URLs in relationship_citations\n",
    "\n",
    "# Output the total number of URLs\n",
    "print(f\"Total number of URLs across the whole dictionary: {total_urls}\")\n",
    "num_groups = len(group_citations_map)  \n",
    "print(f\"Totak number of groups: {num_groups}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54a01859-582c-41ea-9bc6-6a01bb8fb261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load JSONL data\n",
    "def load_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Loads a JSONL file into a list of dictionaries.\n",
    "    Each line of the file is parsed as a JSON object.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as file:\n",
    "        json_data = [json.loads(line) for line in file]\n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3b3f510-4d65-4326-a3ff-93e53d786aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonl_file_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\downloads\\downloads\\20241008_downloads.jsonl\"\n",
    "\n",
    "# Step 1: Load the JSONL data\n",
    "jsonl_data = load_jsonl(jsonl_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a70f22a2-5beb-4290-8f98-e510970ef217",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1417"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(jsonl_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe100250-bb9b-4725-b761-68d96a618d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_hash_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Extracts the file hash from a given filename.\n",
    "    \n",
    "    - If the filename ends with '.download.iocs', it removes this suffix.\n",
    "    - Otherwise, if the filename ends with '.iocs', it removes only '.iocs'.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    filename : str\n",
    "        The input filename.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str or None\n",
    "        The extracted file hash, or None if the filename is invalid.\n",
    "    \"\"\"\n",
    "    if filename.endswith('.download.iocs'):\n",
    "        return filename[:-14]  # Remove '.download.iocs' (14 characters)\n",
    "\n",
    "    if filename.endswith('.iocs'):\n",
    "        return filename[:-5]  # Remove '.iocs'\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ad4d217e-5c36-4811-9c74-52f99b5c0be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Folder where the CVE files are stored\n",
    "#folder_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\downloads\\downloads\\CVEs\"\n",
    "\n",
    "# Folder where the TTP files are stored\n",
    "folder_path = r\"C:\\Users\\ricewater\\Documents\\CTIDownloads\\downloads\\downloads\\iocs2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fda59e75-205b-47b3-81a5-36b93831a3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to store file hash -> URL mapping\n",
    "hash_to_url_map = {}\n",
    "# Loop through all files in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    # Extract file hash from the filename\n",
    "    file_hash = extract_file_hash_from_filename(filename)\n",
    "    if file_hash:\n",
    "        # Look for the matching download_sha256 in the JSONL data\n",
    "        for entry in jsonl_data:\n",
    "            if entry.get('download_sha256') == file_hash:\n",
    "                # If the hashes match, store the URL\n",
    "                hash_to_url_map[file_hash] = entry.get('url')\n",
    "                break  # Exit loop once a match is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cc763645-8ae4-4326-9202-e023567e519b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "122\n"
     ]
    }
   ],
   "source": [
    "print(len(hash_to_url_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8789780-0db0-4adb-901e-5eed536bb0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_to_url_map.get(\"e3394ddac9c861e5be26eb67123da606de98786a1f1c6cc06cfeaf151bc67ac8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe85b757-6ade-4f99-a659-aae5532dc9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new dictionary to store the hash, group_id, and URL\n",
    "hash_group_url_map = {}\n",
    "\n",
    "# Iterate through each hash, url pair in hash_to_url_map\n",
    "for file_hash, url in hash_to_url_map.items():\n",
    "    # Iterate through each group in group_citations_map\n",
    "    for group_id, group_data in group_citations_map.items():\n",
    "        # Search for the URL in associated_groups_citations\n",
    "        if url in group_data.get('associated_groups_citations', {}).values():\n",
    "            # If the URL is found, add the file_hash, group_id, and URL to the hash_group_url_map\n",
    "            hash_group_url_map[file_hash] = {'group_id': group_id, 'url': url}\n",
    "            break  # Exit once we find the match (no need to check further in this group)\n",
    "\n",
    "        # Search for the URL in relationship_citations\n",
    "        if url in group_data.get('relationship_citations', {}).values():\n",
    "            # If the URL is found, add the file_hash, group_id, and URL to the hash_group_url_map\n",
    "            hash_group_url_map[file_hash] = {'group_id': group_id, 'url': url}\n",
    "            break  # Exit once we find the match (no need to check further in this group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5edd10df-eaec-4272-9a1f-825083b380f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hash_group_url_map.get(\"e3394ddac9c861e5be26eb67123da606de98786a1f1c6cc06cfeaf151bc67ac8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a42c57e3-fe8b-45ca-bea1-d6527ba9e206",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Find the hashes that are in hash_to_url_map but not in hash_group_url_map\n",
    "missing_hashes = {key: value for key, value in hash_to_url_map.items() if key not in hash_group_url_map}\n",
    "\n",
    "# Print the list of hashes and URLs that are missing in hash_group_url_map\n",
    "for hash_key, url in missing_hashes.items():\n",
    "    print(f\"Hash: {hash_key}, URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b59185ef-1fab-4378-8336-d736bd3f2b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cves_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all the files in a specified folder with .iocs extension and creates a dictionary\n",
    "    with the file hash (from the filename) as the key and the list of CVEs as the value.\n",
    "    It also removes the 'cve' prefix from each CVE and handles the file content appropriately.\n",
    "\n",
    "    Args:\n",
    "        folder_path (str): The path to the folder containing the .iocs files.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where the key is the file hash and the value is a list of CVEs.\n",
    "    \"\"\"\n",
    "    iocs_dict = {}\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Check if the file has a '.iocs' extension\n",
    "        if filename.endswith(\".iocs\"):\n",
    "            file_hash = filename.split(\".\")[0]  # Extract file hash from the filename\n",
    "            \n",
    "            # Construct the full path to the file\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            try:\n",
    "                # Open the file and read its content\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read().strip()  # Read and strip any unwanted whitespace\n",
    "\n",
    "                    # Remove \"cve\" prefix and the tab character, and store CVEs in a list\n",
    "                    content_lines = content.split('\\n')  # Split content by lines\n",
    "                    cleaned_content = []\n",
    "                    for line in content_lines:\n",
    "                        if line.startswith(\"cve\"):\n",
    "                            # Remove 'cve' and the tab character '\\t'\n",
    "                            cleaned_line = line[4:].strip()  # Strip the 'cve' prefix and any leading/trailing spaces\n",
    "                            cleaned_content.append(cleaned_line)\n",
    "\n",
    "                # Store the file hash and list of CVEs in the dictionary\n",
    "                iocs_dict[file_hash] = cleaned_content\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {filename}: {e}\")\n",
    "\n",
    "    return iocs_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ed43b1b-9b1b-4afd-9680-ebd322bc170b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#iocs_data = read_cves_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b64a82b4-03b4-4621-be1a-4d22bbf89221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_ttps_in_folder(folder_path):\n",
    "    \"\"\"\n",
    "    Reads all .download.iocs files in a specified folder and extracts TTPs (Technique IDs).\n",
    "    \n",
    "    - Extracts the file hash from the filename.\n",
    "    - Parses the file to extract TTPs (entries starting with 'ttp\\tT').\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        The path to the folder containing .download.iocs files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where keys are file hashes and values are lists of extracted TTPs.\n",
    "    \"\"\"\n",
    "    ttps_dict = {}\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith(\".download.iocs\"):\n",
    "            file_hash = extract_file_hash_from_filename(filename)\n",
    "            \n",
    "            if not file_hash:\n",
    "                print(f\"Skipping invalid filename: {filename}\")\n",
    "                continue  # Skip files that don't match expected format\n",
    "            \n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            \n",
    "            try:\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    # Extract TTPs from each line that starts with 'ttp\\tT'\n",
    "                    ttps = [\n",
    "                        line.split('\\t')[1].split(' ')[0]  # Extract only the TTP ID (e.g., T1204)\n",
    "                        for line in file.read().strip().split('\\n')\n",
    "                        if line.startswith(\"ttp\\tT\")\n",
    "                    ]\n",
    "\n",
    "                # Store results in dictionary\n",
    "                ttps_dict[file_hash] = ttps\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error reading file {filename}: {e}\")\n",
    "    \n",
    "    return ttps_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0bd7deb9-946e-43c2-b1b6-ce1f1778039d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ttps_data = read_ttps_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2585a6de-ebca-47b6-919c-329e77224aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_hash_data(iocs_data, hash_group_url_map, data_type='cves'):\n",
    "    \"\"\"\n",
    "    Combines hash-based IOC data with group and URL information.\n",
    "    \n",
    "    Args:\n",
    "        iocs_data (dict): Dictionary where keys are file hashes and values are lists of CVEs or TTPs.\n",
    "        hash_group_url_map (dict): Dictionary mapping file hashes to group information (group_id, URL).\n",
    "        data_type (str): Type of data to process ('cves' or 'ttps').\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are group IDs and values contain hashes and associated data.\n",
    "    \"\"\"\n",
    "    hash_data_map = {}\n",
    "    \n",
    "    # Step 1: Iterate through the hashes in iocs_data\n",
    "    for file_hash, data_list in iocs_data.items():\n",
    "        # Check if the file_hash exists in hash_group_url_map\n",
    "        if file_hash in hash_group_url_map:\n",
    "            group_info = hash_group_url_map[file_hash]\n",
    "            \n",
    "            # Combine the data: CVEs/TTPs, group_id, and URL\n",
    "            hash_data_map[file_hash] = {\n",
    "                data_type: data_list,  # 'cves' or 'ttps'\n",
    "                'group_id': group_info['group_id'],\n",
    "                'url': group_info['url']\n",
    "            }\n",
    "    \n",
    "    # Step 2: Transform hash_data_map into a group-based structure\n",
    "    group_data_map = {}\n",
    "    \n",
    "    for hash_val, data in hash_data_map.items():\n",
    "        group_id = data['group_id']\n",
    "        extracted_data = data[data_type]\n",
    "        url = data['url']\n",
    "        \n",
    "        # Initialize group entry if not present\n",
    "        if group_id not in group_data_map:\n",
    "            group_data_map[group_id] = {\n",
    "                'hashes': [],\n",
    "                'url': url,\n",
    "            }\n",
    "        \n",
    "        # Append hash and extracted data (CVEs or TTPs)\n",
    "        group_data_map[group_id]['hashes'].append({\n",
    "            'hash': hash_val,\n",
    "            data_type: extracted_data,\n",
    "        })\n",
    "    \n",
    "    return group_data_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a1555d7b-772a-4295-b731-0121247a4ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For CVEs\n",
    "#group_cve_map = combine_hash_data(iocs_data, hash_group_url_map, data_type='cves')\n",
    "\n",
    "# For TTPs\n",
    "group_ttp_map = combine_hash_data(ttps_data, hash_group_url_map, data_type='ttps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "55c20676-7562-44e6-a2b6-532b94ec5e79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "63"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(group_ttp_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d3f48e6d-901b-4cf2-9900-093fa642a415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups in TTP data: 63\n"
     ]
    }
   ],
   "source": [
    "def dump_and_count_data(data, data_type='cve', file_prefix='MITRE'):\n",
    "    \"\"\"\n",
    "    Dumps the provided data to a JSON file and counts the number of keys.\n",
    "\n",
    "    Args:\n",
    "        data (dict): The data dictionary to be saved.\n",
    "        data_type (str): The type of data ('cves' or 'ttps').\n",
    "        file_prefix (str): Prefix for the filename.\n",
    "\n",
    "    Returns:\n",
    "        int: The number of keys in the JSON file.\n",
    "    \"\"\"\n",
    "    # Define file name based on data type\n",
    "    file_path = f'{file_prefix}_{data_type}_group_analysis.json'\n",
    "    \n",
    "    # Dump data to JSON file\n",
    "    with open(file_path, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    \n",
    "    # Read and count the number of keys in the JSON file\n",
    "    with open(file_path, 'r') as file:\n",
    "        loaded_data = json.load(file)\n",
    "    \n",
    "    return len(loaded_data)\n",
    "\n",
    "# Example usage:\n",
    "#num_cve_keys = dump_and_count_data(group_cve_map, data_type='cve')\n",
    "num_ttp_keys = dump_and_count_data(group_ttp_map, data_type='ttp')\n",
    "print(f\"Number of groups in TTP data: {num_ttp_keys}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "869be641-8827-45fa-8ecd-706c082d25ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of TTPS: 6285\n",
      "Total number of unique TTPS: 470\n",
      "Top 10 most common TTPS (overall occurrences):\n",
      "T1053: 705\n",
      "T1064: 466\n",
      "T1140: 271\n",
      "T1082: 269\n",
      "T1059: 261\n",
      "T1204: 254\n",
      "T1016: 250\n",
      "T1012: 245\n",
      "T1060: 241\n",
      "T1106: 234\n",
      "\n",
      "Top 10 most common TTPS (across different groups):\n",
      "T1082: 30\n",
      "T1083: 29\n",
      "T1041: 28\n",
      "T1140: 26\n",
      "T1027: 25\n",
      "T1005: 24\n",
      "T1057: 23\n",
      "T1059: 22\n",
      "T1053: 20\n",
      "T1071.001: 20\n",
      "\n",
      "Number of groups with unique TTPS: 42\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_data(group_data_map, data_type=\"cves\"):\n",
    "    \"\"\"\n",
    "    Analyzes the given group data (CVE or TTP) and provides statistical insights.\n",
    "\n",
    "    Args:\n",
    "        group_data_map (dict): Dictionary where keys are group IDs and values contain hashes and data lists.\n",
    "        data_type (str): Either \"cves\" or \"ttps\" to specify the type of data being analyzed.\n",
    "\n",
    "    Returns:\n",
    "        dict: Analysis results including total count, unique count, top 10 common items overall,\n",
    "              top 10 by group frequency, and unique items per group.\n",
    "    \"\"\"\n",
    "    all_items = []\n",
    "    overall_counter = Counter()\n",
    "    groupwise_counter = Counter()\n",
    "    unique_items_per_group = {}\n",
    "\n",
    "    for group_id, group_data in group_data_map.items():\n",
    "        group_items = set()\n",
    "        for hash_data in group_data.get(\"hashes\", []):\n",
    "            items = hash_data.get(data_type, [])\n",
    "            all_items.extend(items)\n",
    "            overall_counter.update(items)\n",
    "            group_items.update(items)\n",
    "\n",
    "        # Update groupwise counter (count each item once per group)\n",
    "        groupwise_counter.update(group_items)\n",
    "\n",
    "        # Compute unique items for this group\n",
    "        other_items = set()\n",
    "        for other_id, other_data in group_data_map.items():\n",
    "            if other_id != group_id:\n",
    "                for hash_data in other_data.get(\"hashes\", []):\n",
    "                    other_items.update(hash_data.get(data_type, []))\n",
    "        unique_items = group_items - other_items\n",
    "        if unique_items:\n",
    "            unique_items_per_group[group_id] = list(unique_items)\n",
    "\n",
    "    total_items = len(all_items)\n",
    "    unique_items = set(all_items)\n",
    "    total_unique_items = len(unique_items)\n",
    "\n",
    "    top_10_overall = overall_counter.most_common(10)\n",
    "    top_10_by_group = groupwise_counter.most_common(10)\n",
    "\n",
    "    # Print analysis results\n",
    "    print(f\"Total number of {data_type.upper()}: {total_items}\")\n",
    "    print(f\"Total number of unique {data_type.upper()}: {total_unique_items}\")\n",
    "    print(f\"Top 10 most common {data_type.upper()} (overall occurrences):\")\n",
    "    for item, count in top_10_overall:\n",
    "        print(f\"{item}: {count}\")\n",
    "    print(f\"\\nTop 10 most common {data_type.upper()} (across different groups):\")\n",
    "    for item, count in top_10_by_group:\n",
    "        print(f\"{item}: {count}\")\n",
    "    print(f\"\\nNumber of groups with unique {data_type.upper()}: {len(unique_items_per_group)}\")\n",
    "\n",
    "    return {\n",
    "        \"total_count\": total_items,\n",
    "        \"unique_count\": total_unique_items,\n",
    "        \"top_10_overall\": top_10_overall,\n",
    "        \"top_10_by_group\": top_10_by_group,\n",
    "        \"unique_per_group\": unique_items_per_group\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "#cve_analysis = analyze_data(group_cve_map, data_type=\"cves\")\n",
    "ttp_analysis = analyze_data(group_ttp_map, data_type=\"ttps\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f071f05d-dfca-43d7-9a64-877c83b281f9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
