{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "37145c57-eedb-4023-955d-b5804baaaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Load the config file (if you have a config.json file)\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "89acccec-3cec-4af5-9fd3-015f6f07bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022 path: C:\\Users\\ricewater\\Documents\\CTITTP\\Malpedia Bib files Analysis\\data\\malpedia-db_2022-07-18_downloader.jsonl\n",
      "2024 path: C:\\Users\\ricewater\\Documents\\CTITTP\\Malpedia Bib files Analysis\\data\\20241204_downloads.jsonl\n",
      "Total number of entries: 15768\n",
      "First entry from the combined data:\n",
      "{'author': 'CERT Division', 'date': '2000', 'download_mime': 'text/html', 'download_redirects': ['https://resources.sei.cmu.edu/library/asset-view.cfm?assetID=496186'], 'download_sha256': 'f2c405b383ebaf4d0793f8d5162841b953d06947a711f7d34242faa20e285a04', 'download_size': 41745, 'download_status': 200, 'download_ts': '2022-07-19 12:43:41.400938+00:00', 'language': 'English', 'organization': 'Carnegie Mellon University', 'origin': ['malpedia:CarnegieMellonUniversity'], 'title': '2000 CERT Advisories', 'url': 'https://resources.sei.cmu.edu/library/asset-view.cfm?assetID=496186'}\n"
     ]
    }
   ],
   "source": [
    "# Function to read JSONL file\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns a list of JSON objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the JSONL file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of JSON objects.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Use os.path.join to construct paths properly\n",
    "base_dir = os.getcwd()\n",
    "malpedia_2022_path = os.path.join(base_dir, *config['jsonl_files']['Malpedia_2022'].split('/'))\n",
    "malpedia_2024_path = os.path.join(base_dir, *config['jsonl_files']['Malpedia_2024'].split('/'))\n",
    "\n",
    "# Print to verify\n",
    "print(f\"2022 path: {malpedia_2022_path}\")\n",
    "print(f\"2024 path: {malpedia_2024_path}\")\n",
    "\n",
    "# Read the files\n",
    "malpedia_2022_data = read_jsonl(malpedia_2022_path)\n",
    "malpedia_2024_data = read_jsonl(malpedia_2024_path)\n",
    "\n",
    "# Combine\n",
    "combined_data = malpedia_2022_data + malpedia_2024_data\n",
    "print(f\"Total number of entries: {len(combined_data)}\")\n",
    "print(\"First entry from the combined data:\")\n",
    "print(combined_data[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2d91794-3051-4577-981e-a7a6cb6b9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hashes(directories):\n",
    "    \"\"\"\n",
    "    Extracts file hashes from filenames in the given directories.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directories : list\n",
    "        A list of directory paths containing the files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of file hashes extracted from the filenames.\n",
    "    \"\"\"\n",
    "    file_hashes = []\n",
    "    \n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".iocs\"):\n",
    "                # Extract the hash from the filename (everything before '.iocs')\n",
    "                file_hash = filename.split('.')[0]\n",
    "                file_hashes.append(file_hash)\n",
    "\n",
    "            elif filename.endswith(\".download.iocs\"):\n",
    "                # Remove the '.download.iocs' suffix from the filename\n",
    "                file_hash = filename.removesuffix(\".download.iocs\")\n",
    "                file_hashes.append(file_hash)\n",
    "\n",
    "    # Remove empty values or invalid hashes\n",
    "    #file_hashes = [h for h in file_hashes if h.strip()]\n",
    "    \n",
    "    return file_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00b63530-a006-4fbd-b4d0-1b4de9322862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get directories from the config ## use CVE_Malpedia if querying CVE\n",
    "directories = [\n",
    "    config[\"directory_paths\"][\"CVE_Malpedia_2022\"],\n",
    "    config[\"directory_paths\"][\"CVE_Malpedia_2024\"]\n",
    "]\n",
    "\n",
    "# Extract file hashes from the directory\n",
    "file_hashes = get_file_hashes(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc687316-3c0d-43cf-b5ae-d2ce21f3097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map file hash to url\n",
    "def create_hash_to_url_map(file_hashes, combined_data):\n",
    "    \"\"\"\n",
    "    Maps file hashes to URLs based on the download_sha256 field in the combined_data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_hashes : list\n",
    "        A list of file hashes.\n",
    "    combined_data : list\n",
    "        A list of dictionaries (combined JSONL data) with download_sha256 and url fields.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary mapping file hashes to URLs.\n",
    "    \"\"\"\n",
    "    hash_to_url = {}\n",
    "    for file_hash in file_hashes:\n",
    "        for entry in combined_data:\n",
    "            if entry.get(\"download_sha256\") == file_hash:\n",
    "                hash_to_url[file_hash] = entry.get(\"url\")\n",
    "                break  # Stop searching once we find the match for the hash\n",
    "    return hash_to_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b022e619-2036-455a-8950-1e896dd95cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000c1ee0c1bab222569623c47397f634b77c4124ad8bb0b0d2533ec98fcc6a16 https://attack.mitre.org/groups/G0046/\n"
     ]
    }
   ],
   "source": [
    "# Create the mapping of file hashes to URLs\n",
    "hash_to_url_map = create_hash_to_url_map(file_hashes, combined_data)\n",
    "\n",
    "first_key, first_value = next(iter(hash_to_url_map.items()))\n",
    "print(first_key, first_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b5935f3f-8949-4b19-95d4-8c540e702211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.clearskysec.com/wp-content/uploads/2021/01/Lebanese-Cedar-APT.pdf'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash_to_url_map.get(\"cb10915f45e3c27ccd203dd3f69aad162802d8db568c9010ee696ff631caa41e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b07c37b6-cd5f-457a-9c92-e957998a4c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique file hashes: 2834\n",
      "Total URLs: 2834\n",
      "Total unique URLs: 2813\n",
      "There are 21 duplicate URLs.\n",
      "\n",
      "Hashes associated with duplicate URLs:\n",
      "Hash: 17a80fd5b67d6896b645e9392684ef7a133fb8331820bd591af003e5fe9177e0 -> URL: https://www.fortinet.com/blog/threat-research/new-strrat-rat-phishing-campaign\n",
      "Hash: 8ac55da2268700ddbc04a4904a12fd67a4cd12cf7303204f14443d97f94e2a2f -> URL: https://www.systemtek.co.uk/2018/07/luoxk-malware-exploiting-cve-2018-2893/\n",
      "Hash: 053f9746cc4513e740852710e36d3fd560459ca4d3ee63e382cf6f70b9c2c99b -> URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-264a\n",
      "Hash: 0576cf4b41d9585d5d7755c277c8e8a13299e20c01cb9fca3e22bf5ca7c30635 -> URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-249a\n",
      "Hash: 08793b766badab4e0c8616907d6d870e8905a8bd232428ba514bab464001d1a4 -> URL: https://www.cisa.gov/news-events/alerts/2023/07/28/cisa-releases-malware-analysis-reports-barracuda-backdoors\n",
      "Hash: 181123657b7738daa558f2dfd33f1abef6a9e226e4772ad9e2be4fae3fc1f61d -> URL: https://www.cisa.gov/news-events/analysis-reports/ar23-250a-0\n",
      "Hash: 1a9d2de4a662d9f7a9a4afa0dd7e180de606adc6221e43289e1b68e25eb7d006 -> URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-277a\n",
      "Hash: 2942ec9e1776b594cc73c288be77d587f21c6297f4e2b868822816c0cb963215 -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-347a\n",
      "Hash: 2c1c5dbb7512a2f35b28a835647b470fb50ba33f598a3921af0c1051570308e4 -> URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-279a\n",
      "Hash: 3429950ccf75c0cfb16ee10a5e1614b1c7f905b93652180fc1e762a75466bcba -> URL: https://www.cisa.gov/news-events/alerts/2023/07/28/cisa-releases-malware-analysis-reports-barracuda-backdoors\n",
      "Hash: 3afb7968bb204a2fff3d4b0a08c759b85afdaf2c825ed53174d2038bddf27461 -> URL: https://www.trellix.com/about/newsroom/stories/research/scattered-spider-the-modus-operandi/\n",
      "Hash: 3cef5e15b95096a29fe392a1f29f1c4d42fed814dac395d57b8b795bd6351bde -> URL: https://www.cisa.gov/news-events/analysis-reports/ar23-209b\n",
      "Hash: 4b2248451a00b8a3b6615e8d043f55dbc2131998adbd7916c56b555dcd923710 -> URL: https://www.fortinet.com/blog/threat-research/new-strrat-rat-phishing-campaign\n",
      "Hash: 4c867052e6301ee00a4700dc6ecc2eb3ba3317d118825fc9d9a88704536487b3 -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a\n",
      "Hash: 506c9fcf3e00a82506b45348875d39dcdb982463f243715be9689be8b446ec79 -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-038a\n",
      "Hash: 594d67fae6b856b461ec0e67385cff23e090a47df731a51264bd15d78bcb5895 -> URL: https://ics-cert.kaspersky.com/publications/reports/2024/04/02/apt-and-financial-attacks-on-industrial-organizations-in-h2-2023/\n",
      "Hash: 5ec55507f9f93c1fcfdf96d11d9cf75fa90b44725fd3ded17612c6a29d5ee05f -> URL: https://www.trellix.com/about/newsroom/stories/research/scattered-spider-the-modus-operandi/\n",
      "Hash: 6382aadc8f15dc73587a2949063c0f0970adc00002b9f9947e1f2c22b8930dd8 -> URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-257a\n",
      "Hash: 6ffceaa7c41c309c23e22403ceeac4b1c022c22ecaa05f83659a80449b635424 -> URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-277a\n",
      "Hash: 8da48c8c5930dfaa8611e77e284a36f0b451a6d8adf6f7a7e6a1138e59807f7c -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-144a\n",
      "Hash: 8ee69c559bb2799e58252d4178153ab7a0149a1f60133c800f6ea3bb56ea461e -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-038a\n",
      "Hash: 902064c4b34a7fb70fb8af4442c9620994ba35944cb8e7b9a57945bde81af895 -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-187a\n",
      "Hash: 96e0b0c26569002277f9d714f9270d2027767210af0b2d19db30e3c1982acd14 -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a\n",
      "Hash: ab461652b634f8a8b6127c69628988cea66432c95bb3974ade9fcdaa4048040a -> URL: https://ics-cert.kaspersky.com/publications/reports/2024/04/02/apt-and-financial-attacks-on-industrial-organizations-in-h2-2023/\n",
      "Hash: af9b2f834fb700b3b3b0db6624a4f3a3fe67d6bfed359da95c3ccabf7cfa5ca9 -> URL: https://www.cisa.gov/news-events/analysis-reports/ar23-209a\n",
      "Hash: b3b46e250fd8e13f8db0fe8ce6e613648391d5dee925672fda347ddbdc2eab8d -> URL: https://www.cisa.gov/news-events/analysis-reports/ar23-250a-0\n",
      "Hash: b6b18a011163c8d9da4ce6bc85d4149e561165c49bfef1d7433501cef017cbbc -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-144a\n",
      "Hash: b78eb779048ba8b3e0ba0e979a4e951c5cb031edbe164d6e873bc9e2e9b76f76 -> URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-264a\n",
      "Hash: b7cb267260fb1cf180e48bcb1216ffe6ab2ad6daf3aedd996894fe03f1893f9c -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-290a\n",
      "Hash: bc85ece15fcfe76e15b325ae7eae8c37fa4472686a7e098f0a077dcf492a9a9b -> URL: https://www.systemtek.co.uk/2018/07/luoxk-malware-exploiting-cve-2018-2893/\n",
      "Hash: c0bacac17fc122f2fc277a679e1eee02bfe0234ceddf0e19054ce02c8ab0a458 -> URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-249a\n",
      "Hash: c4935a40a9c232900b817e164186647524b05fd38fad60aabd8d552fe53570eb -> URL: https://www.cisa.gov/news-events/analysis-reports/ar23-209a\n",
      "Hash: c5fa2d7e73eeda7870a668cab9eca10462356303e15ff394a9d682082858537d -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-040a\n",
      "Hash: c8a9d891229255cde957bf9134b11b5f2e5bfcc5ddf83d4e9e6fd131e4daa6c7 -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-187a\n",
      "Hash: ca230c2bcdcad79fd86d110c217f2352d26332ecc77d711831b2c694d40f4dfa -> URL: https://www.cisa.gov/news-events/analysis-reports/ar23-209b\n",
      "Hash: cfc2ef45df23e4e78310834b61478ef22cb565835d8316d88fffeb7290f51a57 -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-347a\n",
      "Hash: cff258e44bd2d9aab853244a42c1c6b1d1676f0596896021425703f688e15f50 -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-290a\n",
      "Hash: d73d022a1737075df051b56c222a6e7a03268e81e4ebbc16aa937349dc88f806 -> URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-257a\n",
      "Hash: df45b9802f94c84c880bd59fd332c2475f6115745a1a954da509ad38fa8a46be -> URL: https://www.trellix.com/en-us/about/newsroom/stories/research/cyberattacks-targeting-ukraine-increase.html\n",
      "Hash: e802799202f7b1eaf11768508eea88470cec93861ad16d68a9fcb4c5cec43b70 -> URL: https://www.trellix.com/en-us/about/newsroom/stories/research/cyberattacks-targeting-ukraine-increase.html\n",
      "Hash: e966c68646df0e189be0f99ab854899eb58e1d7a3d8424558483435399911c58 -> URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-040a\n",
      "Hash: f3d28f1ed36cf37d52ee17cda9bc4722d386ec9670871c2399480bc51faf4dbe -> URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-279a\n"
     ]
    }
   ],
   "source": [
    "##Common file hashes found: {'bd90e5d64d43cd326049d739d519c270d9f2856db6c1d140569f152b0fa3b757', \n",
    "##'acd626acf50af8e30a681ccf88662b2bcecd5ec6053c18d6b460a42d9d726764', \n",
    "##'a71555ff127721ad3f47e0427411dde35ec792889c2778ba43571d3a4b3f5cca'}\n",
    "unique_hashes = len(hash_to_url_map)\n",
    "print(f\"Total unique file hashes: {unique_hashes}\")\n",
    "\n",
    "# Check if the URLs are unique\n",
    "urls = list(hash_to_url_map.values())\n",
    "unique_urls = len(set(urls))\n",
    "\n",
    "print(f\"Total URLs: {len(urls)}\")\n",
    "print(f\"Total unique URLs: {unique_urls}\")\n",
    "\n",
    "# Check if there are any duplicate URLs\n",
    "if len(urls) == unique_urls:\n",
    "    print(\"All URLs are unique.\")\n",
    "else:\n",
    "    print(f\"There are {len(urls) - unique_urls} duplicate URLs.\")\n",
    "\n",
    "    # Find and print duplicate URLs\n",
    "    url_counts = {}\n",
    "    \n",
    "    # Count the occurrences of each URL\n",
    "    for url in urls:\n",
    "        if url in url_counts:\n",
    "            url_counts[url] += 1\n",
    "        else:\n",
    "            url_counts[url] = 1\n",
    "\n",
    "    # Filter out the duplicate URLs (those that appear more than once)\n",
    "    duplicate_urls = {url: count for url, count in url_counts.items() if count > 1}\n",
    "\n",
    "    # print(\"Duplicate URLs and their occurrences:\")\n",
    "    # for url, count in duplicate_urls.items():\n",
    "    #     print(f\"URL: {url} -> Occurrences: {count}\")\n",
    "\n",
    "    #Find the hashes associated with the duplicate URLs\n",
    "    print(\"\\nHashes associated with duplicate URLs:\")\n",
    "    for hash_key, url in hash_to_url_map.items():\n",
    "        if url in duplicate_urls:\n",
    "            print(f\"Hash: {hash_key} -> URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "292cf82f-9d72-4a40-b38d-b42b59954669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the actors data from the actors_data.json file and malware families data from malware_families.json\n",
    "def load_actors_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the actors data from a JSON file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the actors_data.json file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the actors data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        actors_data = json.load(f)\n",
    "    return actors_data\n",
    "\n",
    "def load_families_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the malware families data from a JSON file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the actors_data.json file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the families data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        families_data = json.load(f)\n",
    "    return families_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63dc9150-71e3-4a06-9b2d-b6142a30f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Function to normalize group names\n",
    "def normalize_group_name(name):\n",
    "    # Convert to lowercase for case-insensitive comparison\n",
    "    name = name.lower().strip()\n",
    "\n",
    "    # Remove 'team' from names like 'Sandworm Team'\n",
    "    if name.endswith(' team'):\n",
    "        name = name.replace(' team', '')\n",
    "\n",
    "    # Replace 'threat group-' with 'tg-' (e.g., 'Threat Group-1314' -> 'TG-1314')\n",
    "    name = re.sub(r'threat group[- ]', 'tg-', name)\n",
    "\n",
    "    # Remove 'temp.' or similar prefixes (e.g., 'Temp.Pittytiger' -> 'Pittytiger')\n",
    "    name = re.sub(r'^temp[\\. ]+', '', name)\n",
    "\n",
    "    # Normalize spaces and dots (e.g., 'pitty tiger' == 'pitty.tiger')\n",
    "    name = re.sub(r'[\\. ]+', ' ', name)\n",
    "\n",
    "    # Remove common suffixes like 'framework' or 'group' (e.g., 'Inception Framework' -> 'Inception')\n",
    "    name = re.sub(r' (framework|group)$', '', name)\n",
    "\n",
    "    # Standardize 'Confucius' and 'Confucious' to 'confucius'\n",
    "    name = re.sub(r'confucious', 'confucius', name)\n",
    "\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3a665e81-2396-4e48-927a-5790d6dc7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url_to_group_map(actors_data, families_data):\n",
    "    \"\"\"\n",
    "    Creates a mapping from URLs to group names based on the actors data.\n",
    "    If a URL is tagged to multiple actors, it is stored as a list of names.\n",
    "\n",
    "    When processing families_data:\n",
    "    - If the URL is already mapped in actors_data, it is skipped.\n",
    "    - If no actor mappings exist, the attribution field is checked.\n",
    "    - If no attribution is present, the family name is used.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actors_data : dict\n",
    "        A dictionary containing the actors data.\n",
    "    families_data : dict\n",
    "        A dictionary containing the families data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary mapping URLs to a list of group names.\n",
    "    \"\"\"\n",
    "    url_to_group_map = {}\n",
    "\n",
    "    # First, check the actors_data for URLs\n",
    "    for group_name, group_info in actors_data.items():\n",
    "        refs = group_info.get(\"meta\", {}).get(\"refs\", [])\n",
    "        normalized_name = normalize_group_name(group_name)\n",
    "\n",
    "        for url in refs:\n",
    "            if url in url_to_group_map:\n",
    "                if isinstance(url_to_group_map[url], list):\n",
    "                    url_to_group_map[url].append(normalized_name)\n",
    "                else:\n",
    "                    url_to_group_map[url] = [url_to_group_map[url], normalized_name]\n",
    "            else:\n",
    "                url_to_group_map[url] = normalized_name\n",
    "\n",
    "    # Check families_data only for URLs NOT in actors_data\n",
    "    for family_name, family_info in families_data.items():\n",
    "        for url in family_info.get(\"urls\", []):\n",
    "            if url in url_to_group_map:\n",
    "                # Skip processing if already found in actors_data\n",
    "                continue  \n",
    "\n",
    "            attribution = family_info.get(\"attribution\", [])\n",
    "            mapped_names = [normalize_group_name(attr) for attr in attribution] if attribution else [family_name]\n",
    "\n",
    "            # Store as a list even if there's only one name for consistency\n",
    "            url_to_group_map[url] = mapped_names if len(mapped_names) > 1 else mapped_names[0]\n",
    "\n",
    "    return url_to_group_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "869c3f0c-0a8e-4413-a49b-1c05449ffdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load the actors data\n",
    "actors_data_file = \"actors_data.json\"  # Update with the actual file path\n",
    "actors_data = load_actors_data(actors_data_file)\n",
    "families_data_file = \"malware_families.json\"\n",
    "families_data = load_families_data(families_data_file)\n",
    "# Create URL-to-group mapping\n",
    "url_to_group_map = create_url_to_group_map(actors_data, families_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b95e8f8d-b967-42f5-bdec-83590d62f911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11935\n"
     ]
    }
   ],
   "source": [
    "def count_single_mapping_urls(url_to_group_map):\n",
    "    \"\"\"\n",
    "    Counts the number of URLs that are mapped to only one group (threat group or family).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url_to_group_map : dict\n",
    "        The final dictionary mapping URLs to group names (or lists of names).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The count of URLs that are mapped to exactly one group.\n",
    "    \"\"\"\n",
    "    single_mapping_count = 0\n",
    "\n",
    "    for groups in url_to_group_map.values():\n",
    "        if isinstance(groups, str) or (isinstance(groups, list) and len(groups) == 1):\n",
    "            single_mapping_count += 1\n",
    "\n",
    "    return single_mapping_count\n",
    "\n",
    "\n",
    "result = count_single_mapping_urls(url_to_group_map)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b37d303-1530-4c7e-9592-c7a70efd7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_group_to_hash_url_map(hash_to_url_map, url_to_group_map):\n",
    "    \"\"\"\n",
    "    Updates the hash-to-group map with group names as keys and hashes/URLs as associated values.\n",
    "    If a URL is associated with multiple groups, it is added to the \"Unknown\" group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hash_to_url_map : dict\n",
    "        A dictionary mapping file hashes to URLs.\n",
    "    url_to_group_map : dict\n",
    "        A dictionary mapping URLs to group names (can be a list of names).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where group names are keys, and associated values are lists of hashes and URLs.\n",
    "    \"\"\"\n",
    "    group_to_hash_url_map = {}\n",
    "\n",
    "    for file_hash, url in hash_to_url_map.items():\n",
    "        group_names = url_to_group_map.get(url, [\"Unknown\"])  # Default to \"Unknown\" if no group found\n",
    "        \n",
    "        # If the group is a list, choose the first group or default to \"Unknown\" if there are multiple groups\n",
    "        if isinstance(group_names, list):\n",
    "            if len(group_names) > 1:\n",
    "                group_name = \"Unknown\"  # Default to \"Unknown\" if multiple groups\n",
    "            else:\n",
    "                group_name = group_names[0]\n",
    "        else:\n",
    "            group_name = group_names\n",
    "        \n",
    "        # If group name not already in the map, initialize it\n",
    "        if group_name not in group_to_hash_url_map:\n",
    "            group_to_hash_url_map[group_name] = {\"hashes\": [], \"urls\": []}\n",
    "\n",
    "        # Add the hash and URL to the corresponding group\n",
    "        group_to_hash_url_map[group_name][\"hashes\"].append(file_hash)\n",
    "        group_to_hash_url_map[group_name][\"urls\"].append(url)\n",
    "\n",
    "    return group_to_hash_url_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2e29a7e-e580-4553-8d19-7e4593c4794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "591\n"
     ]
    }
   ],
   "source": [
    "group_to_hash_url_map = update_group_to_hash_url_map(hash_to_url_map, url_to_group_map)\n",
    "print(len(group_to_hash_url_map))\n",
    "#first_key, first_value = next(iter(group_to_hash_url_map.items()))\n",
    "#print(first_key, first_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "355bab6a-1ef3-4d3e-af08-2ee15c0a0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique groups: 591\n",
      "Length of the hash-to-group map: 1639\n",
      "Number of unique URLs: 1619\n",
      "Number of hashes with 'Unknown' group: 503\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics\n",
    "def calculate_statistics(group_map):\n",
    "    \"\"\"\n",
    "    Calculate the statistics for the group-to-hash URL map.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    group_map : dict\n",
    "        A dictionary where group names are keys, and each value contains a list of hashes and URLs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing:\n",
    "        - num_unique_groups: Number of unique group names (including 'Unknown')\n",
    "        - num_hashes: Total number of hashes across all groups\n",
    "        - num_unique_urls: Total number of unique URLs across all groups\n",
    "        - unknown_groups_count: Number of hashes associated with the 'Unknown' group\n",
    "    \"\"\"\n",
    "    # Number of unique groups (including 'Unknown')\n",
    "    num_unique_groups = len(group_map)\n",
    "    \n",
    "    # Number of unique URLs\n",
    "    unique_urls = set()\n",
    "    for data in group_map.values():\n",
    "        unique_urls.update(data['urls'])\n",
    "    num_unique_urls = len(unique_urls)\n",
    "    \n",
    "    # Length of the hash-to-group map (total hashes)\n",
    "    num_hashes = sum(len(data['hashes']) for data in group_map.values())\n",
    "    \n",
    "    # Number of hashes with 'Unknown' group\n",
    "    unknown_groups_count = len(group_map.get('Unknown', {}).get('hashes', []))\n",
    "    \n",
    "    return num_unique_groups, num_hashes, num_unique_urls, unknown_groups_count\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'hash_to_group_map' is the result of your update function\n",
    "num_unique_groups, num_hashes, num_unique_urls, unknown_groups_count = calculate_statistics(group_to_hash_url_map)\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Number of unique groups: {num_unique_groups}\")\n",
    "print(f\"Length of the hash-to-group map: {num_hashes}\")\n",
    "print(f\"Number of unique URLs: {num_unique_urls}\")\n",
    "print(f\"Number of hashes with 'Unknown' group: {unknown_groups_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff1ecd6d-2b40-4c3e-830e-d3b8b9c72dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hashes_and_indicators(directories, indicator_type=\"cves\"):\n",
    "    \"\"\"\n",
    "    Extracts file hashes from filenames and reads the content of the files to\n",
    "    extract either CVEs or TTPs associated with each hash, based on the indicator_type parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directories : list\n",
    "        A list of directory paths containing the files.\n",
    "    indicator_type : str, optional\n",
    "        The type of indicators to extract. Accepts \"cve\" or \"ttp\". Default is \"cve\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where the key is the file hash, and the value is a list\n",
    "        of CVEs or TTPs associated with that hash.\n",
    "    \"\"\"\n",
    "    hash_to_indicators = {}\n",
    "    \n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            if indicator_type == \"cves\" and filename.endswith(\".iocs\"):\n",
    "                file_hash = filename.split('.')[0]\n",
    "                indicators = []\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    for line in file:\n",
    "                        parts = line.strip().split(\"\\t\")\n",
    "                        if parts[0] == \"cve\" and len(parts) > 1:\n",
    "                            indicators.append(parts[1])\n",
    "                \n",
    "                if indicators:\n",
    "                    hash_to_indicators[file_hash] = indicators\n",
    "\n",
    "            elif indicator_type == \"ttps\" and filename.endswith(\".download.iocs\"):\n",
    "                # Remove the '.download.iocs' suffix from the filename\n",
    "                file_hash = filename.removesuffix(\".download.iocs\")\n",
    "                file_hashes.append(file_hash) # Remove \".download.iocs\" from filename\n",
    "                indicators = []\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                    for line in file:\n",
    "                        parts = line.strip().split(\"\\t\")\n",
    "                        if len(parts) > 1 and parts[0] == \"ttp\":\n",
    "                            indicators.append(parts[1])\n",
    "                \n",
    "                if indicators:\n",
    "                    hash_to_indicators[file_hash] = indicators\n",
    "    \n",
    "    return hash_to_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e4a2efa2-b7f2-4334-b5eb-a468480b3a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1639\n"
     ]
    }
   ],
   "source": [
    "#cve_data = get_file_hashes_and_indicators(directories, \"cves\")\n",
    "ttp_data = get_file_hashes_and_indicators(directories, \"ttps\")\n",
    "\n",
    "\n",
    "print(len(ttp_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d85f3242-1f09-4383-9f4c-6d7375a92e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_group_hash_with_data(group_hash_to_url_map, data, data_type):\n",
    "    \"\"\"\n",
    "    Updates the group_hash_to_url_map by adding TTPs or CVEs from the respective data dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group_hash_to_url_map : dict\n",
    "        A dictionary where each group contains hashes and URLs.\n",
    "    data : dict\n",
    "        A dictionary where the keys are hashes and the values are lists of TTPs or CVEs.\n",
    "    data_type : str\n",
    "        The type of data being processed ('cve' or 'ttp').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The updated group_hash_to_url_map with TTPs or CVEs added for each hash.\n",
    "    \"\"\"\n",
    "    if data_type not in ('cves', 'ttps'):\n",
    "        raise ValueError(\"Invalid data_type. Must be 'cve' or 'ttp'.\")\n",
    "    \n",
    "    for group_name, group_data in group_hash_to_url_map.items():\n",
    "        updated_hashes = []\n",
    "        \n",
    "        for hash_value in group_data['hashes']:\n",
    "            if hash_value in data:\n",
    "                updated_hashes.append({\n",
    "                    \"hash\": hash_value,\n",
    "                    data_type: data[hash_value]  # Dynamically set key as 'cve' or 'ttp'\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Hash {hash_value} not found in {data_type}_data.\")\n",
    "                updated_hashes.append({\n",
    "                    \"hash\": hash_value,\n",
    "                    data_type: []\n",
    "                })\n",
    "        \n",
    "        group_hash_to_url_map[group_name]['hashes'] = updated_hashes\n",
    "    \n",
    "    return group_hash_to_url_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8ea5a3d-f469-4898-b097-a10258ae6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to update the map  --> May need to restart it if already ran once or else it flashes error\n",
    "\n",
    "\n",
    "updated_data = update_group_hash_with_data(group_to_hash_url_map, ttp_data, \"ttps\")\n",
    "\n",
    "#updated_data = update_group_hash_with_data(group_to_hash_url_map, cve_data, \"cves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63740d77-dcae-4634-aa9f-f6a844df347a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "591"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b5f2fb46-42fb-4193-9581-d03ae9970505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys in the JSON file: 591\n"
     ]
    }
   ],
   "source": [
    "def save_json(data, data_type):\n",
    "    \"\"\"\n",
    "    Saves the given data to a JSON file with a dynamic filename based on the data type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        The dictionary data to be saved.\n",
    "    data_type : str\n",
    "        The type of data being saved, either 'cve' or 'ttp'.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    return filename  # Return filename for further use\n",
    "\n",
    "def count_json_keys(json_file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file and counts the number of keys in it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    json_file_path : str\n",
    "        The path to the JSON file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of keys in the JSON file.\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    return len(data)\n",
    "\n",
    "# Example usage\n",
    "#data_type = \"cve\"\n",
    "data_type = \"ttp\" # Change to \"cve\" when dealing with CVE data\n",
    "filename = f\"Malpedia_{data_type}_group_analysis.json\"\n",
    "filename = save_json(updated_data, data_type)\n",
    "\n",
    "num_keys = count_json_keys(filename)\n",
    "print(f\"Number of keys in the JSON file: {num_keys}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0efaaf15-f4c1-4708-a6c5-31476e67e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "def analyze_data(group_data_map, data_type=\"cves\"):\n",
    "    \"\"\"\n",
    "    Analyzes the given group data (CVE or TTP) and provides statistical insights.\n",
    "\n",
    "    Args:\n",
    "        group_data_map (dict): Dictionary where keys are group IDs and values contain hashes and data lists.\n",
    "        data_type (str): Either \"cves\" or \"ttps\" to specify the type of data being analyzed.\n",
    "\n",
    "    Returns:\n",
    "        dict: Analysis results including total count, unique count, top 10 common items,\n",
    "              unique items per group, count per group, and group-wise item appearance counts.\n",
    "    \"\"\"\n",
    "    all_items = []\n",
    "    group_item_map = defaultdict(set)\n",
    "\n",
    "    # Build global item list and per-group item sets\n",
    "    for group_id, group_data in group_data_map.items():\n",
    "        for hash_data in group_data[\"hashes\"]:\n",
    "            group_item_map[group_id].update(hash_data[data_type])\n",
    "            all_items.extend(hash_data[data_type])\n",
    "\n",
    "    # Overall stats\n",
    "    total_items = len(all_items)\n",
    "    unique_items = set(all_items)\n",
    "    total_unique_items = len(unique_items)\n",
    "\n",
    "    # Count of all items (duplicates included)\n",
    "    item_counter = Counter(all_items)\n",
    "    top_10_items = item_counter.most_common(10)\n",
    "\n",
    "    # Unique items per group\n",
    "    unique_items_per_group = {}\n",
    "    for group_id, items in group_item_map.items():\n",
    "        other_items = set()\n",
    "        for other_group, other_group_items in group_item_map.items():\n",
    "            if other_group != group_id:\n",
    "                other_items.update(other_group_items)\n",
    "        unique = items - other_items\n",
    "        if unique:\n",
    "            unique_items_per_group[group_id] = list(unique)\n",
    "\n",
    "    # CVE/Item count per group\n",
    "    count_per_group = {group_id: len(items) for group_id, items in group_item_map.items()}\n",
    "\n",
    "    # Group occurrence of each item\n",
    "    item_group_count = Counter()\n",
    "    for item in unique_items:\n",
    "        for group_id, items in group_item_map.items():\n",
    "            if item in items:\n",
    "                item_group_count[item] += 1\n",
    "\n",
    "    top_10_common_across_groups = item_group_count.most_common(10)\n",
    "\n",
    "    # Output\n",
    "    print(f\"Total number of {data_type.upper()}: {total_items}\")\n",
    "    print(f\"Total number of unique {data_type.upper()}: {total_unique_items}\")\n",
    "    print(f\"Top 10 most common {data_type.upper()} (overall occurrences):\")\n",
    "    for item, count in top_10_items:\n",
    "        print(f\"{item}: {count}\")\n",
    "    print(f\"\\nTop 10 most common {data_type.upper()} (across different groups):\")\n",
    "    for item, count in top_10_common_across_groups:\n",
    "        print(f\"{item}: {count} groups\")\n",
    "    print(f\"\\n{data_type.upper()} count per group:\")\n",
    "    #for group_id, count in count_per_group.items():\n",
    "    #    print(f\"{group_id}: {count}\")\n",
    "    print(f\"\\nNumber of groups with unique {data_type.upper()}: {len(unique_items_per_group)}\")\n",
    "\n",
    "    return {\n",
    "        \"total_count\": total_items,\n",
    "        \"unique_count\": total_unique_items,\n",
    "        \"top_10_overall\": top_10_items,\n",
    "        \"top_10_across_groups\": top_10_common_across_groups,\n",
    "        \"unique_per_group\": unique_items_per_group,\n",
    "        \"count_per_group\": count_per_group\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d4e2420c-4811-4f2a-83fa-8cbbcdef124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of TTPS: 24457\n",
      "Total number of unique TTPS: 879\n",
      "Top 10 most common TTPS (overall occurrences):\n",
      "T1082: 531\n",
      "T1083: 455\n",
      "T1140: 430\n",
      "T1027: 419\n",
      "T1059: 368\n",
      "T1041: 334\n",
      "T1105: 327\n",
      "T1057: 325\n",
      "T1486: 320\n",
      "T1071.001: 303\n",
      "\n",
      "Top 10 most common TTPS (across different groups):\n",
      "T1082: 269 groups\n",
      "T1083: 243 groups\n",
      "T1140: 236 groups\n",
      "T1027: 216 groups\n",
      "T1059: 187 groups\n",
      "T1041: 176 groups\n",
      "T1105: 170 groups\n",
      "T1057: 168 groups\n",
      "T1486: 149 groups\n",
      "T1071.001: 149 groups\n",
      "\n",
      "TTPS count per group:\n",
      "\n",
      "Number of groups with unique TTPS: 84\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "#cve_analysis = analyze_data(updated_data, data_type=\"cves\")\n",
    "ttp_analysis = analyze_data(updated_data, data_type=\"ttps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1df22-02e3-4f30-a820-68593dccbae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48c2c219-e1fe-4aba-b8ea-13248c9e3dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
