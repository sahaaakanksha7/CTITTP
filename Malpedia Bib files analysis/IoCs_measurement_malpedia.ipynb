{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37145c57-eedb-4023-955d-b5804baaaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Load the config file (if you have a config.json file)\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89acccec-3cec-4af5-9fd3-015f6f07bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries: 15765\n",
      "First entry from the combined data:\n",
      "{'author': 'CERT Division', 'date': '2000', 'download_mime': 'text/html', 'download_redirects': ['https://resources.sei.cmu.edu/library/asset-view.cfm?assetID=496186'], 'download_sha256': 'f2c405b383ebaf4d0793f8d5162841b953d06947a711f7d34242faa20e285a04', 'download_size': 41745, 'download_status': 200, 'download_ts': '2022-07-19 12:43:41.400938+00:00', 'language': 'English', 'organization': 'Carnegie Mellon University', 'origin': ['malpedia:CarnegieMellonUniversity'], 'title': '2000 CERT Advisories', 'url': 'https://resources.sei.cmu.edu/library/asset-view.cfm?assetID=496186'}\n"
     ]
    }
   ],
   "source": [
    "# Function to read JSONL file\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns a list of JSON objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the JSONL file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of JSON objects.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# File paths for the JSONL files\n",
    "malpedia_2022_file = config['jsonl_files']['Malpedia_2022']\n",
    "malpedia_2024_file = config['jsonl_files']['Malpedia_2024']\n",
    "\n",
    "# Ensure the file paths are relative to the running script's directory\n",
    "malpedia_2022_path = os.path.join(os.getcwd(), malpedia_2022_file)\n",
    "malpedia_2024_path = os.path.join(os.getcwd(), malpedia_2024_file)\n",
    "\n",
    "# Read the files\n",
    "malpedia_2022_data = read_jsonl(malpedia_2022_path)\n",
    "malpedia_2024_data = read_jsonl(malpedia_2024_path)\n",
    "\n",
    "# Concatenate the data from both JSONL files\n",
    "combined_data = malpedia_2022_data + malpedia_2024_data\n",
    "\n",
    "# Example: print the length of the combined data and the first entry\n",
    "print(f\"Total number of entries: {len(combined_data)}\")\n",
    "print(\"First entry from the combined data:\")\n",
    "print(combined_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a2d91794-3051-4577-981e-a7a6cb6b9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hashes(directories):\n",
    "    \"\"\"\n",
    "    Extracts file hashes from filenames in the given directories.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directories : list\n",
    "        A list of directory paths containing the files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of file hashes extracted from the filenames.\n",
    "    \"\"\"\n",
    "    file_hashes = []\n",
    "    \n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".iocs\"):\n",
    "                # Extract the hash from the filename (everything before '.iocs')\n",
    "                file_hash = filename.split('.')[0]\n",
    "                file_hashes.append(file_hash)\n",
    "\n",
    "            elif filename.endswith(\".download.iocs\"):\n",
    "                # Remove the '.download.iocs' suffix from the filename\n",
    "                file_hash = filename.removesuffix(\".download.iocs\")\n",
    "                file_hashes.append(file_hash)\n",
    "\n",
    "    # Remove empty values or invalid hashes\n",
    "    #file_hashes = [h for h in file_hashes if h.strip()]\n",
    "    \n",
    "    return file_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b63530-a006-4fbd-b4d0-1b4de9322862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get directories from the config ## use CVE_Malpedia if querying CVE\n",
    "directories = [\n",
    "    config[\"directory_paths\"][\"TTP_Malpedia_2022\"],\n",
    "    config[\"directory_paths\"][\"TTP_Malpedia_2024\"]\n",
    "]\n",
    "\n",
    "# Extract file hashes from the directory\n",
    "file_hashes = get_file_hashes(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc687316-3c0d-43cf-b5ae-d2ce21f3097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map file hash to url\n",
    "def create_hash_to_url_map(file_hashes, combined_data):\n",
    "    \"\"\"\n",
    "    Maps file hashes to URLs based on the download_sha256 field in the combined_data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_hashes : list\n",
    "        A list of file hashes.\n",
    "    combined_data : list\n",
    "        A list of dictionaries (combined JSONL data) with download_sha256 and url fields.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary mapping file hashes to URLs.\n",
    "    \"\"\"\n",
    "    hash_to_url = {}\n",
    "    for file_hash in file_hashes:\n",
    "        for entry in combined_data:\n",
    "            if entry.get(\"download_sha256\") == file_hash:\n",
    "                hash_to_url[file_hash] = entry.get(\"url\")\n",
    "                break  # Stop searching once we find the match for the hash\n",
    "    return hash_to_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b022e619-2036-455a-8950-1e896dd95cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00090904f5cf8855553fb323ee9a1d1fc089e75c948f560ed5b95eaa914a61de https://www.infinitumit.com.tr/en/conti-ransomware-group-behind-the-karakurt-hacking-team/\n"
     ]
    }
   ],
   "source": [
    "# Create the mapping of file hashes to URLs\n",
    "hash_to_url_map = create_hash_to_url_map(file_hashes, combined_data)\n",
    "\n",
    "first_key, first_value = next(iter(hash_to_url_map.items()))\n",
    "print(first_key, first_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3561e314-b6bf-434a-ab18-b4cdc7cceba5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1637"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hash_to_url_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b07c37b6-cd5f-457a-9c92-e957998a4c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique file hashes: 1637\n",
      "Total URLs: 1637\n",
      "Total unique URLs: 1617\n",
      "There are 20 duplicate URLs.\n"
     ]
    }
   ],
   "source": [
    "##Common file hashes found: {'bd90e5d64d43cd326049d739d519c270d9f2856db6c1d140569f152b0fa3b757', \n",
    "##'acd626acf50af8e30a681ccf88662b2bcecd5ec6053c18d6b460a42d9d726764', \n",
    "##'a71555ff127721ad3f47e0427411dde35ec792889c2778ba43571d3a4b3f5cca'}\n",
    "unique_hashes = len(hash_to_url_map)\n",
    "print(f\"Total unique file hashes: {unique_hashes}\")\n",
    "\n",
    "# Check if the URLs are unique\n",
    "urls = list(hash_to_url_map.values())\n",
    "unique_urls = len(set(urls))\n",
    "\n",
    "print(f\"Total URLs: {len(urls)}\")\n",
    "print(f\"Total unique URLs: {unique_urls}\")\n",
    "\n",
    "# Check if there are any duplicate URLs\n",
    "if len(urls) == unique_urls:\n",
    "    print(\"All URLs are unique.\")\n",
    "else:\n",
    "    print(f\"There are {len(urls) - unique_urls} duplicate URLs.\")\n",
    "\n",
    "    # Find and print duplicate URLs\n",
    "    url_counts = {}\n",
    "    \n",
    "    # Count the occurrences of each URL\n",
    "    for url in urls:\n",
    "        if url in url_counts:\n",
    "            url_counts[url] += 1\n",
    "        else:\n",
    "            url_counts[url] = 1\n",
    "\n",
    "    # Filter out the duplicate URLs (those that appear more than once)\n",
    "    duplicate_urls = {url: count for url, count in url_counts.items() if count > 1}\n",
    "\n",
    "    # print(\"Duplicate URLs and their occurrences:\")\n",
    "    # for url, count in duplicate_urls.items():\n",
    "    #     print(f\"URL: {url} -> Occurrences: {count}\")\n",
    "\n",
    "    # # Find the hashes associated with the duplicate URLs\n",
    "    # print(\"\\nHashes associated with duplicate URLs:\")\n",
    "    # for hash_key, url in hash_to_url_map.items():\n",
    "    #     if url in duplicate_urls:\n",
    "    #         print(f\"Hash: {hash_key} -> URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "292cf82f-9d72-4a40-b38d-b42b59954669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the actors data from the actors_data.json file and malware families data from malware_families.json\n",
    "def load_actors_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the actors data from a JSON file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the actors_data.json file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the actors data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        actors_data = json.load(f)\n",
    "    return actors_data\n",
    "\n",
    "def load_families_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the malware families data from a JSON file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the actors_data.json file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the families data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        families_data = json.load(f)\n",
    "    return families_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63dc9150-71e3-4a06-9b2d-b6142a30f487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "# Function to normalize group names\n",
    "def normalize_group_name(name):\n",
    "    # Convert to lowercase for case-insensitive comparison\n",
    "    name = name.lower().strip()\n",
    "\n",
    "    # Remove 'team' from names like 'Sandworm Team'\n",
    "    if name.endswith(' team'):\n",
    "        name = name.replace(' team', '')\n",
    "\n",
    "    # Replace 'threat group-' with 'tg-' (e.g., 'Threat Group-1314' -> 'TG-1314')\n",
    "    name = re.sub(r'threat group[- ]', 'tg-', name)\n",
    "\n",
    "    # Remove 'temp.' or similar prefixes (e.g., 'Temp.Pittytiger' -> 'Pittytiger')\n",
    "    name = re.sub(r'^temp[\\. ]+', '', name)\n",
    "\n",
    "    # Normalize spaces and dots (e.g., 'pitty tiger' == 'pitty.tiger')\n",
    "    name = re.sub(r'[\\. ]+', ' ', name)\n",
    "\n",
    "    # Remove common suffixes like 'framework' or 'group' (e.g., 'Inception Framework' -> 'Inception')\n",
    "    name = re.sub(r' (framework|group)$', '', name)\n",
    "\n",
    "    # Standardize 'Confucius' and 'Confucious' to 'confucius'\n",
    "    name = re.sub(r'conficious', 'confucius', name)\n",
    "\n",
    "    return name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a665e81-2396-4e48-927a-5790d6dc7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_url_to_group_map(actors_data, families_data):\n",
    "    \"\"\"\n",
    "    Creates a mapping from URLs to group names based on the actors data.\n",
    "    If a URL is tagged to multiple actors, it is stored as a list of names.\n",
    "\n",
    "    When processing families_data:\n",
    "    - If the URL is already mapped in actors_data, it is skipped.\n",
    "    - If no actor mappings exist, the attribution field is checked.\n",
    "    - If no attribution is present, the family name is used.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actors_data : dict\n",
    "        A dictionary containing the actors data.\n",
    "    families_data : dict\n",
    "        A dictionary containing the families data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary mapping URLs to a list of group names.\n",
    "    \"\"\"\n",
    "    url_to_group_map = {}\n",
    "\n",
    "    # First, check the actors_data for URLs\n",
    "    for group_name, group_info in actors_data.items():\n",
    "        refs = group_info.get(\"meta\", {}).get(\"refs\", [])\n",
    "        normalized_name = normalize_group_name(group_name)\n",
    "\n",
    "        for url in refs:\n",
    "            if url in url_to_group_map:\n",
    "                if isinstance(url_to_group_map[url], list):\n",
    "                    url_to_group_map[url].append(normalized_name)\n",
    "                else:\n",
    "                    url_to_group_map[url] = [url_to_group_map[url], normalized_name]\n",
    "            else:\n",
    "                url_to_group_map[url] = normalized_name\n",
    "\n",
    "    # Check families_data only for URLs NOT in actors_data\n",
    "    for family_name, family_info in families_data.items():\n",
    "        for url in family_info.get(\"urls\", []):\n",
    "            if url in url_to_group_map:\n",
    "                # Skip processing if already found in actors_data\n",
    "                continue  \n",
    "\n",
    "            attribution = family_info.get(\"attribution\", [])\n",
    "            mapped_names = [normalize_group_name(attr) for attr in attribution] if attribution else [family_name]\n",
    "\n",
    "            # Store as a list even if there's only one name for consistency\n",
    "            url_to_group_map[url] = mapped_names if len(mapped_names) > 1 else mapped_names[0]\n",
    "\n",
    "    return url_to_group_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "869c3f0c-0a8e-4413-a49b-1c05449ffdbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load the actors data\n",
    "actors_data_file = \"actors_data.json\"  # Update with the actual file path\n",
    "actors_data = load_actors_data(actors_data_file)\n",
    "families_data_file = \"malware_families.json\"\n",
    "families_data = load_families_data(families_data_file)\n",
    "# Create URL-to-group mapping\n",
    "url_to_group_map = create_url_to_group_map(actors_data, families_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b95e8f8d-b967-42f5-bdec-83590d62f911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11935\n"
     ]
    }
   ],
   "source": [
    "def count_single_mapping_urls(url_to_group_map):\n",
    "    \"\"\"\n",
    "    Counts the number of URLs that are mapped to only one group (threat group or family).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    url_to_group_map : dict\n",
    "        The final dictionary mapping URLs to group names (or lists of names).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The count of URLs that are mapped to exactly one group.\n",
    "    \"\"\"\n",
    "    single_mapping_count = 0\n",
    "\n",
    "    for groups in url_to_group_map.values():\n",
    "        if isinstance(groups, str) or (isinstance(groups, list) and len(groups) == 1):\n",
    "            single_mapping_count += 1\n",
    "\n",
    "    return single_mapping_count\n",
    "\n",
    "\n",
    "result = count_single_mapping_urls(url_to_group_map)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6b37d303-1530-4c7e-9592-c7a70efd7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_group_to_hash_url_map(hash_to_url_map, url_to_group_map):\n",
    "    \"\"\"\n",
    "    Updates the hash-to-group map with group names as keys and hashes/URLs as associated values.\n",
    "    If a URL is associated with multiple groups, it is added to the \"Unknown\" group.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hash_to_url_map : dict\n",
    "        A dictionary mapping file hashes to URLs.\n",
    "    url_to_group_map : dict\n",
    "        A dictionary mapping URLs to group names (can be a list of names).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where group names are keys, and associated values are lists of hashes and URLs.\n",
    "    \"\"\"\n",
    "    group_to_hash_url_map = {}\n",
    "\n",
    "    for file_hash, url in hash_to_url_map.items():\n",
    "        group_names = url_to_group_map.get(url, [\"Unknown\"])  # Default to \"Unknown\" if no group found\n",
    "        \n",
    "        # If the group is a list, choose the first group or default to \"Unknown\" if there are multiple groups\n",
    "        if isinstance(group_names, list):\n",
    "            if len(group_names) > 1:\n",
    "                group_name = \"Unknown\"  # Default to \"Unknown\" if multiple groups\n",
    "            else:\n",
    "                group_name = group_names[0]\n",
    "        else:\n",
    "            group_name = group_names\n",
    "        \n",
    "        # If group name not already in the map, initialize it\n",
    "        if group_name not in group_to_hash_url_map:\n",
    "            group_to_hash_url_map[group_name] = {\"hashes\": [], \"urls\": []}\n",
    "\n",
    "        # Add the hash and URL to the corresponding group\n",
    "        group_to_hash_url_map[group_name][\"hashes\"].append(file_hash)\n",
    "        group_to_hash_url_map[group_name][\"urls\"].append(url)\n",
    "\n",
    "    return group_to_hash_url_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a2e29a7e-e580-4553-8d19-7e4593c4794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "589\n"
     ]
    }
   ],
   "source": [
    "group_to_hash_url_map = update_group_to_hash_url_map(hash_to_url_map, url_to_group_map)\n",
    "print(len(group_to_hash_url_map))\n",
    "# first_key, first_value = next(iter(group_to_hash_url_map.items()))\n",
    "# print(first_key, first_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "355bab6a-1ef3-4d3e-af08-2ee15c0a0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique groups: 589\n",
      "Length of the hash-to-group map: 1637\n",
      "Number of unique URLs: 1617\n",
      "Number of hashes with 'Unknown' group: 503\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics\n",
    "def calculate_statistics(group_map):\n",
    "    \"\"\"\n",
    "    Calculate the statistics for the group-to-hash URL map.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    group_map : dict\n",
    "        A dictionary where group names are keys, and each value contains a list of hashes and URLs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple\n",
    "        A tuple containing:\n",
    "        - num_unique_groups: Number of unique group names (including 'Unknown')\n",
    "        - num_hashes: Total number of hashes across all groups\n",
    "        - num_unique_urls: Total number of unique URLs across all groups\n",
    "        - unknown_groups_count: Number of hashes associated with the 'Unknown' group\n",
    "    \"\"\"\n",
    "    # Number of unique groups (including 'Unknown')\n",
    "    num_unique_groups = len(group_map)\n",
    "    \n",
    "    # Number of unique URLs\n",
    "    unique_urls = set()\n",
    "    for data in group_map.values():\n",
    "        unique_urls.update(data['urls'])\n",
    "    num_unique_urls = len(unique_urls)\n",
    "    \n",
    "    # Length of the hash-to-group map (total hashes)\n",
    "    num_hashes = sum(len(data['hashes']) for data in group_map.values())\n",
    "    \n",
    "    # Number of hashes with 'Unknown' group\n",
    "    unknown_groups_count = len(group_map.get('Unknown', {}).get('hashes', []))\n",
    "    \n",
    "    return num_unique_groups, num_hashes, num_unique_urls, unknown_groups_count\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'hash_to_group_map' is the result of your update function\n",
    "num_unique_groups, num_hashes, num_unique_urls, unknown_groups_count = calculate_statistics(group_to_hash_url_map)\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Number of unique groups: {num_unique_groups}\")\n",
    "print(f\"Length of the hash-to-group map: {num_hashes}\")\n",
    "print(f\"Number of unique URLs: {num_unique_urls}\")\n",
    "print(f\"Number of hashes with 'Unknown' group: {unknown_groups_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff1ecd6d-2b40-4c3e-830e-d3b8b9c72dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hashes_and_indicators(directories, indicator_type=\"cves\"):\n",
    "    \"\"\"\n",
    "    Extracts file hashes from filenames and reads the content of the files to\n",
    "    extract either CVEs or TTPs associated with each hash, based on the indicator_type parameter.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directories : list\n",
    "        A list of directory paths containing the files.\n",
    "    indicator_type : str, optional\n",
    "        The type of indicators to extract. Accepts \"cve\" or \"ttp\". Default is \"cve\".\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where the key is the file hash, and the value is a list\n",
    "        of CVEs or TTPs associated with that hash.\n",
    "    \"\"\"\n",
    "    hash_to_indicators = {}\n",
    "    \n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            \n",
    "            if indicator_type == \"cves\" and filename.endswith(\".iocs\"):\n",
    "                file_hash = filename.split('.')[0]\n",
    "                indicators = []\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    for line in file:\n",
    "                        parts = line.strip().split(\"\\t\")\n",
    "                        if parts[0] == \"cve\" and len(parts) > 1:\n",
    "                            indicators.append(parts[1])\n",
    "                \n",
    "                if indicators:\n",
    "                    hash_to_indicators[file_hash] = indicators\n",
    "\n",
    "            elif indicator_type == \"ttps\" and filename.endswith(\".download.iocs\"):\n",
    "                # Remove the '.download.iocs' suffix from the filename\n",
    "                file_hash = filename.removesuffix(\".download.iocs\")\n",
    "                file_hashes.append(file_hash) # Remove \".download.iocs\" from filename\n",
    "                indicators = []\n",
    "                \n",
    "                with open(file_path, 'r', encoding='utf-8', errors='ignore') as file:\n",
    "                    for line in file:\n",
    "                        parts = line.strip().split(\"\\t\")\n",
    "                        if len(parts) > 1 and parts[0] == \"ttp\":\n",
    "                            indicators.append(parts[1])\n",
    "                \n",
    "                if indicators:\n",
    "                    hash_to_indicators[file_hash] = indicators\n",
    "    \n",
    "    return hash_to_indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4a2efa2-b7f2-4334-b5eb-a468480b3a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1637\n"
     ]
    }
   ],
   "source": [
    "# Get directories from the config ## use CVE_Malpedia if querying CVE\n",
    "directories = [\n",
    "    config[\"directory_paths\"][\"TTP_Malpedia_2022\"],\n",
    "    config[\"directory_paths\"][\"TTP_Malpedia_2024\"]\n",
    "]\n",
    "\n",
    "#cve_data = get_file_hashes_and_indicators(directories, \"cves\")\n",
    "ttp_data = get_file_hashes_and_indicators(directories, \"ttps\")\n",
    "\n",
    "\n",
    "print(len(ttp_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d85f3242-1f09-4383-9f4c-6d7375a92e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_group_hash_with_data(group_hash_to_url_map, data, data_type):\n",
    "    \"\"\"\n",
    "    Updates the group_hash_to_url_map by adding TTPs or CVEs from the respective data dictionary.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    group_hash_to_url_map : dict\n",
    "        A dictionary where each group contains hashes and URLs.\n",
    "    data : dict\n",
    "        A dictionary where the keys are hashes and the values are lists of TTPs or CVEs.\n",
    "    data_type : str\n",
    "        The type of data being processed ('cve' or 'ttp').\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        The updated group_hash_to_url_map with TTPs or CVEs added for each hash.\n",
    "    \"\"\"\n",
    "    if data_type not in ('cves', 'ttps'):\n",
    "        raise ValueError(\"Invalid data_type. Must be 'cve' or 'ttp'.\")\n",
    "    \n",
    "    for group_name, group_data in group_hash_to_url_map.items():\n",
    "        updated_hashes = []\n",
    "        \n",
    "        for hash_value in group_data['hashes']:\n",
    "            if hash_value in data:\n",
    "                updated_hashes.append({\n",
    "                    \"hash\": hash_value,\n",
    "                    data_type: data[hash_value]  # Dynamically set key as 'cve' or 'ttp'\n",
    "                })\n",
    "            else:\n",
    "                print(f\"Hash {hash_value} not found in {data_type}_data.\")\n",
    "                updated_hashes.append({\n",
    "                    \"hash\": hash_value,\n",
    "                    data_type: []\n",
    "                })\n",
    "        \n",
    "        group_hash_to_url_map[group_name]['hashes'] = updated_hashes\n",
    "    \n",
    "    return group_hash_to_url_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ecedcc13-d400-4320-b096-b9e3e8311b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ttp_data.get(\"00090904f5cf8855553fb323ee9a1d1fc089e75c948f560ed5b95eaa914a61de\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d8ea5a3d-f469-4898-b097-a10258ae6a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to update the map  --> May need to restart it if already ran once or else it flashes error\n",
    "#updated_group_hash_to_url_map = update_group_hash_with_ttps(group_to_hash_url_map, ttp_data)\n",
    "\n",
    "updated_data = update_group_hash_with_data(group_to_hash_url_map, ttp_data, \"ttps\")\n",
    "\n",
    "#updated_data = update_group_hash_with_data(group_to_hash_url_map, cve_data, \"cves\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "63740d77-dcae-4634-aa9f-f6a844df347a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "589"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(updated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b5f2fb46-42fb-4193-9581-d03ae9970505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of keys in the JSON file: 589\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def save_json(data, data_type):\n",
    "    \"\"\"\n",
    "    Saves the given data to a JSON file with a dynamic filename based on the data type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data : dict\n",
    "        The dictionary data to be saved.\n",
    "    data_type : str\n",
    "        The type of data being saved, either 'cve' or 'ttp'.\n",
    "    \"\"\"\n",
    "    with open(filename, 'w') as json_file:\n",
    "        json.dump(data, json_file, indent=4)\n",
    "    return filename  # Return filename for further use\n",
    "\n",
    "def count_json_keys(json_file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSON file and counts the number of keys in it.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    json_file_path : str\n",
    "        The path to the JSON file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The number of keys in the JSON file.\n",
    "    \"\"\"\n",
    "    with open(json_file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    \n",
    "    return len(data)\n",
    "\n",
    "# Example usage\n",
    "#data_type = \"cve\"\n",
    "#data_type = \"ttps\" # Change to \"cve\" when dealing with CVE data\n",
    "filename = f\"Malpedia_{data_type}_group_analysis.json\"\n",
    "#filename = save_json(updated_data, data_type)\n",
    "\n",
    "num_keys = count_json_keys(filename)\n",
    "print(f\"Number of keys in the JSON file: {num_keys}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "75b5f622-3954-4be7-b015-1c40043c152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_cve_group_info_by_group_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0efaaf15-f4c1-4708-a6c5-31476e67e5f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def analyze_data(group_data_map, data_type=\"cves\"):\n",
    "    \"\"\"\n",
    "    Analyzes the given group data (CVE or TTP) and provides statistical insights.\n",
    "\n",
    "    Args:\n",
    "        group_data_map (dict): Dictionary where keys are group IDs and values contain hashes and data lists.\n",
    "        data_type (str): Either \"cves\" or \"ttps\" to specify the type of data being analyzed.\n",
    "\n",
    "    Returns:\n",
    "        dict: Analysis results including total count, unique count, top 10 common items, and unique items per group.\n",
    "    \"\"\"\n",
    "    # Initialize a list to store all items (CVE/TTPs) across groups\n",
    "    all_items = []\n",
    "    \n",
    "    # Extract all CVEs or TTPs from the data\n",
    "    for group_id, group_data in group_data_map.items():\n",
    "        for hash_data in group_data['hashes']:\n",
    "            all_items.extend(hash_data[data_type])\n",
    "    \n",
    "    # Calculate total and unique counts\n",
    "    total_items = len(all_items)\n",
    "    unique_items = set(all_items)\n",
    "    total_unique_items = len(unique_items)\n",
    "    \n",
    "    # Identify the top 10 most common CVEs or TTPs\n",
    "    item_counter = Counter(all_items)\n",
    "    top_10_items = item_counter.most_common(10)\n",
    "    \n",
    "    # Identify unique items per group\n",
    "    unique_items_per_group = {}\n",
    "    for group_id, data in group_data_map.items():\n",
    "        group_items = set()\n",
    "        for hash_entry in data['hashes']:\n",
    "            group_items.update(hash_entry[data_type])\n",
    "\n",
    "        # Find items unique to this group\n",
    "        other_groups_items = set()\n",
    "        for other_group_id, other_data in group_data_map.items():\n",
    "            if other_group_id != group_id:\n",
    "                for other_hash_entry in other_data['hashes']:\n",
    "                    other_groups_items.update(other_hash_entry[data_type])\n",
    "\n",
    "        unique_items = group_items - other_groups_items\n",
    "        if unique_items:\n",
    "            unique_items_per_group[group_id] = list(unique_items)\n",
    "    \n",
    "    # Print analysis results\n",
    "    print(f\"Total number of {data_type.upper()}: {total_items}\")\n",
    "    print(f\"Total number of unique {data_type.upper()}: {total_unique_items}\")\n",
    "    print(f\"Top 10 most common {data_type.upper()}:\")\n",
    "    for item, count in top_10_items:\n",
    "        print(f\"{item}: {count}\")\n",
    "    print(f\"Number of groups with unique {data_type.upper()}: {len(unique_items_per_group)}\")\n",
    "    \n",
    "    # Return analysis results as a dictionary\n",
    "    return {\n",
    "        \"total_count\": total_items,\n",
    "        \"unique_count\": total_unique_items,\n",
    "        \"top_10\": top_10_items,\n",
    "        \"unique_per_group\": unique_items_per_group\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4e2420c-4811-4f2a-83fa-8cbbcdef124c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of TTPS: 24398\n",
      "Total number of unique TTPS: 876\n",
      "Top 10 most common TTPS:\n",
      "T1082: 530\n",
      "T1083: 454\n",
      "T1140: 429\n",
      "T1027: 418\n",
      "T1059: 367\n",
      "T1041: 333\n",
      "T1105: 326\n",
      "T1057: 325\n",
      "T1486: 320\n",
      "T1071.001: 303\n",
      "Number of groups with unique TTPS: 83\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "#cve_analysis = analyze_data(updated_data, data_type=\"cves\")\n",
    "ttp_analysis = analyze_data(updated_data, data_type=\"ttps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74f1df22-02e3-4f30-a820-68593dccbae1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
