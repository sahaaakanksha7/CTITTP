{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "37145c57-eedb-4023-955d-b5804baaaf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Load the config file (if you have a config.json file)\n",
    "with open('config.json', 'r') as config_file:\n",
    "    config = json.load(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89acccec-3cec-4af5-9fd3-015f6f07bf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of entries: 15765\n",
      "First entry from the combined data:\n",
      "{'author': 'CERT Division', 'date': '2000', 'download_mime': 'text/html', 'download_redirects': ['https://resources.sei.cmu.edu/library/asset-view.cfm?assetID=496186'], 'download_sha256': 'f2c405b383ebaf4d0793f8d5162841b953d06947a711f7d34242faa20e285a04', 'download_size': 41745, 'download_status': 200, 'download_ts': '2022-07-19 12:43:41.400938+00:00', 'language': 'English', 'organization': 'Carnegie Mellon University', 'origin': ['malpedia:CarnegieMellonUniversity'], 'title': '2000 CERT Advisories', 'url': 'https://resources.sei.cmu.edu/library/asset-view.cfm?assetID=496186'}\n"
     ]
    }
   ],
   "source": [
    "# Function to read JSONL file\n",
    "def read_jsonl(file_path):\n",
    "    \"\"\"\n",
    "    Reads a JSONL file and returns a list of JSON objects.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the JSONL file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of JSON objects.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    \n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line))\n",
    "    \n",
    "    return data\n",
    "\n",
    "# File paths for the JSONL files\n",
    "malpedia_2022_file = config['jsonl_files']['Malpedia_2022']\n",
    "malpedia_2024_file = config['jsonl_files']['Malpedia_2024']\n",
    "\n",
    "# Ensure the file paths are relative to the running script's directory\n",
    "malpedia_2022_path = os.path.join(os.getcwd(), malpedia_2022_file)\n",
    "malpedia_2024_path = os.path.join(os.getcwd(), malpedia_2024_file)\n",
    "\n",
    "# Read the files\n",
    "malpedia_2022_data = read_jsonl(malpedia_2022_path)\n",
    "malpedia_2024_data = read_jsonl(malpedia_2024_path)\n",
    "\n",
    "# Concatenate the data from both JSONL files\n",
    "combined_data = malpedia_2022_data + malpedia_2024_data\n",
    "\n",
    "# Example: print the length of the combined data and the first entry\n",
    "print(f\"Total number of entries: {len(combined_data)}\")\n",
    "print(\"First entry from the combined data:\")\n",
    "print(combined_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2d91794-3051-4577-981e-a7a6cb6b9a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract file hashes from filenames\n",
    "def get_file_hashes(directory):\n",
    "    \"\"\"\n",
    "    Extracts file hashes from filenames in the given directory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str\n",
    "        The path to the directory containing the files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of file hashes extracted from the filenames.\n",
    "    \"\"\"\n",
    "    file_hashes = []\n",
    "    for directory in directories:\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".iocs\"):\n",
    "                # Extract the hash from the filename (everything before '.iocs')\n",
    "                file_hash = filename.split('.')[0]\n",
    "                file_hashes.append(file_hash)\n",
    "    return file_hashes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc687316-3c0d-43cf-b5ae-d2ce21f3097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to map file hash to url\n",
    "def create_hash_to_url_map(file_hashes, combined_data):\n",
    "    \"\"\"\n",
    "    Maps file hashes to URLs based on the download_sha256 field in the combined_data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_hashes : list\n",
    "        A list of file hashes.\n",
    "    combined_data : list\n",
    "        A list of dictionaries (combined JSONL data) with download_sha256 and url fields.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary mapping file hashes to URLs.\n",
    "    \"\"\"\n",
    "    hash_to_url = {}\n",
    "    for file_hash in file_hashes:\n",
    "        for entry in combined_data:\n",
    "            if entry.get(\"download_sha256\") == file_hash:\n",
    "                hash_to_url[file_hash] = entry.get(\"url\")\n",
    "                break  # Stop searching once we find the match for the hash\n",
    "    return hash_to_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b022e619-2036-455a-8950-1e896dd95cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000c1ee0c1bab222569623c47397f634b77c4124ad8bb0b0d2533ec98fcc6a16 https://attack.mitre.org/groups/G0046/\n"
     ]
    }
   ],
   "source": [
    "# Get directories from the config\n",
    "directories = [\n",
    "    config[\"directory_paths\"][\"CVE_Malpedia_2022\"],\n",
    "    config[\"directory_paths\"][\"CVE_Malpedia_2024\"]\n",
    "]\n",
    "\n",
    "# Extract file hashes from the directory\n",
    "file_hashes = get_file_hashes(directories)\n",
    "\n",
    "# Create the mapping of file hashes to URLs\n",
    "hash_to_url_map = create_hash_to_url_map(file_hashes, combined_data)\n",
    "\n",
    "first_key, first_value = next(iter(hash_to_url_map.items()))\n",
    "print(first_key, first_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b07c37b6-cd5f-457a-9c92-e957998a4c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique file hashes: 2832\n",
      "Total URLs: 2832\n",
      "Total unique URLs: 2811\n",
      "There are 21 duplicate URLs.\n",
      "Duplicate URLs and their occurrences:\n",
      "URL: https://www.fortinet.com/blog/threat-research/new-strrat-rat-phishing-campaign -> Occurrences: 2\n",
      "URL: https://www.systemtek.co.uk/2018/07/luoxk-malware-exploiting-cve-2018-2893/ -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-264a -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-249a -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/news-events/alerts/2023/07/28/cisa-releases-malware-analysis-reports-barracuda-backdoors -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/news-events/analysis-reports/ar23-250a-0 -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-277a -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-347a -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-279a -> Occurrences: 2\n",
      "URL: https://www.trellix.com/about/newsroom/stories/research/scattered-spider-the-modus-operandi/ -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/news-events/analysis-reports/ar23-209b -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-131a -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-038a -> Occurrences: 2\n",
      "URL: https://ics-cert.kaspersky.com/publications/reports/2024/04/02/apt-and-financial-attacks-on-industrial-organizations-in-h2-2023/ -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/uscert/ncas/alerts/aa22-257a -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-144a -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-187a -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/news-events/analysis-reports/ar23-209a -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa24-290a -> Occurrences: 2\n",
      "URL: https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-040a -> Occurrences: 2\n",
      "URL: https://www.trellix.com/en-us/about/newsroom/stories/research/cyberattacks-targeting-ukraine-increase.html -> Occurrences: 2\n"
     ]
    }
   ],
   "source": [
    "##Common file hashes found: {'bd90e5d64d43cd326049d739d519c270d9f2856db6c1d140569f152b0fa3b757', \n",
    "##'acd626acf50af8e30a681ccf88662b2bcecd5ec6053c18d6b460a42d9d726764', \n",
    "##'a71555ff127721ad3f47e0427411dde35ec792889c2778ba43571d3a4b3f5cca'}\n",
    "unique_hashes = len(hash_to_url_map)\n",
    "print(f\"Total unique file hashes: {unique_hashes}\")\n",
    "\n",
    "# Check if the URLs are unique\n",
    "urls = list(hash_to_url_map.values())\n",
    "unique_urls = len(set(urls))\n",
    "\n",
    "print(f\"Total URLs: {len(urls)}\")\n",
    "print(f\"Total unique URLs: {unique_urls}\")\n",
    "\n",
    "# Check if there are any duplicate URLs\n",
    "if len(urls) == unique_urls:\n",
    "    print(\"All URLs are unique.\")\n",
    "else:\n",
    "    print(f\"There are {len(urls) - unique_urls} duplicate URLs.\")\n",
    "\n",
    "    # Find and print duplicate URLs\n",
    "    url_counts = {}\n",
    "    \n",
    "    # Count the occurrences of each URL\n",
    "    for url in urls:\n",
    "        if url in url_counts:\n",
    "            url_counts[url] += 1\n",
    "        else:\n",
    "            url_counts[url] = 1\n",
    "\n",
    "    # Filter out the duplicate URLs (those that appear more than once)\n",
    "    duplicate_urls = {url: count for url, count in url_counts.items() if count > 1}\n",
    "\n",
    "    print(\"Duplicate URLs and their occurrences:\")\n",
    "    for url, count in duplicate_urls.items():\n",
    "        print(f\"URL: {url} -> Occurrences: {count}\")\n",
    "\n",
    "    # # Find the hashes associated with the duplicate URLs\n",
    "    # print(\"\\nHashes associated with duplicate URLs:\")\n",
    "    # for hash_key, url in hash_to_url_map.items():\n",
    "    #     if url in duplicate_urls:\n",
    "    #         print(f\"Hash: {hash_key} -> URL: {url}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "292cf82f-9d72-4a40-b38d-b42b59954669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load the actors data from the actors_data.json file and malware families data from malware_families.json\n",
    "def load_actors_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the actors data from a JSON file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the actors_data.json file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the actors data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        actors_data = json.load(f)\n",
    "    return actors_data\n",
    "\n",
    "\n",
    "def load_families_data(file_path):\n",
    "    \"\"\"\n",
    "    Loads the malware families data from a JSON file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path to the actors_data.json file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the families data.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        families_data = json.load(f)\n",
    "    return families_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3a665e81-2396-4e48-927a-5790d6dc7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a URL-to-group mapping\n",
    "def create_url_to_group_map(actors_data, families_data):\n",
    "    \"\"\"\n",
    "    Creates a mapping from URLs to group names based on the actors data.\n",
    "    If a URL is not found in the actors data, it checks in the families data.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    actors_data : dict\n",
    "        A dictionary containing the actors data.\n",
    "    families_data : dict\n",
    "        A dictionary containing the families data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary mapping URLs to group names.\n",
    "    \"\"\"\n",
    "    url_to_group_map = {}\n",
    "\n",
    "    # First, check the actors_data for URLs\n",
    "    for group_name, group_info in actors_data.items():\n",
    "        refs = group_info.get(\"meta\", {}).get(\"refs\", [])\n",
    "        for url in refs:\n",
    "            url_to_group_map[url] = group_name\n",
    "\n",
    "    # If a URL is not found in actors_data, check in families_data\n",
    "    for family_name, family_info in families_data.items():\n",
    "        urls = family_info.get(\"urls\", [])\n",
    "        for url in urls:\n",
    "            # Only add the URL if it isn't already in the map\n",
    "            if url not in url_to_group_map:\n",
    "                url_to_group_map[url] = family_name\n",
    "\n",
    "    return url_to_group_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b37d303-1530-4c7e-9592-c7a70efd7c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to update the hash-to-group map with hashes and URLs grouped by group name\n",
    "def update_group_to_hash_url_map(hash_to_url_map, url_to_group_map):\n",
    "    \"\"\"\n",
    "    Updates the hash-to-group map with group names as keys and hashes/URLs as associated values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hash_to_url_map : dict\n",
    "        A dictionary mapping file hashes to URLs.\n",
    "    url_to_group_map : dict\n",
    "        A dictionary mapping URLs to group names.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where group names are keys, and associated values are lists of hashes and URLs.\n",
    "    \"\"\"\n",
    "    group_to_hash_url_map = {}\n",
    "\n",
    "    for file_hash, url in hash_to_url_map.items():\n",
    "        group_name = url_to_group_map.get(url, \"Unknown\")  # If no group found, mark as 'Unknown'\n",
    "        \n",
    "        if group_name not in group_to_hash_url_map:\n",
    "            group_to_hash_url_map[group_name] = {\"hashes\": [], \"urls\": []}\n",
    "\n",
    "        group_to_hash_url_map[group_name][\"hashes\"].append(file_hash)\n",
    "        group_to_hash_url_map[group_name][\"urls\"].append(url)\n",
    "\n",
    "    return group_to_hash_url_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "51677e28-4e84-4de2-a9c2-8e694f525b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Load the actors data\n",
    "actors_data_file = \"actors_data.json\"  # Update with the actual file path\n",
    "actors_data = load_actors_data(actors_data_file)\n",
    "families_data_file = \"malware_families.json\"\n",
    "families_data = load_families_data(families_data_file)\n",
    "# Create URL-to-group mapping\n",
    "url_to_group_map = create_url_to_group_map(actors_data, families_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1de9362e-5ff3-4cc8-b157-f7a75084aa14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14301"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(url_to_group_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2e29a7e-e580-4553-8d19-7e4593c4794f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIN7 {'hashes': ['000c1ee0c1bab222569623c47397f634b77c4124ad8bb0b0d2533ec98fcc6a16', '33a41a05c070a7a339f8f76bb20cc7f79aa9f19359fe4298c162adea20ccde1d', '791661450f54c3e286e3140373d3df095d7b27bf052a4f1d4d8f37472f4dbb94', '7d2d812d40a62f3288883afcb14ee275716af1871f68587e12ed2485cddf9b9d', 'ebdada9350107eba5023f3fa116379b04ff1302d8ffdb62f70d1c4d6596771b0'], 'urls': ['https://attack.mitre.org/groups/G0046/', 'https://www.group-ib.com/resources/threat-research/Anunak_APT_against_financial_institutions.pdf', 'https://www.crowdstrike.com/blog/arrests-put-new-focus-on-carbon-spider-adversary-group/', 'https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2018/03/08064518/Carbanak_APT_eng.pdf', 'https://securelist.com/fin7-5-the-infamous-cybercrime-rig-fin7-continues-its-activities/90703/']}\n"
     ]
    }
   ],
   "source": [
    "# Assuming `hash_to_url_map` is already created (from previous steps)\n",
    "# Update the hash-to-URL map with group names\n",
    "hash_to_group_map = update_group_to_hash_url_map(hash_to_url_map, url_to_group_map)\n",
    "\n",
    "first_key, first_value = next(iter(hash_to_group_map.items()))\n",
    "print(first_key, first_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "99153e2a-4483-466a-8156-448705f6e264",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1095"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hash_to_group_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "355bab6a-1ef3-4d3e-af08-2ee15c0a0a28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique groups: 1095\n",
      "Length of the hash-to-group map: 2832\n",
      "Number of unique URLs: 2811\n",
      "Number of hashes with 'Unknown' group: 484\n"
     ]
    }
   ],
   "source": [
    "# Calculate statistics\n",
    "def calculate_statistics(group_map):\n",
    "    # Number of unique groups\n",
    "    num_unique_groups = len(group_map)\n",
    "    \n",
    "    # Number of unique URLs\n",
    "    unique_urls = set()\n",
    "    for data in group_map.values():\n",
    "        unique_urls.update(data['urls'])\n",
    "    num_unique_urls = len(unique_urls)\n",
    "    \n",
    "    # Length of the hash-to-group map (total hashes)\n",
    "    num_hashes = sum(len(data['hashes']) for data in group_map.values())\n",
    "    \n",
    "    # Number of hashes with 'Unknown' group\n",
    "    unknown_groups_count = len(group_map.get('Unknown', {}).get('hashes', []))\n",
    "    \n",
    "    return num_unique_groups, num_hashes, num_unique_urls, unknown_groups_count\n",
    "\n",
    "# Get the statistics\n",
    "num_unique_groups, num_hashes, num_unique_urls, unknown_groups_count = calculate_statistics(hash_to_group_map)\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Number of unique groups: {num_unique_groups}\")\n",
    "print(f\"Length of the hash-to-group map: {num_hashes}\")\n",
    "print(f\"Number of unique URLs: {num_unique_urls}\")\n",
    "print(f\"Number of hashes with 'Unknown' group: {unknown_groups_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff1ecd6d-2b40-4c3e-830e-d3b8b9c72dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_hashes_and_cves(directory):\n",
    "    \"\"\"\n",
    "    Extracts file hashes from filenames and reads the content of the files to\n",
    "    extract CVEs associated with each hash.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    directory : str\n",
    "        The path to the directory containing the files.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where the key is the file hash, and the value is a list\n",
    "        of CVEs associated with that hash.\n",
    "    \"\"\"\n",
    "    hash_to_cves = {}\n",
    "    for directory in directories:\n",
    "        # Iterate over the files in the directory\n",
    "        for filename in os.listdir(directory):\n",
    "            if filename.endswith(\".iocs\"):\n",
    "                # Extract the hash from the filename (everything before '.iocs')\n",
    "                file_hash = filename.split('.')[0]\n",
    "    \n",
    "                # Open and read the content of the file\n",
    "                cves = []\n",
    "                file_path = os.path.join(directory, filename)\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    for line in file:\n",
    "                        # Assuming each line starts with 'cve' followed by the CVE identifier\n",
    "                        parts = line.strip().split(\"\\t\")\n",
    "                        if parts[0] == \"cve\" and len(parts) > 1:\n",
    "                            cves.append(parts[1])\n",
    "    \n",
    "                # Store the hash and its associated CVEs in the dictionary\n",
    "                hash_to_cves[file_hash] = cves\n",
    "\n",
    "    return hash_to_cves\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4a2efa2-b7f2-4334-b5eb-a468480b3a4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2832\n",
      "000c1ee0c1bab222569623c47397f634b77c4124ad8bb0b0d2533ec98fcc6a16 ['CVE-2020-1472']\n"
     ]
    }
   ],
   "source": [
    "###Read directory path from config\n",
    "directories = [\n",
    "    config[\"directory_paths\"][\"CVE_Malpedia_2022\"],\n",
    "    config[\"directory_paths\"][\"CVE_Malpedia_2024\"]\n",
    "]\n",
    "\n",
    "hash_to_cves = get_file_hashes_and_cves(directories)\n",
    "\n",
    "print(len(hash_to_cves))\n",
    "\n",
    "first_key, first_value = next(iter(hash_to_cves.items()))\n",
    "print(first_key, first_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cd817c4-2573-48f9-857a-f23e9fdb37b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_hash_data(hash_to_cves, hash_to_group_map):\n",
    "    \"\"\"\n",
    "    Combines the CVE and group information for each hash.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    hash_to_cves : dict\n",
    "        A dictionary where the key is the file hash and the value is a list of CVEs associated with that hash.\n",
    "    hash_to_group_map : dict\n",
    "        A dictionary where the key is the file hash and the value is a dictionary containing the URL and group name for that hash.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A combined dictionary where the key is the file hash, and the value is a dictionary containing\n",
    "        both the CVEs and group information for that hash.\n",
    "    \"\"\"\n",
    "    combined_data = {}\n",
    "\n",
    "    # Iterate over the hashes in hash_to_cves\n",
    "    for file_hash, cves in hash_to_cves.items():\n",
    "        # Check if the hash exists in hash_to_group_map\n",
    "        if file_hash in hash_to_group_map:\n",
    "            # If both data exist, combine them into a new dictionary\n",
    "            combined_data[file_hash] = {\n",
    "                'cves': cves,\n",
    "                'group_info': hash_to_group_map[file_hash]\n",
    "            }\n",
    "        else:\n",
    "            # If no group info is found, use None or a default value for group_info\n",
    "            combined_data[file_hash] = {\n",
    "                'cves': cves,\n",
    "                'group_info': None\n",
    "            }\n",
    "\n",
    "    # If you want to include hashes that exist in hash_to_group_map but not in hash_to_cves, \n",
    "    # you can add that logic here too.\n",
    "    for file_hash, group_info in hash_to_group_map.items():\n",
    "        if file_hash not in combined_data:\n",
    "            # If no CVEs are found for this hash, use None or an empty list for CVEs\n",
    "            combined_data[file_hash] = {\n",
    "                'cves': None,\n",
    "                'group_info': group_info\n",
    "            }\n",
    "\n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cc59959d-eb08-4eec-b248-20021335823c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "000c1ee0c1bab222569623c47397f634b77c4124ad8bb0b0d2533ec98fcc6a16 {'cves': ['CVE-2020-1472'], 'group_info': None}\n"
     ]
    }
   ],
   "source": [
    "# Assuming you have hash_to_cves and hash_to_group_map already populated\n",
    "\n",
    "combined_data = combine_hash_data(hash_to_cves, hash_to_group_map)\n",
    "\n",
    "first_key, first_value = next(iter(combined_data.items()))\n",
    "print(first_key, first_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d71d8406-0881-4c80-b0a5-632dbd5db7be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to combine CVE and group information for each hash with Group ID as the main key\n",
    "def combine_cve_group_info_by_group_id(hash_to_group_map, hash_to_cves):\n",
    "    combined_info_by_group = {}\n",
    "\n",
    "    for group, group_data in hash_to_group_map.items():\n",
    "        combined_info_by_group[group] = []\n",
    "        for i, hash_value in enumerate(group_data['hashes']):\n",
    "            cves = hash_to_cves.get(hash_value, [])  # Get CVEs for the current hash\n",
    "            url = group_data['urls'][i] if i < len(group_data['urls']) else None  # Get corresponding URL\n",
    "\n",
    "            # Append the group info with hashes, CVEs, and URLs to the group entry\n",
    "            combined_info_by_group[group].append({\n",
    "                'hash': hash_value,\n",
    "                'cves': cves,\n",
    "                'url': url\n",
    "            })\n",
    "\n",
    "\n",
    "    return combined_info_by_group\n",
    "\n",
    "# Combine the CVE and group information with Group ID as the main key\n",
    "combined_cve_group_info_by_group_id = combine_cve_group_info_by_group_id(hash_to_group_map, hash_to_cves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71390ed6-7a8a-45d8-acbf-40bbd89f25ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIN7 [{'hash': '000c1ee0c1bab222569623c47397f634b77c4124ad8bb0b0d2533ec98fcc6a16', 'cves': ['CVE-2020-1472'], 'url': 'https://attack.mitre.org/groups/G0046/'}, {'hash': '33a41a05c070a7a339f8f76bb20cc7f79aa9f19359fe4298c162adea20ccde1d', 'cves': ['CVE-2012-0158', 'CVE-2012-2539'], 'url': 'https://www.group-ib.com/resources/threat-research/Anunak_APT_against_financial_institutions.pdf'}, {'hash': '791661450f54c3e286e3140373d3df095d7b27bf052a4f1d4d8f37472f4dbb94', 'cves': ['CVE-2014-4114', 'CVE-2015-1641', 'CVE-2015-1770', 'CVE-2015-2545'], 'url': 'https://www.crowdstrike.com/blog/arrests-put-new-focus-on-carbon-spider-adversary-group/'}, {'hash': '7d2d812d40a62f3288883afcb14ee275716af1871f68587e12ed2485cddf9b9d', 'cves': ['CVE-2012-0158', 'CVE-2013-3660', 'CVE-2013-3906'], 'url': 'https://media.kasperskycontenthub.com/wp-content/uploads/sites/43/2018/03/08064518/Carbanak_APT_eng.pdf'}, {'hash': 'ebdada9350107eba5023f3fa116379b04ff1302d8ffdb62f70d1c4d6596771b0', 'cves': ['CVE-2017-11882'], 'url': 'https://securelist.com/fin7-5-the-infamous-cybercrime-rig-fin7-continues-its-activities/90703/'}]\n"
     ]
    }
   ],
   "source": [
    "# Assuming combined_data is already populated\n",
    "#group_to_data = group_data_by_group_id(combined_data)\n",
    "\n",
    "\n",
    "first_key, first_value = next(iter(combined_cve_group_info_by_group_id.items()))\n",
    "print(first_key, first_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0e8370f6-f7a9-4567-8351-6e9d79a97444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of groups with unique CVEs (excluding 'Unknown'): 239\n",
      "Unique CVEs for up to 10 groups:\n",
      "  Group: HAFNIUM\n",
      "    Unique CVEs: {'CVE-2020-147212', 'CVE-2021-337716', 'CVE-2022-27511'}\n",
      "  Group: win.8t_dropper\n",
      "    Unique CVEs: {'CVE-2018-8570'}\n",
      "  Group: win.crat\n",
      "    Unique CVEs: {'CVE-2018-9375'}\n",
      "  Group: win.cobalt_strike\n",
      "    Unique CVEs: {'CVE-2019-0567', 'CVE-2020-116511', 'CVE-2021-36798', 'CVE-2009-3960', 'CVE-2021-26868', 'CVE-2017-11318', 'CVE-2021-1844'}\n",
      "  Group: win.clop\n",
      "    Unique CVEs: {'CVE-2020-12061', 'CVE-2023-35036'}\n",
      "  Group: elf.mirai\n",
      "    Unique CVEs: {'CVE-2017-16725', 'CVE-2020-29957', 'CVE-2021-1498', 'CVE-2020-7115', 'CVE-2018-15716', 'CVE-2020-29557', 'CVE-2021-22991', 'CVE-2018-19276', 'CVE-2020-1937', 'CVE-2020-1956', 'CVE-2016-5674', 'CVE-2017-18377', 'CVE-2021-38649', 'CVE-2021-25502', 'CVE-2020-1017', 'CVE-2021-38645', 'CVE-2019-16057', 'CVE-2019-7276', 'CVE-2013-2251', 'CVE-2009-4490', 'CVE-2021-38648'}\n",
      "  Group: win.shadowpad\n",
      "    Unique CVEs: {'CVE-2018-8872', 'CVE-2020-15782'}\n",
      "  Group: win.babuk\n",
      "    Unique CVEs: {'CVE-2021-21986', 'CVE-2022-0337'}\n",
      "  Group: win.supernova\n",
      "    Unique CVEs: {'CVE-2019-6340', 'CVE-2019-8917', 'CVE-2020-14005'}\n",
      "  Group: elf.bpfdoor\n",
      "    Unique CVEs: {'CVE-2019-3010', 'CVE-2020-8835'}\n",
      "\n",
      "Top 10 most common CVEs across groups:\n",
      "  CVE-2017-11882: 263 occurrences\n",
      "  CVE-2021-44228: 183 occurrences\n",
      "  CVE-2017-0199: 155 occurrences\n",
      "  CVE-2022-30190: 152 occurrences\n",
      "  CVE-2012-0158: 135 occurrences\n",
      "  CVE-2021-26855: 124 occurrences\n",
      "  CVE-2021-27065: 103 occurrences\n",
      "  CVE-2020-1472: 85 occurrences\n",
      "  CVE-2019-19781: 84 occurrences\n",
      "  CVE-2022-26134: 81 occurrences\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def compute_cve_statistics(group_to_data):\n",
    "    \"\"\"\n",
    "    Compute statistics about the CVEs, groups, and other relevant data.\n",
    "    Identify CVEs that are unique to a group (excluding 'Unknown') and \n",
    "    count the top 10 most occurring CVEs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    group_to_data : dict\n",
    "        A dictionary where the key is the group name, and the value contains hashes, CVEs, and URLs for that group.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary containing the computed statistics, including groups with unique CVEs and top 10 common CVEs.\n",
    "    \"\"\"\n",
    "    cve_to_groups = {}  # Map each CVE to a set of groups\n",
    "    group_unique_cves = {}  # Store the unique CVEs for each group\n",
    "    cve_counter = Counter()  # Count how many times each CVE occurs\n",
    "\n",
    "    # Iterate over each group and their data\n",
    "    for group, data_list in group_to_data.items():\n",
    "        group_cves = set()  # Track the CVEs for this group\n",
    "        for entry in data_list:\n",
    "            cves = entry.get('cves', [])\n",
    "            for cve in cves:\n",
    "                group_cves.add(cve)\n",
    "                if cve not in cve_to_groups:\n",
    "                    cve_to_groups[cve] = set()\n",
    "                cve_to_groups[cve].add(group)\n",
    "                cve_counter[cve] += 1  # Count the occurrence of this CVE\n",
    "        \n",
    "        # Store all CVEs for this group temporarily\n",
    "        group_unique_cves[group] = group_cves\n",
    "\n",
    "    # Now find which CVEs are unique to a group, excluding 'Unknown'\n",
    "    final_unique_cves_per_group = {}\n",
    "    for group, cves in group_unique_cves.items():\n",
    "        if group.lower() == 'unknown':\n",
    "            continue  # Skip the 'Unknown' group\n",
    "        unique_cves = {cve for cve in cves if len(cve_to_groups[cve]) == 1}\n",
    "        if unique_cves:\n",
    "            final_unique_cves_per_group[group] = unique_cves\n",
    "\n",
    "    # Limit the list to only 10 groups for analysis\n",
    "    limited_unique_cves_per_group = dict(list(final_unique_cves_per_group.items())[:10])\n",
    "\n",
    "    # Compute the number of groups with unique CVEs\n",
    "    num_groups_with_unique_cves = len(final_unique_cves_per_group)\n",
    "\n",
    "    # Find the top 10 most common CVEs\n",
    "    top_10_common_cves = cve_counter.most_common(10)\n",
    "\n",
    "    # Gather statistics\n",
    "    statistics = {\n",
    "        \"num_groups_with_unique_cves\": num_groups_with_unique_cves,\n",
    "        \"unique_cves_per_group\": limited_unique_cves_per_group,\n",
    "        \"top_10_common_cves\": top_10_common_cves\n",
    "    }\n",
    "\n",
    "    return statistics\n",
    "\n",
    "\n",
    "# Example usage with the combined data\n",
    "statistics = compute_cve_statistics(combined_cve_group_info_by_group_id)\n",
    "\n",
    "# Print the statistics\n",
    "print(f\"Number of groups with unique CVEs (excluding 'Unknown'): {statistics['num_groups_with_unique_cves']}\")\n",
    "print(\"Unique CVEs for up to 10 groups:\")\n",
    "for group, cves in statistics['unique_cves_per_group'].items():\n",
    "    print(f\"  Group: {group}\")\n",
    "    print(f\"    Unique CVEs: {cves}\")\n",
    "\n",
    "print(\"\\nTop 10 most common CVEs across groups:\")\n",
    "for cve, count in statistics['top_10_common_cves']:\n",
    "    print(f\"  {cve}: {count} occurrences\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ba3b15-7ce5-466b-901a-0c4db25dcba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "Total number of CVEs: 11336\n",
    "Total unique CVEs: 2019\n",
    "Unique groups (excluding 'Unknown'): 258\n",
    "Most common CVEs: [('CVE-2017-11882', 263), ('CVE-2021-44228', 183), ('CVE-2017-0199', 155), ('CVE-2022-30190', 152), ('CVE-2012-0158', 135),\n",
    "                   ('CVE-2021-26855', 124), ('CVE-2021-27065', 103), ('CVE-2020-1472', 85), ('CVE-2019-19781', 84), ('CVE-2022-26134', 81)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
