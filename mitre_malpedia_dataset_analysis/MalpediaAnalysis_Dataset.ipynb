{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2b63b9-6de5-4c2f-8aef-95b11741f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import difflib\n",
    "import re\n",
    "\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7d872a4-af30-4b4e-892e-eaa20ddf6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Locate and load config.json from one level up\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "config_path = os.path.normpath(os.path.join(parent_dir, 'config.json'))\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Step 2: Build absolute path prefix based on where config.json was loaded from\n",
    "config_dir = os.path.dirname(config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ee24807-6242-4cdb-8479-5f001d129055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('C:\\\\Users\\\\ricewater\\\\Documents\\\\CTITTP\\\\config.json',\n",
       " 'C:\\\\Users\\\\ricewater\\\\Documents\\\\CTITTP')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config_path, config_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ca2df42-170a-4677-9362-737edbd74bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_jsonl_file(file_path):\n",
    "    \"\"\"\n",
    "    Reads a specified JSONL file and yields each line as a JSON object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path of the JSONL file to read.\n",
    "\n",
    "    Yields\n",
    "    ------\n",
    "    dict\n",
    "        A dictionary representation of each line in the JSONL file.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            yield json.loads(line)\n",
    "\n",
    "def merge_jsonl_data(file_path1, file_path2):\n",
    "    \"\"\"\n",
    "    Merges JSONL data from two files into a single list of dictionaries.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path1 : str\n",
    "        The path to the first JSONL file.\n",
    "    file_path2 : str\n",
    "        The path to the second JSONL file.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of dictionaries containing merged data from both files.\n",
    "    \"\"\"\n",
    "    combined_data = []\n",
    "    \n",
    "    # Read and merge data from both files\n",
    "    for data in read_jsonl_file(file_path1):\n",
    "        combined_data.append(data)\n",
    "    \n",
    "    for data in read_jsonl_file(file_path2):\n",
    "        combined_data.append(data)\n",
    "    \n",
    "    return combined_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e85d6a61-cb7f-47ae-9658-74bf5cc01d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build fully resolved and normalized paths\n",
    "file_path1 = os.path.normpath(os.path.join(config_dir, config['jsonl_files']['Malpedia_2022']))\n",
    "file_path2 = os.path.normpath(os.path.join(config_dir, config['jsonl_files']['Malpedia_2024']))\n",
    "\n",
    "# Use the paths\n",
    "merged_data = merge_jsonl_data(file_path1, file_path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d80cb7d9-18fe-4331-a9c8-14696e2497f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total merged entries: 15768\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total merged entries: {len(merged_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae80fd4e-f761-4dfd-9d6c-e88a78eeb92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_jsonl_entries(file_path):\n",
    "    \"\"\"\n",
    "    Counts the number of entries in a JSONL file.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path : str\n",
    "        The path of the JSONL file to count entries.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The count of entries in the JSONL file.\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for _ in file:\n",
    "            count += 1\n",
    "    return count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425422a7-2e9f-4794-a442-db80cff097b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1 has 12050 entries.\n",
      "File 2 has 3718 entries.\n"
     ]
    }
   ],
   "source": [
    "# Show the count of entries in each file\n",
    "count1 = count_jsonl_entries(file_path1)\n",
    "count2 = count_jsonl_entries(file_path2)\n",
    "\n",
    "print(f\"File 1 has {count1} entries.\")\n",
    "print(f\"File 2 has {count2} entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3495f351-bb2c-4bf9-be9d-a7b961741d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries with duplicate URLs: 67\n"
     ]
    }
   ],
   "source": [
    "def find_duplicate_urls(merged_data):\n",
    "    \"\"\"\n",
    "    Identifies entries with duplicate URLs and returns their associated download_sha256 values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    merged_data : list of dict\n",
    "        The merged JSONL data.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where each key is a URL and the value is a list of download_sha256\n",
    "        values for entries with the same URL.\n",
    "    \"\"\"\n",
    "    # Dictionary to store URLs as keys and lists of download_sha256 as values\n",
    "    url_to_sha256 = defaultdict(list)\n",
    "    \n",
    "    # Iterate through all entries in the merged data\n",
    "    for entry in merged_data:\n",
    "        url = entry.get('url')  # Safely get the 'url', if it exists\n",
    "        download_sha256 = entry.get('download_sha256')  # Safely get the 'download_sha256'\n",
    "        \n",
    "        # Only proceed if both url and download_sha256 are present\n",
    "        if url and download_sha256:\n",
    "            # Add the download_sha256 to the list corresponding to the URL\n",
    "            url_to_sha256[url].append(download_sha256)\n",
    "    \n",
    "    # Filter to keep only URLs with more than one associated download_sha256 (duplicate URLs)\n",
    "    duplicate_urls = {url: sha256s for url, sha256s in url_to_sha256.items() if len(sha256s) > 1}\n",
    "    \n",
    "    return duplicate_urls\n",
    "\n",
    "\n",
    "# Find and print duplicate URLs and their download_sha256 values\n",
    "duplicate_urls = find_duplicate_urls(merged_data)\n",
    "#print(duplicate_urls)\n",
    "\n",
    "\n",
    "if duplicate_urls:\n",
    "    print(f\"Total entries with duplicate URLs: {len(duplicate_urls)}\")\n",
    "    #print(\"Entries with duplicate URLs:\")\n",
    "    \n",
    "    #for url, sha256s in duplicate_urls.items():\n",
    "    #   print(f\"URL: {url}\")\n",
    "    #   print(f\"Download SHA256 values: {sha256s}\\n\")\n",
    "else:\n",
    "    print(\"No duplicate URLs found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f93daea8-35ad-442b-a001-92e67a340c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_files_in_folders(folder_path, secondary_folder_path):\n",
    "    \"\"\"\n",
    "    Count the number of files in the given folder paths.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        The primary folder containing the files.\n",
    "    secondary_folder_path : str\n",
    "        A secondary folder to check if files are not found in the primary folder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        The total count of files in both folders.\n",
    "    \"\"\"\n",
    "    # Count files in the primary folder\n",
    "    primary_folder_files = [f for f in os.listdir(folder_path) if os.path.isfile(os.path.join(folder_path, f))]\n",
    "\n",
    "    # Count files in the secondary folder\n",
    "    secondary_folder_files = [f for f in os.listdir(secondary_folder_path) if os.path.isfile(os.path.join(secondary_folder_path, f))]\n",
    "\n",
    "    # Calculate the total count of files in both folders\n",
    "    total_files_count = len(primary_folder_files) + len(secondary_folder_files)\n",
    "\n",
    "    # Output the count\n",
    "    print(f\"Total number of files in {folder_path}: {len(primary_folder_files)}\")\n",
    "    print(f\"Total number of files in {secondary_folder_path}: {len(secondary_folder_files)}\")\n",
    "    print(f\"Total number of files in both folders: {total_files_count}\")\n",
    "\n",
    "    return total_files_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77c83316-8ff5-4643-95e5-0265fceb083a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Build full normalized paths dynamically\n",
    "malpedia_2022_reports = os.path.normpath(os.path.join(config_dir, config[\"threat_report_directories\"][\"Malpedia_2022\"]))\n",
    "malpedia_2024_reports = os.path.normpath(os.path.join(config_dir, config[\"threat_report_directories\"][\"Malpedia_2024\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b1dfed1-c979-40a2-bd38-41883e8b115a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in C:\\Users\\ricewater\\Documents\\CTIDownloads\\malpedia_20220718\\malpedia_20220718\\documents: 11363\n",
      "Total number of files in C:\\Users\\ricewater\\Documents\\CTIDownloads\\20241204_malpedia_downloads\\20241204_malpedia_downloads: 3620\n",
      "Total number of files in both folders: 14983\n"
     ]
    }
   ],
   "source": [
    "# Get total file count\n",
    "total_files = count_files_in_folders(malpedia_2022_reports, malpedia_2024_reports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "05963f1c-d019-4fc9-8ed4-8b914efee894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of files in C:\\Users\\ricewater\\Documents\\CTIDownloads\\malpedia_20220718\\malpedia_20220718\\iocs: 1060\n",
      "Total number of files in C:\\Users\\ricewater\\Documents\\CTIDownloads\\20241204_malpedia_downloads\\iocs: 590\n",
      "Total number of files in both folders: 1650\n"
     ]
    }
   ],
   "source": [
    "malpedia_2022_ttp = os.path.normpath(os.path.join(config_dir, config[\"directory_paths_ioc\"][\"TTP_Malpedia_2022\"]))\n",
    "malpedia_2024_ttp = os.path.normpath(os.path.join(config_dir, config[\"directory_paths_ioc\"][\"TTP_Malpedia_2024\"]))\n",
    "\n",
    "# Get total file count\n",
    "total_files = count_files_in_folders(malpedia_2022_ttp, malpedia_2024_ttp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78a804f7-db88-4d8d-b6da-43ae6a09e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cumulative_unique_ttps_malpedia(folder_path, secondary_folder_path):\n",
    "    \"\"\"\n",
    "    Count cumulative unique TTP IDs across all files in the given folder paths.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        Path to the first folder.\n",
    "    secondary_folder_path : str\n",
    "        Path to the second folder.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    set\n",
    "        A set of unique TTP IDs found in both folders.\n",
    "    \"\"\"\n",
    "    unique_ttps = set()\n",
    "\n",
    "    # Regex pattern to capture full TTP IDs (e.g., T1003, T1003.001)\n",
    "    ttp_pattern = re.compile(r'\\bT\\d{4}(?:\\.\\d+)?\\b')\n",
    "\n",
    "    # Loop through both folders\n",
    "    for folder in [folder_path, secondary_folder_path]:\n",
    "        for file_name in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, file_name)\n",
    "\n",
    "            # Skip hidden or backup files\n",
    "            if file_name.startswith('_'):\n",
    "                print(f\"Skipping hidden or backup file: {file_name}\")\n",
    "                continue\n",
    "\n",
    "            if os.path.isfile(file_path):\n",
    "                #print(f\"Reading TTPs from file: {file_name}\")\n",
    "                try:\n",
    "                    # Open the file and check each line for TTP patterns\n",
    "                    with open(file_path, 'r', errors='ignore') as file:\n",
    "                        for line in file:\n",
    "                            # Find all TTP IDs in the line and add them directly to the cumulative set\n",
    "                            matches = ttp_pattern.findall(line)\n",
    "                            unique_ttps.update(matches)  # Add each unique TTP ID found in the line\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_name}: {e}\")\n",
    "    \n",
    "    return unique_ttps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "56d1ea84-355c-4978-b281-32c5a69b0402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Unique TTPs found: 879\n"
     ]
    }
   ],
   "source": [
    "unique_ttps_malpedia= count_cumulative_unique_ttps_malpedia(malpedia_2022_ttp, malpedia_2024_ttp)\n",
    "\n",
    "# Display the unique TTPs\n",
    "print(f\"Cumulative Unique TTPs found: {len(unique_ttps_malpedia)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f72aecd0-0005-4e4f-a84f-aabafc7953ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_cumulative_unique_ttps_mitre(folder_path):\n",
    "    \"\"\"\n",
    "    Count cumulative unique TTP IDs across all files in the given folder paths.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        Path to the first folder.\n",
    "    secondary_folder_path : str\n",
    "        Path to the second folder.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    set\n",
    "        A set of unique TTP IDs found in both folders.\n",
    "    \"\"\"\n",
    "    unique_ttps = set()\n",
    "\n",
    "    # Regex pattern to capture full TTP IDs (e.g., T1003, T1003.001)\n",
    "    ttp_pattern = re.compile(r'\\bT\\d{4}(?:\\.\\d+)?\\b')\n",
    "\n",
    "    # Loop through both folders\n",
    "    for folder in [folder_path]:\n",
    "        for file_name in os.listdir(folder):\n",
    "            file_path = os.path.join(folder, file_name)\n",
    "\n",
    "            # Skip hidden or backup files\n",
    "            if file_name.startswith('_'):\n",
    "                print(f\"Skipping hidden or backup file: {file_name}\")\n",
    "                continue\n",
    "\n",
    "            if os.path.isfile(file_path):\n",
    "                #print(f\"Reading TTPs from file: {file_name}\")\n",
    "                try:\n",
    "                    # Open the file and check each line for TTP patterns\n",
    "                    with open(file_path, 'r', errors='ignore') as file:\n",
    "                        for line in file:\n",
    "                            # Find all TTP IDs in the line and add them directly to the cumulative set\n",
    "                            matches = ttp_pattern.findall(line)\n",
    "                            unique_ttps.update(matches)  # Add each unique TTP ID found in the line\n",
    "                except Exception as e:\n",
    "                    print(f\"Error reading file {file_name}: {e}\")\n",
    "    \n",
    "    return unique_ttps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b2d1b9a5-8917-4a61-8310-b17ba192858c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Unique TTPs found: 470\n"
     ]
    }
   ],
   "source": [
    "mitre_ttp_path = os.path.normpath(os.path.join(config_dir, config[\"directory_paths_ioc\"][\"TTP_MITRE\"]))\n",
    "unique_ttps_mitre = count_cumulative_unique_ttps_mitre(mitre_ttp_path)\n",
    "\n",
    "# Display the unique TTPs\n",
    "print(f\"Cumulative Unique TTPs found: {len(unique_ttps_mitre)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e159512c-6796-4642-88e0-701a7df2bc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "union_ttps = set(unique_ttps_mitre).union(set(unique_ttps_malpedia))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "40726ec4-b9d0-4b09-a0c0-b9faaa8964f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "883"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(union_ttps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
