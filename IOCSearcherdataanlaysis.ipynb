{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "253a930c-ee95-491e-bf32-7bec284d7fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_file_hashes(folder_path):\n",
    "    \"\"\"\n",
    "    Extracts file hashes (part before .download.iocs) from filenames in a folder.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        The path to the folder.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of file hashes extracted from the filenames.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # List all files in the specified folder and keep only the hashes\n",
    "        file_hashes = [\n",
    "            file.split('.download.iocs')[0] \n",
    "            for file in os.listdir(folder_path) \n",
    "            if os.path.isfile(os.path.join(folder_path, file))\n",
    "        ]\n",
    "        return file_hashes\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "08d9aa69-03ab-425b-8b38-6f1c88f5c70a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the path to the folder\n",
    "folder_path = r\"C:\\Users\\Aakanksha Saha\\Documents\\CTI_downloads\\downloads\\20241008_downloads\\iocs2\"\n",
    "# Get and print the list of file hashes\n",
    "file_hashes = get_file_hashes(folder_path)\n",
    "#print(\"File hashes:\", file_hashes)\n",
    "len(file_hashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "27f5a05f-2da2-4470-9c3f-dad7a57d3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from PyPDF2.errors import WrongPasswordError\n",
    "\n",
    "def read_cti_reports(folder_path, file_hashes, password=None):\n",
    "    \"\"\"\n",
    "    Reads the CTI report files with .download extension for each hash in file_hashes. Handles text files and encrypted PDFs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    folder_path : str\n",
    "        The path to the folder containing the CTI reports.\n",
    "    file_hashes : list\n",
    "        A list of file hashes to look for and read.\n",
    "    password : str, optional\n",
    "        Password to decrypt encrypted PDF files, by default None.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary where keys are file hashes and values are the file content or error message.\n",
    "    \"\"\"\n",
    "    report_results = {}\n",
    "\n",
    "    for file_hash in file_hashes:\n",
    "        file_path = os.path.join(folder_path, f\"{file_hash}.download\")\n",
    "\n",
    "        # Check if the file exists and is readable\n",
    "        if os.path.exists(file_path) and os.path.isfile(file_path):\n",
    "            try:\n",
    "                # Attempt to read the file as UTF-8 text\n",
    "                with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                    content = file.read()\n",
    "\n",
    "                parsed_data = parse_cti_report(content)  # This function parses the report\n",
    "                report_results[file_hash] = parsed_data\n",
    "\n",
    "            except UnicodeDecodeError:\n",
    "                # If a UnicodeDecodeError occurs, the file might be a binary PDF\n",
    "                #print(f\"Error reading file as text. Attempting to read as PDF: {file_path}\")\n",
    "                try:\n",
    "                    with open(file_path, 'rb') as file:\n",
    "                        pdf_reader = PyPDF2.PdfReader(file)\n",
    "\n",
    "                        # Handle encrypted PDF if a password is required\n",
    "                        if pdf_reader.is_encrypted:\n",
    "                            try:\n",
    "                                pdf_reader.decrypt(password or \"\")\n",
    "                            except WrongPasswordError:\n",
    "                                report_results[file_hash] = {\"error\": \"Failed to decrypt PDF, wrong password.\"}\n",
    "                                continue  # Skip to next file hash\n",
    "\n",
    "                        text = \"\"\n",
    "                        # Extract text from each page of the PDF\n",
    "                        for page_num in range(len(pdf_reader.pages)):\n",
    "                            text += pdf_reader.pages[page_num].extract_text()\n",
    "\n",
    "                        parsed_pdfdata = parse_pdf_cti_report(text)\n",
    "                        report_results[file_hash] = parsed_pdfdata\n",
    "\n",
    "                except Exception as e:\n",
    "                    report_results[file_hash] = {\"error\": f\"Failed to read file as PDF. Error: {e}\"}\n",
    "\n",
    "            except Exception as e:\n",
    "                # Handle any other errors during the file read process\n",
    "                report_results[file_hash] = {\"error\": f\"Error reading file: {e}\"}\n",
    "\n",
    "        else:\n",
    "            report_results[file_hash] = {\"error\": \"File does not exist or is not readable.\"}\n",
    "\n",
    "    return report_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cd64fbf4-aba8-4262-808d-e7896153acb2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of reports processed: 120\n",
      "\n",
      "File Hash: 02f2fc63bd20b7487591a4c3e833971ae4595c873a10bbb6b9bc683cdf90f4fb\n",
      "File Hash: 0382a53442f0a3f298a88663c233b9dd8b0673be66032978e6d70d62e5ce7de7\n",
      "File Hash: 06620af18d054b4fc4d325ab3b704b3f2d0a1edc253180222296cd3da6570fc3\n",
      "File Hash: 0b539f609729a6955e96fb233af7f739264d7be59e39370d5c88a8c49bc7de88\n",
      "File Hash: 0cb04099bd009700bddc94a322c531e0f3dabd6b8942a7759a213203fff35d1a\n",
      "File Hash: 12d5737b92ff5b884e1d9b810778a4f3a26d021917b4500a7804b68eebf8fef4\n",
      "File Hash: 169b2dec8cbbefbecfaa2d364350ca27f61934d1532142fae21a85013fe10e1b\n",
      "File Hash: 21424dec735d0c5d53c61f7a811a5f971a3c84ea1391dd1e4b07002d243401b0\n",
      "File Hash: 26c4b9714fd65fa07ac6c1dd514e7a8507a31a161ac66b9d1a1095a986ecbeef\n",
      "File Hash: 2a35b2945366c76a5f14636df5bd1720051d66106a86e49ac3065122ba11ca8a\n",
      "File Hash: 2e65ebf1966a9ea0fd8c15056ec082ed4eeed63843536fc699519a7262ec6231\n",
      "File Hash: 305233a3c56b1983ae4a42cd850bc68b92ecbe4af2e2f3aaeb17927d75cb4dfd\n",
      "File Hash: 3063db67750089075a77b4367ced61d82f307f447830f89b214a54d1b2a02c32\n",
      "File Hash: 3a41f47356147b2470a1239449d119441df95949b3b1c6235332089b1185aecf\n",
      "File Hash: 3c88bf349d55925f5976acd43d95252551e31d4db28b8dc02074bc0ae42335f7\n",
      "File Hash: 3e244eeece3b0e0da9cd12d00b508ff9d74c94b72d25f2466c3de228c136ddcf\n",
      "File Hash: 42be21c3a576ff6faa9b33c91b7cb72d4d83bf598ff658ec998931fec0e95c4a\n",
      "File Hash: 4344252e5155a3544c0b32a5e8df6026788ad64327500cddcb9f192d359adea6\n",
      "File Hash: 4af908bee1183a3c663a12ccc7b7a7fc01efd45df5050596496c2545941fd7e4\n",
      "File Hash: 4f8cb79942c675ef3112dcd1e6d2fbf5943b68c3ba57f737d6c119d8259134c0\n",
      "File Hash: 52c254f98e19c4be5c91a8f8c001500d31817940069ecc8a37c1b709d56d7470\n",
      "File Hash: 5316d614e82bd44bbe1e2befa5ee845cbb1c417ddd1fcc89277911988eb4facf\n",
      "File Hash: 5319f4ee45d1d7d9e51d2d3a3b404c97d939040644d1ed9ffee3b59db5d3d271\n",
      "File Hash: 57826f3e413c93f28c5a2ab3034ab095d1ffa8b3a5c344ba43faa1c28d0aa5de\n",
      "File Hash: 59e54e56c51f73e66b25aecd83fe6c1de4227937ad3afa1a33c6d328130df1fb\n",
      "File Hash: 5d22dbafca7525fb9df3f4bdfa66477c1c19deb0dd18937f443b86ac6ac8afe7\n",
      "File Hash: 6645913209afadb2532467920b4dcbe925fc9cd0ac278f935d8a9990e85bcae8\n",
      "File Hash: 6c2a5a7456f5c420876ae0ee7cb46cb216f734fd0af35db8ce01ee5dac8468a1\n",
      "File Hash: 6e6546ba8c4d1c9c06410dc8e450bf899f25e52007f0bf1474de85ce36151dda\n",
      "File Hash: 6fc903a56fbc5cdeacbbff949923f03333c221321ba4bb27d32674d2aff6b0c0\n",
      "File Hash: 77bba6a0889a0c4c32ba566df59e70d57031b27fc6aba9065adf732a319ad6b0\n",
      "File Hash: 7ba43f05552d927c203becb6f00d7c4e50b9f4b758e2e67682d981a7207aceeb\n",
      "File Hash: 7c59845c4a586e2a5211e3d80e7627aca6da4b57c44121509c2686c563faf349\n",
      "File Hash: 8077a6c2e5ff819c5ca16bb7f3d551097b2f04acd6f27e92fe130b5831613d2c\n",
      "File Hash: 81b35040a7207abc6fafa6df16b6205e890d724b4764a0c33d7b1a06353fa11e\n",
      "File Hash: 873cbacc54f4538633bd8e2e590bc9cd05c1e48650e19c4e52c0f18ad6ab1a64\n",
      "File Hash: 88ab4375ebcbf1da5ea7513780bc8691450327cf438b541058ee0f0ef502e0a2\n",
      "File Hash: 89ad678e0c9246e33d081261e9d916088fe2bd42d0c3889102cb01734079b0dc\n",
      "File Hash: 8ae87e31a626efb12b6f60ddfbd525d3a72890a0610065883022053510d2dec4\n",
      "File Hash: 8c82705aa69481f65f596865c35e8aa0c245faef95c56bdc9ecc4334252f4c34\n",
      "File Hash: 9100aaa8cf816c63e465c937be623824e710948d98b9bb64ec8b49eeb29db7bb\n",
      "File Hash: 98578a93824c0119eba38b252fafdc7f51b95b4362d4e4fad8663dee8912f8d4\n",
      "Error processing 9ab52ada0a79a93556686842c68d8b9b3af219a2b4ed474f3f20fe9db55a3a40: Failed to read file as PDF. Error: PyCryptodome is required for AES algorithm\n",
      "File Hash: 9b4de816941a6d9d3a3406fcf06c3fdf3761ca13ad9e71da88da42980181a269\n",
      "File Hash: a88bab77f73efc6d1862c2fed5f73623632f2c94fb8e75aa6f270d7c053ad0d6\n",
      "File Hash: a8b57ca7b883775ce7afccce01adf6c315428393a41860ce4fda8be396d36716\n",
      "File Hash: aa14157afbbf4352fbc19eec93cfac6248805b4e489b79f7fd146335e0ba798c\n",
      "File Hash: aa8f1a0936678c6fd66da7350fa5a9c4b3065f41b5ebf54b9ff20adb7fdc9086\n",
      "File Hash: ad4cbaf30d17e072ac9093993dc15ea4a128c62bfc5902c1552b00a65e5bd5c1\n",
      "File Hash: c1505d3e835700d000a7f5aa74acf737677b91ca636834f9b434191edd288706\n",
      "File Hash: caf7d43297c065bcd7cbf18ded9fda345e4448171fb0e2c945799fda3ad059b3\n",
      "File Hash: d01b4d574ca7ea3e0c685c4072722426d889d7d1a84bf3fa2c670ee8b3c90ac2\n",
      "File Hash: d820c1c717630139f90e0e892b6285185c5b170839aba64ec6bf31dec219225d\n",
      "File Hash: d9e0e4e585fb0cee470b04773e80e6262675f9cd45ce742dc7485fcc17b75ad2\n",
      "File Hash: dbeb31c43ae762def2c6ca6a7a3d4c650e7a3f7996a6172dfb9ff0815229b3a1\n",
      "File Hash: df5a172a97e5f243e6e2e4011c66723b3be1e858eb2f8acca5bf3456dc357bbe\n",
      "File Hash: e47acfbde4a660fd25923bc4f4321ce0afe1dd7dd3504b2449139836f66377e8\n",
      "File Hash: e5f780fd672b31a4b802ca09a6679c3ef86e4a6ff76bda64197bfa407cab5c16\n",
      "File Hash: ebe7aeae666fd3f6ba10f5b0f962fd43426229ef9ee794fb3cf9b8b567bbbf59\n",
      "File Hash: ecfc4a1e67fd98bcb1709d36fbf98db6d45e9414c4ee450dcefce04e98a4ee25\n",
      "File Hash: ef59b4f3ae8057f1f769ad57d09a1dd4815a6445a1577be0f9a11e5be0a4b5c5\n",
      "File Hash: efe266b24925e2e15494c5a3ebdae338bbe163026ecd62b219f865eee163a7c6\n",
      "File Hash: f28dbb0a7868d8319e0e7874633284f26179c9e2b5174f6a7e27e3a24264b3e1\n",
      "File Hash: f335dc7b6c9b30eb4f5c65b8317e409dbafc6bd137bc8319afcd17be902c419a\n",
      "File Hash: f7624edb2ecb0258e2861dfc00fdec323dce97278ef5681d408983dc19e975e8\n",
      "File Hash: f8f93bdcaf797fdea93ccbbb066f31c95973a1495b5d7f1e78db51dc0370bff4\n",
      "Total file hashes where TTPs in text = 0 and TTPs in table or list > 0: 65\n",
      "Total file hashes where TTPs in table or list = 0 and TTPs in text > 0: 44\n"
     ]
    }
   ],
   "source": [
    "folder_path = r\"C:\\Users\\Aakanksha Saha\\Documents\\CTI_downloads\\downloads\\20241008_downloads\"\n",
    "password = \"infected\"\n",
    "cti_reports = read_cti_reports(folder_path, file_hashes, password=password)\n",
    "total_reports = len(cti_reports)\n",
    "file_hashes_count_table = 0\n",
    "file_hashes_count_text = 0\n",
    "print(f\"Total number of reports processed: {total_reports}\\n\")\n",
    "\n",
    "for file_hash, result in cti_reports.items():\n",
    "    if \"error\" in result:\n",
    "        print(f\"Error processing {file_hash}: {result['error']}\")\n",
    "    else:\n",
    "        # Get the lengths of TTPs in text and in structured content (tables/lists)\n",
    "        ttps_in_text_length = len(result['ttps_in_text'])\n",
    "        ttps_in_table_or_list_length = len(result['ttps_in_table_or_list'])\n",
    "\n",
    "        if ttps_in_text_length == 0 and ttps_in_table_or_list_length > 0:\n",
    "            file_hashes_count_table += 1\n",
    "            print(f\"File Hash: {file_hash}\")\n",
    "        if ttps_in_table_or_list_length == 0 and ttps_in_text_length > 0:\n",
    "            file_hashes_count_text += 1\n",
    "        \n",
    "        \n",
    "        #print(f\"File Hash: {file_hash}\")\n",
    "        #print(f\"TTPs in text: {ttps_in_text_length} entries\")\n",
    "        #print(f\"TTPs in text: {result['ttps_in_text']}\")\n",
    "        #print(f\"TTPs in table or list: {ttps_in_table_or_list_length} entries\")\n",
    "        #print(f\"TTPs in table or list: {result['ttps_in_table_or_list']}\")\n",
    "        #print(\"-\" * 50)\n",
    "\n",
    "# Output the total count of file hashes that meet the criteria\n",
    "print(f\"Total file hashes where TTPs in text = 0 and TTPs in table or list > 0: {file_hashes_count_table}\")\n",
    "print(f\"Total file hashes where TTPs in table or list = 0 and TTPs in text > 0: {file_hashes_count_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a8c3926e-3228-46ef-89dd-888addca2866",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Analysis hashes: \n",
    "\n",
    "#0c4e350517502f10c3986a7e20d7ce2b78fbc417a36aa22a370a56b124026e9b - https://www.cisa.gov/news-events/cybersecurity-advisories/aa22-055a\n",
    "#2ef3aa28b21deee943bed752a1ac7950382c951a54fd50323bf67b39ee3f5476 - https://www.lacework.com/blog/taking-teamtnt-docker-images-offline/\n",
    "#5d39c90de10384fa86bafb4016761bf84f6d83964a06a38ef4c0db7f0d3b4532 - https://us-cert.cisa.gov/ncas/alerts/aa21-048a\n",
    "#a6c1cbd286cdc07366367c3a9313719dac1c472eb8cd65361f211781eeaf809c - https://us-cert.cisa.gov/ncas/alerts/aa20-301a\n",
    "#e4c8a7ace1cc91c65aadd49419614b71da517763443105b77ee7379d2421528e - https://www.cisa.gov/news-events/cybersecurity-advisories/aa23-320a\n",
    "\n",
    "\n",
    "## Hashes to remove\n",
    "###bff733b5ddd507076bcef720c7d068d3c970c7bb934bde080a29a091432973e8 - https://blog.trendmicro.com/trendlabs-security-intelligence/gamaredon-apt-group-use-covid-19-lure-in-campaigns/\n",
    "##15e4f4ac4a0a5c1b6876c4b4907c66110d319ba03917c991214e8a6edffdb817 - https://blog.trendmicro.com/trendlabs-security-intelligence/gamaredon-apt-group-use-covid-19-lure-in-campaigns/\n",
    "#e9240c31bbb736b7a49df9da3d79f0b65e1173743b2de749da4c55c44fedddf4 - https://unit42.paloaltonetworks.com/molerats-delivers-spark-backdoor/\n",
    "##False list\n",
    "#424f242d75014b9a8f8eda8dbed34a324c32a4106cafe832a5fb5a54f3512230 - https://www.lacework.com/blog/taking-teamtnt-docker-images-offline/\n",
    "##-->This is duplicate of 2ef3\n",
    "##9acadc6a13214395837884616121f6254b86dcab0951564b4d5dbc58c64c5166 - https://www.mandiant.com/resources/blog/sandworm-disrupts-power-ukraine-operational-technology\n",
    "##->This ttp in the text is not really correct because it is a part of the YARA signature content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f9b73959-3cfe-4653-9628-4d1c1706cc5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Hash: 0c4e350517502f10c3986a7e20d7ce2b78fbc417a36aa22a370a56b124026e9b\n",
      "TTPs in text: 15 entries\n",
      "TTPs in table or list: 56 entries\n",
      "Number of common TTPs (Intersection): 12\n",
      "Common TTPs: {'T1041', 'T1071.001', 'T1036.005', 'T1059.006', 'T1027', 'T1204.002', 'T1480', 'T1059.001', 'T1566.001', 'T1132.002', 'T1574.002', 'T1547.001'}\n",
      "TTPs in text but not in table/list (A - B): 3\n",
      "TTPs in text but not in table/list (A - B): {'T1001.001', 'T1572', 'T1005'}\n",
      "TTPs in table/list but not in text (B - A): 44\n",
      "TTPs in table/list but not in text (B - A): {'T1049', 'T1559.002', 'T1560.001', 'T1566.002', 'T1059.007', 'T1102.002', 'T1003.001', 'T1105', 'T1059.005', 'T1548.002', 'T1033', 'T1016', 'T1082', 'T1027.004', 'T1218.005', 'T1113', 'T1053.005', 'T1588.002', 'T1589.002', 'T1087.002', 'T1559.001', 'T1090.002', 'T1047', 'T1518', 'T1219', 'T1132.001', 'T1204.001', 'T1518.001', 'T1218.003', 'T1137.001', 'T1027.003', 'T1083', 'T1218.011', 'T1057', 'T1555', 'T1583.006', 'T1104', 'T1003.004', 'T1555.003', 'T1552.001', 'T1203', 'T1003.005', 'T1562.001', 'T1140'}\n",
      "***After excluding the subtecniques***\n",
      "Number of common TTPs (Intersection): 11\n",
      "Common TTPs: {'T1041', 'T1566', 'T1071', 'T1027', 'T1480', 'T1036', 'T1574', 'T1547', 'T1204', 'T1059', 'T1132'}\n",
      "TTPs in text but not in table/list (A - B): 3\n",
      "TTPs in text but not in table/list (A - B): {'T1001', 'T1572', 'T1005'}\n",
      "TTPs in table/list but not in text (B - A): 30\n",
      "TTPs in table/list but not in text (B - A): {'T1560', 'T1049', 'T1588', 'T1548', 'T1090', 'T1105', 'T1003', 'T1033', 'T1016', 'T1137', 'T1082', 'T1087', 'T1113', 'T1102', 'T1589', 'T1047', 'T1559', 'T1518', 'T1583', 'T1219', 'T1552', 'T1218', 'T1083', 'T1057', 'T1053', 'T1555', 'T1104', 'T1203', 'T1140', 'T1562'}\n",
      "--------------------------------------------------\n",
      "File Hash: 2ef3aa28b21deee943bed752a1ac7950382c951a54fd50323bf67b39ee3f5476\n",
      "TTPs in text: 6 entries\n",
      "TTPs in table or list: 5 entries\n",
      "Number of common TTPs (Intersection): 4\n",
      "Common TTPs: {'T1525', 'T1204.003', 'T1078.004', 'T1610'}\n",
      "TTPs in text but not in table/list (A - B): 2\n",
      "TTPs in text but not in table/list (A - B): {'T1552.001', 'T1613'}\n",
      "TTPs in table/list but not in text (B - A): 1\n",
      "TTPs in table/list but not in text (B - A): {'T1552.002'}\n",
      "***After excluding the subtecniques***\n",
      "Number of common TTPs (Intersection): 5\n",
      "Common TTPs: {'T1078', 'T1552', 'T1525', 'T1204', 'T1610'}\n",
      "TTPs in text but not in table/list (A - B): 1\n",
      "TTPs in text but not in table/list (A - B): {'T1613'}\n",
      "TTPs in table/list but not in text (B - A): 0\n",
      "TTPs in table/list but not in text (B - A): set()\n",
      "--------------------------------------------------\n",
      "File Hash: 5d39c90de10384fa86bafb4016761bf84f6d83964a06a38ef4c0db7f0d3b4532\n",
      "TTPs in text: 22 entries\n",
      "TTPs in table or list: 20 entries\n",
      "Number of common TTPs (Intersection): 19\n",
      "Common TTPs: {'T1566.002', 'T1204.002', 'T1548', 'T1033', 'T1543.004', 'T1053.005', 'T1573.001', 'T1547', 'T1587.001', 'T1041', 'T1027', 'T1583.001', 'T1071.001', 'T1564.001', 'T1583.006', 'T1588.003', 'T1059.004', 'T1053.004', 'T1059'}\n",
      "TTPs in text but not in table/list (A - B): 3\n",
      "TTPs in text but not in table/list (A - B): {'T1059.002', 'T1543.003', 'T1588.004'}\n",
      "TTPs in table/list but not in text (B - A): 1\n",
      "TTPs in table/list but not in text (B - A): {'T1573'}\n",
      "***After excluding the subtecniques***\n",
      "Number of common TTPs (Intersection): 16\n",
      "Common TTPs: {'T1041', 'T1587', 'T1566', 'T1588', 'T1071', 'T1564', 'T1543', 'T1027', 'T1053', 'T1548', 'T1583', 'T1033', 'T1547', 'T1204', 'T1059', 'T1573'}\n",
      "TTPs in text but not in table/list (A - B): 0\n",
      "TTPs in text but not in table/list (A - B): set()\n",
      "TTPs in table/list but not in text (B - A): 0\n",
      "TTPs in table/list but not in text (B - A): set()\n",
      "--------------------------------------------------\n",
      "File Hash: a6c1cbd286cdc07366367c3a9313719dac1c472eb8cd65361f211781eeaf809c\n",
      "TTPs in text: 16 entries\n",
      "TTPs in table or list: 18 entries\n",
      "Number of common TTPs (Intersection): 7\n",
      "Common TTPs: {'T1546.001', 'T1566.002', 'T1562.004', 'T1566.001', 'T1185', 'T1547.001', 'T1056.001'}\n",
      "TTPs in text but not in table/list (A - B): 9\n",
      "TTPs in text but not in table/list (A - B): {'T1560', 'T1074.001', 'T1059.006', 'T1550.002', 'T1114.003', 'T1573.001', 'T1083', 'T1219', 'T1189'}\n",
      "TTPs in table/list but not in text (B - A): 11\n",
      "TTPs in table/list but not in text (B - A): {'T1040', 'T1021.001', 'T1055', 'T1070.004', 'T1059.001', 'T1505.003', 'T1548.002', 'T1003', 'T1547', 'T1082', 'T1218.005'}\n",
      "***After excluding the subtecniques***\n",
      "Number of common TTPs (Intersection): 7\n",
      "Common TTPs: {'T1566', 'T1056', 'T1546', 'T1185', 'T1547', 'T1059', 'T1562'}\n",
      "TTPs in text but not in table/list (A - B): 8\n",
      "TTPs in text but not in table/list (A - B): {'T1560', 'T1550', 'T1074', 'T1083', 'T1114', 'T1219', 'T1189', 'T1573'}\n",
      "TTPs in table/list but not in text (B - A): 9\n",
      "TTPs in table/list but not in text (B - A): {'T1040', 'T1021', 'T1218', 'T1055', 'T1548', 'T1070', 'T1003', 'T1082', 'T1505'}\n",
      "--------------------------------------------------\n",
      "File Hash: e4c8a7ace1cc91c65aadd49419614b71da517763443105b77ee7379d2421528e\n",
      "TTPs in text: 30 entries\n",
      "TTPs in table or list: 36 entries\n",
      "Number of common TTPs (Intersection): 30\n",
      "Common TTPs: {'T1566.004', 'T1078', 'T1567.002', 'T1199', 'T1136', 'T1656', 'T1660', 'T1566', 'T1578.002', 'T1538', 'T1589', 'T1585.001', 'T1114', 'T1219', 'T1074', 'T1552.004', 'T1213.003', 'T1583.001', 'T1018', 'T1083', 'T1530', 'T1484.002', 'T1556.006', 'T1486', 'T1078.002', 'T1552.001', 'T1021.007', 'T1648', 'T1606', 'T1213.002'}\n",
      "TTPs in text but not in table/list (A - B): 0\n",
      "TTPs in text but not in table/list (A - B): set()\n",
      "TTPs in table/list but not in text (B - A): 6\n",
      "TTPs in table/list but not in text (B - A): {'T1217', 'T1598', 'T1539', 'T1621', 'T1657', 'T1204'}\n",
      "***After excluding the subtecniques***\n",
      "Number of common TTPs (Intersection): 26\n",
      "Common TTPs: {'T1078', 'T1199', 'T1213', 'T1136', 'T1656', 'T1660', 'T1566', 'T1538', 'T1589', 'T1114', 'T1583', 'T1578', 'T1219', 'T1074', 'T1552', 'T1021', 'T1567', 'T1018', 'T1083', 'T1530', 'T1484', 'T1486', 'T1585', 'T1556', 'T1648', 'T1606'}\n",
      "TTPs in text but not in table/list (A - B): 0\n",
      "TTPs in text but not in table/list (A - B): set()\n",
      "TTPs in table/list but not in text (B - A): 6\n",
      "TTPs in table/list but not in text (B - A): {'T1217', 'T1598', 'T1539', 'T1621', 'T1657', 'T1204'}\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "excluded_hashes = {\n",
    "    \"bff733b5ddd507076bcef720c7d068d3c970c7bb934bde080a29a091432973e8\",\n",
    "    \"15e4f4ac4a0a5c1b6876c4b4907c66110d319ba03917c991214e8a6edffdb817\",\n",
    "    \"e9240c31bbb736b7a49df9da3d79f0b65e1173743b2de749da4c55c44fedddf4\",\n",
    "    \"424f242d75014b9a8f8eda8dbed34a324c32a4106cafe832a5fb5a54f3512230\",\n",
    "    \"9acadc6a13214395837884616121f6254b86dcab0951564b4d5dbc58c64c5166\"   \n",
    "}\n",
    "\n",
    "# Creating reports_with_both_ttps without the excluded file hashes\n",
    "reports_with_both_ttps = {\n",
    "    hash_key: result for hash_key, result in cti_reports.items()\n",
    "    if hash_key not in excluded_hashes and \"error\" not in result and result[\"ttps_in_text\"] and result[\"ttps_in_table_or_list\"]\n",
    "}\n",
    "\n",
    "# Printing only those results\n",
    "if reports_with_both_ttps:\n",
    "    #print(f\"Reports with TTPs in both text and table/list: {len(reports_with_both_ttps)}\\n\")\n",
    "\n",
    "    for hash_key, result in reports_with_both_ttps.items():\n",
    "        ttps_in_text_length = len(result['ttps_in_text'])\n",
    "        ttps_in_table_or_list_length = len(result['ttps_in_table_or_list'])\n",
    "        \n",
    "        print(f\"File Hash: {hash_key}\")\n",
    "        print(f\"TTPs in text: {ttps_in_text_length} entries\")\n",
    "        #print(f\"TTPs in text: {result['ttps_in_text']}\")\n",
    "        print(f\"TTPs in table or list: {ttps_in_table_or_list_length} entries\")\n",
    "        #print(f\"TTPs in table or list: {result['ttps_in_table_or_list']}\")\n",
    "\n",
    "        # Extracting just the TTP IDs (without the suffixes)\n",
    "        base_ttp_text = set([ttp[0] for ttp in result['ttps_in_text']])\n",
    "        base_ttp_table = set([ttp[0] for ttp in result['ttps_in_table_or_list']])\n",
    "\n",
    "        # Find common TTPs (Intersection)\n",
    "        common_ttps = base_ttp_text.intersection(base_ttp_table)\n",
    "        \n",
    "        # Find TTPs in text but not in the table (A - B)\n",
    "        ttps_in_text_only = base_ttp_text.difference(base_ttp_table)\n",
    "        \n",
    "        # Find TTPs in table but not in the text (B - A)\n",
    "        ttps_in_table_only = base_ttp_table.difference(base_ttp_text)\n",
    "\n",
    "        print(f\"Number of common TTPs (Intersection): {len(common_ttps)}\")\n",
    "        print(f\"Common TTPs: {common_ttps}\")\n",
    "        # Printing lengths of TTPs that are in text but not in the table/list (A - B) and vice versa (B - A)\n",
    "        print(f\"TTPs in text but not in table/list (A - B): {len(ttps_in_text_only)}\")\n",
    "        print(f\"TTPs in text but not in table/list (A - B): {ttps_in_text_only}\")\n",
    "        print(f\"TTPs in table/list but not in text (B - A): {len(ttps_in_table_only)}\")\n",
    "        print(f\"TTPs in table/list but not in text (B - A): {ttps_in_table_only}\")\n",
    "\n",
    "        print(\"***After excluding the subtecniques***\")\n",
    "\n",
    "        ###Identifying overlap when the sub-technique IDs are not included\n",
    "        # Extracting just the main TTP IDs (ignoring the sub-technique part if present)\n",
    "        base_ttp_text_1 = set([ttp[0].split('.')[0] for ttp in result['ttps_in_text']])\n",
    "        base_ttp_table_1 = set([ttp[0].split('.')[0] for ttp in result['ttps_in_table_or_list']])\n",
    "\n",
    "        # Find common TTPs (Intersection)\n",
    "        common_ttps_1 = base_ttp_text_1.intersection(base_ttp_table_1)\n",
    "        \n",
    "        # Find TTPs in text but not in the table (A - B)\n",
    "        ttps_in_text_only_1 = base_ttp_text_1.difference(base_ttp_table_1)\n",
    "        \n",
    "        # Find TTPs in table but not in the text (B - A)\n",
    "        ttps_in_table_only_1 = base_ttp_table_1.difference(base_ttp_text_1)\n",
    "\n",
    "        print(f\"Number of common TTPs (Intersection): {len(common_ttps_1)}\")\n",
    "        print(f\"Common TTPs: {common_ttps_1}\")\n",
    "        # Printing lengths of TTPs that are in text but not in the table/list (A - B) and vice versa (B - A)\n",
    "        print(f\"TTPs in text but not in table/list (A - B): {len(ttps_in_text_only_1)}\")\n",
    "        print(f\"TTPs in text but not in table/list (A - B): {ttps_in_text_only_1}\")\n",
    "        print(f\"TTPs in table/list but not in text (B - A): {len(ttps_in_table_only_1)}\")\n",
    "        print(f\"TTPs in table/list but not in text (B - A): {ttps_in_table_only_1}\")\n",
    "\n",
    "        print(\"-\" * 50)\n",
    "       \n",
    "else:\n",
    "    print(\"No reports found with both TTPs in text and table/list.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6036614b-e1a9-45f0-af67-f1f7c499a357",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def identify_structured_sections(content):\n",
    "    \"\"\"\n",
    "    Identifies potential structured sections (like lists or tables) in the PDF text content by\n",
    "    detecting patterns such as bullet points, numbered lists, or tabular-like text structures.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content : str\n",
    "        The text content of the PDF to analyze.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with plain text and structured sections.\n",
    "    \"\"\"\n",
    "    # Split the content into lines for easier processing\n",
    "    lines = content.splitlines()\n",
    "\n",
    "    structured_content = []\n",
    "    plain_text_content = []\n",
    "    \n",
    "    in_structured_section = False\n",
    "    structured_section = []\n",
    "\n",
    "    for line in lines:\n",
    "        stripped_line = line.strip()\n",
    "        \n",
    "        # Detect simple bullet points, numbered lists, or table-like sections\n",
    "        if re.match(r'^(?:\\d+\\.\\d+|\\d+\\.)', stripped_line):  # This matches numeric lists (1., 1.1, etc.)\n",
    "            in_structured_section = True\n",
    "            structured_section.append(stripped_line)\n",
    "        elif re.match(r'^\\s{2,}', stripped_line):  # Indentation in the content indicates tabular format\n",
    "            in_structured_section = True\n",
    "            structured_section.append(stripped_line)\n",
    "        else:\n",
    "            if in_structured_section:\n",
    "                # We assume the structured section ends here\n",
    "                structured_content.append(' '.join(structured_section))\n",
    "                structured_section = []\n",
    "                in_structured_section = False\n",
    "            plain_text_content.append(stripped_line)\n",
    "\n",
    "    # In case there's an unclosed structured section\n",
    "    if structured_section:\n",
    "        structured_content.append(' '.join(structured_section))\n",
    "\n",
    "    # Join plain text content together into a single block\n",
    "    plain_text = ' '.join(plain_text_content)\n",
    "\n",
    "    return {\n",
    "        \"structured_content\": ' '.join(structured_content),\n",
    "        \"plain_text_content\": plain_text\n",
    "    }\n",
    "\n",
    "\n",
    "def parse_pdf_cti_report(pdf_content):\n",
    "    \"\"\"\n",
    "    Separates plain text content from structured (list/table-like) content in a PDF, \n",
    "    then extracts TTPs from each section.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    pdf_content : str\n",
    "        The complete content of a CTI report extracted from a PDF.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with TTPs found in text, list, and table-like content.\n",
    "    \"\"\"\n",
    "    ttp_pattern = r'\\b(T[0-9]{4}(?:\\.[0-9]{3})?)\\b'\n",
    "\n",
    "    # Identify structured sections like lists or table-like data\n",
    "    sections = identify_structured_sections(pdf_content)\n",
    "    \n",
    "    # Extract TTPs from structured content (detected as lists or table-like)\n",
    "    ttps_in_structured_content = extract_ttps(sections['structured_content'], ttp_pattern)\n",
    "    ttps_in_structured_content = list(set(ttps_in_structured_content))\n",
    "\n",
    "    # Extract TTPs from plain text content (excluding structured sections)\n",
    "    ttps_in_text = extract_ttps(sections['plain_text_content'], ttp_pattern)\n",
    "    ttps_in_text = list(set(ttps_in_text))\n",
    "\n",
    "    # Ensure that TTPs in text and structured data are truly separate\n",
    "    #if set(ttps_in_text).issubset(ttps_in_structured_content):\n",
    "    #    ttps_in_text = []\n",
    "\n",
    "    return {\n",
    "        \"ttps_in_text\": ttps_in_text,\n",
    "        \"ttps_in_table_or_list\": ttps_in_structured_content\n",
    "    }\n",
    "\n",
    "def extract_ttps(content, pattern):\n",
    "    \"\"\"\n",
    "    Extracts TTPs from the given content using the specified regex pattern.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content : str\n",
    "        The text content to search for TTPs.\n",
    "    pattern : str\n",
    "        The regex pattern to match TTPs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of matched TTPs.\n",
    "    \"\"\"\n",
    "    return re.findall(pattern, content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2128fb95-2285-41a7-8b71-c9d92655ece9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_ttps(content, pattern):\n",
    "    \"\"\"\n",
    "    Extracts TTPs from the given content using the specified regex pattern.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content : str\n",
    "        The text content to search for TTPs.\n",
    "    pattern : str\n",
    "        The regex pattern to match TTPs.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of matched TTPs.\n",
    "    \"\"\"\n",
    "    return re.findall(pattern, content)\n",
    "\n",
    "def parse_cti_report(content):\n",
    "    \"\"\"\n",
    "    Separates plain text content from table or list content, then extracts TTPs from each section.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    content : str\n",
    "        The complete content of a CTI report.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        A dictionary with TTPs found in text, list, and table content.\n",
    "    \"\"\"\n",
    "    ttp_pattern = r'\\b(T[0-9]{4}([.][0-9]{3})?)\\b'\n",
    "    \n",
    "    # Convert the content to a BeautifulSoup object for easier parsing\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "    # 1. Extract all structured content: tables and lists with TTPs\n",
    "    ttps_in_table_or_list = []\n",
    "\n",
    "    # Extract from tables\n",
    "    for table in soup.find_all(\"table\"):\n",
    "        for row in table.find_all(\"tr\"):\n",
    "            for cell in row.find_all(\"td\"):\n",
    "                ttps_in_table_or_list.extend(extract_ttps(cell.get_text(), ttp_pattern))\n",
    "    \n",
    "    # Extract from unordered lists\n",
    "    for ul in soup.find_all(\"ul\"):\n",
    "        for li in ul.find_all(\"li\"):\n",
    "            ttps_in_table_or_list.extend(extract_ttps(li.get_text(), ttp_pattern))\n",
    "\n",
    "    # Extract from ordered lists\n",
    "    for ol in soup.find_all(\"ol\"):\n",
    "        for li in ol.find_all(\"li\"):\n",
    "            ttps_in_table_or_list.extend(extract_ttps(li.get_text(), ttp_pattern))\n",
    "\n",
    "    # Remove duplicates from structured content\n",
    "    ttps_in_table_or_list = list(set(ttps_in_table_or_list))\n",
    "\n",
    "    # 2. Extract plain text content, excluding tables and lists\n",
    "    for element in soup([\"table\", \"ul\", \"ol\"]):\n",
    "        element.extract()  # Remove structured elements from soup to isolate plain text\n",
    "\n",
    "    # Find TTPs in plain text content\n",
    "    plain_text_content = soup.get_text()\n",
    "    ttps_in_text = extract_ttps(plain_text_content, ttp_pattern)\n",
    "    ttps_in_text = list(set(ttps_in_text))\n",
    "\n",
    "    # 3. Check if TTPs in text and structured data are truly separate\n",
    "    #if set(ttps_in_text).issubset(ttps_in_table_or_list):\n",
    "    #    ttps_in_text = []\n",
    "\n",
    "    return {\n",
    "        \"ttps_in_text\": ttps_in_text,\n",
    "        \"ttps_in_table_or_list\": ttps_in_table_or_list\n",
    "    }"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
