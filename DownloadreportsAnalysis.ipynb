{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0f516d4-d62b-41ad-b658-8ff365b39220",
   "metadata": {},
   "outputs": [],
   "source": [
    "##17 hashes are duplicated i.e, have duplicate URLs and have been downled as a same report with same TTPs as two different hashes -> 120 - 17 is the true ground truth\n",
    "# Function to find unique hashes with URLs and track removed hashes\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "393ddb2c-ab87-4721-a441-e9cfe70347e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of URLs: 1372\n"
     ]
    }
   ],
   "source": [
    "# Function to count the number of URLs in the file\n",
    "def count_urls(file_path):\n",
    "    url_count = 0\n",
    "    \n",
    "    # Open and read the file\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Count non-empty lines (assuming each line contains one URL)\n",
    "            if line.strip():  # Strip any whitespace and check if the line is not empty\n",
    "                url_count += 1\n",
    "                \n",
    "    return url_count\n",
    "\n",
    "file_path = r\"C:\\Users\\Aakanksha Saha\\Documents\\CTI_downloads\\downloads\\20240930_urls.txt\"\n",
    "\n",
    "# Call the function and print the result\n",
    "print(f'Total number of URLs: {count_urls(file_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4a2765cb-8cfc-4676-86ba-2e844fcf0ed8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of downloaded files (SHA256 hash names):\n",
      "\n",
      "Total number of downloaded files: 11339\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Function to list and count files in a folder, excluding certain file types\n",
    "def list_downloaded_files(folder_path):\n",
    "    downloaded_files = []\n",
    "\n",
    "    # Walk through the directory structure\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file_name in files:\n",
    "            # Exclude files ending with '.download.iocs'\n",
    "            if not file_name.endswith('.download.iocs'):\n",
    "                downloaded_files.append(file_name)\n",
    "\n",
    "    # Return the list of files and the total count\n",
    "    return downloaded_files, len(downloaded_files)\n",
    "\n",
    "# Specify the path to the folder containing downloaded files\n",
    "#folder_path_downloads = r\"C:\\Users\\Aakanksha Saha\\Documents\\CTI_downloads\\downloads\\20241008_downloads\"\n",
    "folder_path_downloads = 'C:/Users/Aakanksha Saha/Documents/CTI_downloads/malpedia_20220718/malpedia_20220718/documents'\n",
    "\n",
    "# Call the function and get the list of files and their count\n",
    "downloaded_files, total_count = list_downloaded_files(folder_path_downloads)\n",
    "\n",
    "# Print the list of files and the total count\n",
    "print(\"List of downloaded files (SHA256 hash names):\")\n",
    "#for file_name in downloaded_files:\n",
    "#    print(file_name)\n",
    "\n",
    "print(f\"\\nTotal number of downloaded files: {total_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "61f03abc-79f4-4f78-bfc3-6d0c56dd1e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to identify successful downloads, filtering by download size\n",
    "def identify_successful_downloads(file_path):\n",
    "    successful_downloads = []\n",
    "    \n",
    "    # Open and read the file line by line\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Parse each line as a JSON object\n",
    "                download_info = json.loads(line)\n",
    "                \n",
    "                # Check if the download status is 200 (successful) and size is 20KB or more\n",
    "                if download_info.get(\"download_status\") == 200:\n",
    "                #and download_info.get(\"download_size\", 0) >= 20000:\n",
    "                    successful_downloads.append({\n",
    "                        \"url\": download_info.get(\"url\"),\n",
    "                        \"download_sha256\": download_info.get(\"download_sha256\"),\n",
    "                        \"download_status\": download_info.get(\"download_status\"),\n",
    "                        \"download_size\": download_info.get(\"download_size\"),\n",
    "                        \"download_ts\": download_info.get(\"download_ts\")\n",
    "                    })\n",
    "            except json.JSONDecodeError:\n",
    "                # Handle case where the line is not a valid JSON (optional)\n",
    "                print(f\"Error decoding JSON for line: {line.strip()}\")\n",
    "    \n",
    "    return successful_downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "422ca229-378f-4a6d-ac9f-62cccdb7067b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total number of successful downloads (size >= 20KB): 11385\n"
     ]
    }
   ],
   "source": [
    "# Call the function and get the successful downloads\n",
    "successful_downloads = identify_successful_downloads(file_path_for_jsonl)\n",
    "\n",
    "# Print the total count of successful downloads with size >= 20KB\n",
    "print(f\"\\nTotal number of successful downloads (size >= 20KB): {len(successful_downloads)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4785f933-79b9-40c1-b72b-cf169f008cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Extracting report hashes from the filenames\n",
    "# Function to recursively extract the hash part of filenames ending with '.download.iocs'\n",
    "def extract_hashes_from_filenames(folder_path_of_iocs):\n",
    "    file_hash_list = []\n",
    "\n",
    "    # Walk through the directory structure recursively\n",
    "    for root, dirs, files in os.walk(folder_path_of_iocs):\n",
    "        for file_name in files:\n",
    "            # Check if the filename ends with '.download.iocs'\n",
    "            if file_name.endswith('.download.iocs'):\n",
    "                # Extract the hash part (the filename before the extension)\n",
    "                hash_part = file_name.split('.download.iocs')[0]\n",
    "                file_hash_list.append(hash_part)\n",
    "\n",
    "    return file_hash_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "27ece7b5-b763-4636-9530-6cc34ddea894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total file hashes extracted: 1051\n"
     ]
    }
   ],
   "source": [
    "folder_path_of_iocs = 'C:/Users/Aakanksha Saha/Documents/CTI_downloads/malpedia_20220718/malpedia_20220718/iocs'\n",
    "#folder_path_of_iocs = 'C:/Users/Aakanksha Saha/Documents/CTI_downloads/downloads/20241008_downloads/iocs2'\n",
    "# Extract the hashes from filenames and store them in a list\n",
    "file_hash_list = extract_hashes_from_filenames(folder_path_of_iocs)\n",
    "\n",
    "# Print the total number of hashes\n",
    "print(f\"\\nTotal file hashes extracted: {len(file_hash_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9c095ca-429a-431e-bdae-e1db31010c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_path_for_jsonl = r\"C:\\Users\\Aakanksha Saha\\Documents\\CTI_downloads\\downloads\\20241008_downloads.jsonl\"\n",
    "file_path_for_jsonl = r\"C:\\Users\\Aakanksha Saha\\Documents\\CTI_downloads\\malpedia_20220718\\malpedia_20220718\\malpedia-db_2022-07-18_downloader.jsonl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65438a3a-d78f-463d-b93b-437eb2df6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_hashes_grouped_by_url(file_path_for_jsonl, file_hash_list):\n",
    "    hash_url_dict = {}  # Dictionary to store URLs as keys, and hashes as values (list)\n",
    "    removed_hashes = {}  # To store removed hashes and their duplicated URLs\n",
    "    seen_urls = set()    # Set to track URLs and remove duplicates\n",
    "\n",
    "    # Open and read the file line by line\n",
    "    with open(file_path_for_jsonl, 'r') as file:\n",
    "        for line in file:\n",
    "            try:\n",
    "                # Parse each line as a JSON object\n",
    "                download_info = json.loads(line)\n",
    "\n",
    "                # Get the hash and url from the JSON line\n",
    "                sha256_hash = download_info.get(\"download_sha256\")\n",
    "                url = download_info.get(\"url\")\n",
    "\n",
    "                # Check if the download_sha256 is in the provided hash list\n",
    "                if sha256_hash in file_hash_list:\n",
    "                    # Only process hashes with unique URLs\n",
    "                    if url not in seen_urls:\n",
    "                        # If the URL is new, create an entry in the dictionary\n",
    "                        hash_url_dict[url] = [sha256_hash]\n",
    "                        seen_urls.add(url)  # Mark the URL as seen\n",
    "                    else:\n",
    "                        # If the URL is duplicated, add the hash to the URL's list\n",
    "                        hash_url_dict[url].append(sha256_hash)\n",
    "\n",
    "                        # Track the removed hash if it's a duplicate for that URL\n",
    "                        removed_hashes[sha256_hash] = url\n",
    "                        file_hash_list.remove(sha256_hash)\n",
    "\n",
    "            except json.JSONDecodeError:\n",
    "                # Handle case where the line is not a valid JSON\n",
    "                print(f\"Error decoding JSON for line: {line.strip()}\")\n",
    "    \n",
    "    return hash_url_dict, removed_hashes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0de2c03a-77be-4db7-ad8d-7e71e28bc884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total unique hash groups: 1055\n",
      "\n",
      "Total removed hashes: 0\n"
     ]
    }
   ],
   "source": [
    "# Call the function and retrieve the hash-to-URL dictionary and removed hashes\n",
    "hash_url_dict, removed_hashes = find_hashes_grouped_by_url(file_path_for_jsonl, file_hash_list)\n",
    "\n",
    "# Print the resulting dictionary of hashes grouped by URL\n",
    "#print(\"Hashes grouped by URL:\")\n",
    "#for url, hashes in hash_url_dict.items():\n",
    "#    print(f\"URL: {url}\")\n",
    "#    for sha256_hash in hashes:\n",
    "#        print(f\"    Hash: {sha256_hash}\")\n",
    "\n",
    "# Print the total number of unique hash groups (URLs)\n",
    "print(f\"\\nTotal unique hash groups: {len(hash_url_dict)}\")\n",
    "\n",
    "# Print the removed hashes and their associated URLs\n",
    "#print(\"\\nRemoved hashes due to duplicate URLs:\")\n",
    "#for sha256_hash, url in removed_hashes.items():\n",
    "#    print(f\"Removed Hash: {sha256_hash}, URL: {url}\")\n",
    "\n",
    "# Print the total number of removed hashes\n",
    "print(f\"\\nTotal removed hashes: {len(removed_hashes)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd1a55-1d93-4636-8cd2-6d0cb23b975e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649adb7-db43-49f2-a5e7-f8c00dbb8a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
